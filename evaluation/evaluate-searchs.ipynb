{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "with open('fixed_ground_truth.pkl','rb') as infile:\n",
    "    data = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'vid_id': 'Q75JgLEXMsM',\n",
       "  'title': 'LLM Zoomcamp 1.1 - Introduction to LLM and RAG',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so\",\n",
       "  'timecode_text': 'Introduction to LLM Zoomcamp',\n",
       "  'description': \"Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=Q75JgLEXMsM&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.1 - Introduction to LLM and RAG hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       "  'id': '26c14fe6727a6d5697b4853bcaa6e984',\n",
       "  'student_question': 'What is Retrieval Augmented Generation (RAG) and how is it used in this course?'},\n",
       " {'vid_id': 'Q75JgLEXMsM',\n",
       "  'title': 'LLM Zoomcamp 1.1 - Introduction to LLM and RAG',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so\",\n",
       "  'timecode_text': 'Introduction to LLM Zoomcamp',\n",
       "  'description': \"Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=Q75JgLEXMsM&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.1 - Introduction to LLM and RAG hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       "  'id': '26c14fe6727a6d5697b4853bcaa6e984',\n",
       "  'student_question': 'How do large language models (LLMs) work in the context of answering frequently asked questions?'},\n",
       " {'vid_id': 'Q75JgLEXMsM',\n",
       "  'title': 'LLM Zoomcamp 1.1 - Introduction to LLM and RAG',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so\",\n",
       "  'timecode_text': 'Introduction to LLM Zoomcamp',\n",
       "  'description': \"Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=Q75JgLEXMsM&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.1 - Introduction to LLM and RAG hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       "  'id': '26c14fe6727a6d5697b4853bcaa6e984',\n",
       "  'student_question': 'What practical applications of LLMs will we explore in this course?'},\n",
       " {'vid_id': 'Q75JgLEXMsM',\n",
       "  'title': 'LLM Zoomcamp 1.1 - Introduction to LLM and RAG',\n",
       "  'timecode': '04:03',\n",
       "  'text': \"imagine your phone right so you open WhatsApp and you want to text somebody let's say you want to text your body your friend so you start typing how space i space and the phone your phone will typically suggest you as the next logical word because typically what you type is how are you so it recognizes how are so logically the next word be you right or how are there maybe you want to ask how are the things right so it suggests you commonly common words that go after how are and um also on your phone this is personalized to you and your phones use language models for that so typically it's something simple like na base uh that just predicts the SEC the next word based on what you typed of far so these are just simple or we can call them small language models so they don't have a lot of parameters um they're quite simple they're also quite um how to say stupid let's say because stupid compared to large language models so lot usual simple language models do their job fine but large large language models U there's a reason they call large because they are huge so they have a lot of param they have billions and billions and billions of parameters and then they are trained on tons and tons of data and what they do is essentially the same they predict the next word based on the words before but the way they do it it feels like you're talking to a human at least if you use something like CH GPT yeah it feels like you're talking to an intelligent being because it understands what you ask and gives answers uh but under the hood this is a language model with tons and tons and tons of perameters and train trained on tons and tons and tons of data inside uh let me let me draw a oops let me draw an llm here so it will be a box like that so inside they use uh neur networks like Transformers but we actually will not go into that so in this course we will not cover the theory behind llms we will not try to look inside the llms and we are going to treat them as black boxes so to our purposes this is a super smart thing that can figure out what you ask and give meaningful answers so this is how we are going to use it and what is inside it's for us in this particular course is secondary there are a lot of there are a lot of courses already a lot of books that talk about um the internals of llms but here we will not cover that and typically so LM receives an input so we will talk about the simple case when the input is text and we usually call this input prompt so this is what goes in inside the llm and the output is some answer so how a prompt may look like um for example this is what we're going to use in the course is a question like um so this is the course uh this is the prompt it starts with question um how do I enroll in the course context based on what we should answer the question and there will be some context and then answer and after that we may just leave it at that so answer colum and that's it and remember I talked about language models so the what language models do is they complete the input and they find the next logical term or the next logical uh word for that right so it is that answer column so expect it expects based on the context it expects an answer to the question or we expect it so it will try to to do that so this is a prompt uh an example of a prompt modern llms like chbd you don't actually need to do that like you don't need to write answer at the end uh but this is just an example right and the answer could be actual answer right everything that goes after after this so these are llms and the next thing we will talk about uh is Rock so rock stands for the\",\n",
       "  'timecode_text': 'Understanding LLMs',\n",
       "  'description': \"Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=Q75JgLEXMsM&t=243s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.1 - Introduction to LLM and RAG imagine your phone right so you open WhatsApp and you want to text somebody let's say you want to text your body your friend so you start typing how space i space and the phone your phone will typically suggest you as the next logical word because typically what you type is how are you so it recognizes how are so logically the next word be you right or how are there maybe you want to ask how are the things right so it suggests you commonly common words that go after how are and um also on your phone this is personalized to you and your phones use language models for that so typically it's something simple like na base uh that just predicts the SEC the next word based on what you typed of far so these are just simple or we can call them small language models so they don't have a lot of parameters um they're quite simple they're also quite um how to say stupid let's say because stupid compared to large language models so lot usual simple language models do their job fine but large large language models U there's a reason they call large because they are huge so they have a lot of param they have billions and billions and billions of parameters and then they are trained on tons and tons of data and what they do is essentially the same they predict the next word based on the words before but the way they do it it feels like you're talking to a human at least if you use something like CH GPT yeah it feels like you're talking to an intelligent being because it understands what you ask and gives answers uh but under the hood this is a language model with tons and tons and tons of perameters and train trained on tons and tons and tons of data inside uh let me let me draw a oops let me draw an llm here so it will be a box like that so inside they use uh neur networks like Transformers but we actually will not go into that so in this course we will not cover the theory behind llms we will not try to look inside the llms and we are going to treat them as black boxes so to our purposes this is a super smart thing that can figure out what you ask and give meaningful answers so this is how we are going to use it and what is inside it's for us in this particular course is secondary there are a lot of there are a lot of courses already a lot of books that talk about um the internals of llms but here we will not cover that and typically so LM receives an input so we will talk about the simple case when the input is text and we usually call this input prompt so this is what goes in inside the llm and the output is some answer so how a prompt may look like um for example this is what we're going to use in the course is a question like um so this is the course uh this is the prompt it starts with question um how do I enroll in the course context based on what we should answer the question and there will be some context and then answer and after that we may just leave it at that so answer colum and that's it and remember I talked about language models so the what language models do is they complete the input and they find the next logical term or the next logical uh word for that right so it is that answer column so expect it expects based on the context it expects an answer to the question or we expect it so it will try to to do that so this is a prompt uh an example of a prompt modern llms like chbd you don't actually need to do that like you don't need to write answer at the end uh but this is just an example right and the answer could be actual answer right everything that goes after after this so these are llms and the next thing we will talk about uh is Rock so rock stands for the\",\n",
       "  'student_question': 'What are the main differences between simple language models and large language models?',\n",
       "  'id': '3b81512927800f5b8a8e8c94d6e7caf5'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer(model_name)\n",
    "es = Elasticsearch(\"http://localhost:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_data = []\n",
    "for i in data:\n",
    "    d = {}\n",
    "    d['title'] = i['title']\n",
    "    d['text'] = i['text']\n",
    "    d['timecode_text'] = i['timecode_text']\n",
    "    d['description'] = i['description']\n",
    "    d['id'] = i['id']\n",
    "    d['text_vector'] = model.encode(i['title']+' '+i['text']+' '+i['description'])\n",
    "    vector_data.append(d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in data:\n",
    "    if i['id']:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector_data[0]['text_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'vector-search'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es_client = Elasticsearch('http://localhost:9200') \n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"text\"},\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"timecode_text\": {\"type\": \"text\"},\n",
    "            \"description\": {\"type\": \"keyword\"},\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"text_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "index_name = \"vector-search\"\n",
    "\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in vector_data:\n",
    "    try:\n",
    "        es_client.index(index=index_name, document=doc)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_query(question):\n",
    "    return  {\n",
    "        \"field\": \"text_vector\",\n",
    "        \"query_vector\": model.encode(question),\n",
    "        \"k\": 5,\n",
    "        \"num_candidates\": 10000,\n",
    "        \"boost\": 0.5,\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When did we talk about Mage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(question):\n",
    "    res = es_client.search(index=index_name, knn=knn_query(question), source=[\"id\"])\n",
    "    return res[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'vector-search',\n",
       "  '_id': '23ZD-JEBH2OOoKBj1asY',\n",
       "  '_score': 0.28033978,\n",
       "  '_source': {'id': '3cba0dbf73297ec686a5b91511d47dbe'}},\n",
       " {'_index': 'vector-search',\n",
       "  '_id': '3HZD-JEBH2OOoKBj1asc',\n",
       "  '_score': 0.28033978,\n",
       "  '_source': {'id': '3cba0dbf73297ec686a5b91511d47dbe'}},\n",
       " {'_index': 'vector-search',\n",
       "  '_id': '3XZD-JEBH2OOoKBj1asg',\n",
       "  '_score': 0.28033978,\n",
       "  '_source': {'id': '3cba0dbf73297ec686a5b91511d47dbe'}},\n",
       " {'_index': 'vector-search',\n",
       "  '_id': 'vnZD-JEBH2OOoKBj0Kri',\n",
       "  '_score': 0.2729825,\n",
       "  '_source': {'id': 'db6058ddeee3cd31cb9bcc0aa8db80ea'}},\n",
       " {'_index': 'vector-search',\n",
       "  '_id': 'v3ZD-JEBH2OOoKBj0Krm',\n",
       "  '_score': 0.2729825,\n",
       "  '_source': {'id': 'db6058ddeee3cd31cb9bcc0aa8db80ea'}}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_search(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3028d5847e3849fdaeb49d23aa61dbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "relevance_total = []\n",
    "for q in tqdm(data):\n",
    "    doc_id = q['id']\n",
    "    results = vector_search(q['student_question'])\n",
    "    relevance = [d[\"_source\"]['id'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VECTOR SEARCH Hit Rate is: 0.4540023894862604\n"
     ]
    }
   ],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "print(f\"VECTOR SEARCH Hit Rate is: {hit_rate(relevance_total)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VECTOR SEARCH  MRR is: 0.6554958183990423\n"
     ]
    }
   ],
   "source": [
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)\n",
    "print(f\"VECTOR SEARCH  MRR is: {mrr(relevance_total)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_data = []\n",
    "for i in data:\n",
    "    d = {}\n",
    "    d['title'] = i['title']\n",
    "    d['text'] = i['text']\n",
    "    d['timecode_text'] = i['timecode_text']\n",
    "    d['description'] = i['description']\n",
    "    d['id'] = i['id']\n",
    "    d['title_vector'] = model.encode(i['title'])\n",
    "    d['timecode_vector'] = model.encode(i['timecode_text'])\n",
    "    d['text_vector'] = model.encode(i['text'])\n",
    "    d['description_vector'] = model.encode(i['description'])\n",
    "    hybrid_data.append(d)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'hybrid-search'})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es_client = Elasticsearch('http://localhost:9200') \n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\"type\": \"text\"},\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"timecode_text\": {\"type\": \"text\"},\n",
    "            \"description\": {\"type\": \"keyword\"},\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"title_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"timecode_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"text_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"description_vector\": {\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "index_name = \"hybrid-search\"\n",
    "\n",
    "es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in hybrid_data:\n",
    "    try:\n",
    "        es_client.index(index=index_name, document=doc)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def knn_query(question, vector):\n",
    "    return  {\n",
    "        \"field\": f\"{vector}\",\n",
    "        \"query_vector\": model.encode(question),\n",
    "        \"k\": 5,\n",
    "        \"num_candidates\": 10000,\n",
    "        \"boost\": 0.5,\n",
    "        \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_query(question):\n",
    "    return {\n",
    "        \"bool\": {\n",
    "            \"must\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": f\"{question}\",\n",
    "                    \"fields\": [\"description^3\", \"text\", \"title\"],\n",
    "                    \"type\": \"best_fields\",\n",
    "                    \"boost\": 0.5,\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_search(key_word, vector):\n",
    "    response = es_client.search(\n",
    "        index=index_name,\n",
    "        query=keyword_query(key_word),\n",
    "        knn=knn_query(key_word, vector),\n",
    "        size=10\n",
    "    )\n",
    "    return response[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b0c96c5a7845d2aecd955512c04e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Hit rate for title_vector is: 0.5543608124253285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a6c4b5aa5d47b3bb1eb621e3c28b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Hit rate for timecode_vector is: 0.5639187574671446\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620cee40b26544f29975b428972582f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Hit rate for text_vector is: 0.5579450418160096\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaae4a7ff95149c9b8f2a6c7ed80bbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Hit rate for description_vector is: 0.5531660692951016\n"
     ]
    }
   ],
   "source": [
    "for vector in ['title_vector','timecode_vector','text_vector','description_vector']:\n",
    "    from tqdm.auto import tqdm\n",
    "    relevance_total = []\n",
    "    for q in tqdm(data):\n",
    "        doc_id = q['id']\n",
    "        results = multi_search(q['student_question'], vector)\n",
    "        relevance = [d[\"_source\"]['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "        cnt = 0\n",
    "        for line in relevance_total:\n",
    "            if True in line:\n",
    "                cnt = cnt + 1\n",
    "\n",
    "    \n",
    "    print(f\"Hybrid Hit rate for {vector} is: {cnt / len(relevance_total)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beadf9a8b4e44628b054b32940adc494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR is title_vector: 0.727193681136334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b230faeea3654687b3d8dcb83a16c796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR is timecode_vector: 0.7417022244979242\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5af931cf4946f59968efa7baa7dab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR is text_vector: 0.737086818000797\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bafb12edb034e6f9ad534a317a5c3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR is description_vector: 0.7285970302099337\n"
     ]
    }
   ],
   "source": [
    "for vector in ['title_vector','timecode_vector','text_vector','description_vector']:\n",
    "    from tqdm.auto import tqdm\n",
    "    relevance_total = []\n",
    "    for q in tqdm(data):\n",
    "        doc_id = q['id']\n",
    "        results = multi_search(q['student_question'], vector)\n",
    "        relevance = [d[\"_source\"]['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "        total_score = 0.0\n",
    "        for line in relevance_total:\n",
    "            for rank in range(len(line)):\n",
    "                if line[rank] == True:\n",
    "                    total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "\n",
    "    print(f\"MRR is {vector}: {total_score / len(relevance_total)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def hit_rate(relevance_total):\n",
    "#     cnt = 0\n",
    "\n",
    "#     for line in relevance_total:\n",
    "#         if True in line:\n",
    "#             cnt = cnt + 1\n",
    "\n",
    "#     return cnt / len(relevance_total)\n",
    "# print(f\"Hybrid Hit rate is: {hit_rate(relevance_total)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mrr(relevance_total):\n",
    "#     total_score = 0.0\n",
    "\n",
    "#     for line in relevance_total:\n",
    "#         for rank in range(len(line)):\n",
    "#             if line[rank] == True:\n",
    "#                 total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "#     return total_score / len(relevance_total)\n",
    "# print(f\"MRR is: {mrr(relevance_total)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
