{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('transcripts_metadata_records.pkl', 'rb') as infile:\n",
    "    transcripts = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vid_id': 'Q75JgLEXMsM',\n",
       " 'title': 'LLM Zoomcamp 1.1 - Introduction to LLM and RAG',\n",
       " 'timecode': '00:00',\n",
       " 'text': \"hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so\",\n",
       " 'timecode_text': 'Introduction to LLM Zoomcamp',\n",
       " 'description': \"Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       " 'link': 'https://www.youtube.com/watch?v=Q75JgLEXMsM&t=0s'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for transcript in transcripts:\n",
    "    transcript['text_vector'] = transcript['title']+' '+transcript['text']+' '+transcript['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vid_id': 'Q75JgLEXMsM',\n",
       " 'title': 'LLM Zoomcamp 1.1 - Introduction to LLM and RAG',\n",
       " 'timecode': '00:00',\n",
       " 'text': \"hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so\",\n",
       " 'timecode_text': 'Introduction to LLM Zoomcamp',\n",
       " 'description': \"Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       " 'link': 'https://www.youtube.com/watch?v=Q75JgLEXMsM&t=0s',\n",
       " 'text_vector': \"LLM Zoomcamp 1.1 - Introduction to LLM and RAG hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def gen_ids(data):\n",
    "    for rec in data:\n",
    "        unique_id = rec['vid_id'].strip()+rec['timecode'].strip()\n",
    "        hash_object = hashlib.md5(unique_id.encode())\n",
    "        hash_hex = hash_object.hexdigest()\n",
    "        rec['id'] = hash_hex\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'vid_id': 'Q75JgLEXMsM',\n",
       "  'title': 'LLM Zoomcamp 1.1 - Introduction to LLM and RAG',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so\",\n",
       "  'timecode_text': 'Introduction to LLM Zoomcamp',\n",
       "  'description': \"Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=Q75JgLEXMsM&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.1 - Introduction to LLM and RAG hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       "  'id': '26c14fe6727a6d5697b4853bcaa6e984'},\n",
       " {'vid_id': 'Q75JgLEXMsM',\n",
       "  'title': 'LLM Zoomcamp 1.1 - Introduction to LLM and RAG',\n",
       "  'timecode': '04:03',\n",
       "  'text': \"imagine your phone right so you open WhatsApp and you want to text somebody let's say you want to text your body your friend so you start typing how space i space and the phone your phone will typically suggest you as the next logical word because typically what you type is how are you so it recognizes how are so logically the next word be you right or how are there maybe you want to ask how are the things right so it suggests you commonly common words that go after how are and um also on your phone this is personalized to you and your phones use language models for that so typically it's something simple like na base uh that just predicts the SEC the next word based on what you typed of far so these are just simple or we can call them small language models so they don't have a lot of parameters um they're quite simple they're also quite um how to say stupid let's say because stupid compared to large language models so lot usual simple language models do their job fine but large large language models U there's a reason they call large because they are huge so they have a lot of param they have billions and billions and billions of parameters and then they are trained on tons and tons of data and what they do is essentially the same they predict the next word based on the words before but the way they do it it feels like you're talking to a human at least if you use something like CH GPT yeah it feels like you're talking to an intelligent being because it understands what you ask and gives answers uh but under the hood this is a language model with tons and tons and tons of perameters and train trained on tons and tons and tons of data inside uh let me let me draw a oops let me draw an llm here so it will be a box like that so inside they use uh neur networks like Transformers but we actually will not go into that so in this course we will not cover the theory behind llms we will not try to look inside the llms and we are going to treat them as black boxes so to our purposes this is a super smart thing that can figure out what you ask and give meaningful answers so this is how we are going to use it and what is inside it's for us in this particular course is secondary there are a lot of there are a lot of courses already a lot of books that talk about um the internals of llms but here we will not cover that and typically so LM receives an input so we will talk about the simple case when the input is text and we usually call this input prompt so this is what goes in inside the llm and the output is some answer so how a prompt may look like um for example this is what we're going to use in the course is a question like um so this is the course uh this is the prompt it starts with question um how do I enroll in the course context based on what we should answer the question and there will be some context and then answer and after that we may just leave it at that so answer colum and that's it and remember I talked about language models so the what language models do is they complete the input and they find the next logical term or the next logical uh word for that right so it is that answer column so expect it expects based on the context it expects an answer to the question or we expect it so it will try to to do that so this is a prompt uh an example of a prompt modern llms like chbd you don't actually need to do that like you don't need to write answer at the end uh but this is just an example right and the answer could be actual answer right everything that goes after after this so these are llms and the next thing we will talk about uh is Rock so rock stands for the\",\n",
       "  'timecode_text': 'Understanding LLMs',\n",
       "  'description': \"Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=Q75JgLEXMsM&t=243s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.1 - Introduction to LLM and RAG imagine your phone right so you open WhatsApp and you want to text somebody let's say you want to text your body your friend so you start typing how space i space and the phone your phone will typically suggest you as the next logical word because typically what you type is how are you so it recognizes how are so logically the next word be you right or how are there maybe you want to ask how are the things right so it suggests you commonly common words that go after how are and um also on your phone this is personalized to you and your phones use language models for that so typically it's something simple like na base uh that just predicts the SEC the next word based on what you typed of far so these are just simple or we can call them small language models so they don't have a lot of parameters um they're quite simple they're also quite um how to say stupid let's say because stupid compared to large language models so lot usual simple language models do their job fine but large large language models U there's a reason they call large because they are huge so they have a lot of param they have billions and billions and billions of parameters and then they are trained on tons and tons of data and what they do is essentially the same they predict the next word based on the words before but the way they do it it feels like you're talking to a human at least if you use something like CH GPT yeah it feels like you're talking to an intelligent being because it understands what you ask and gives answers uh but under the hood this is a language model with tons and tons and tons of perameters and train trained on tons and tons and tons of data inside uh let me let me draw a oops let me draw an llm here so it will be a box like that so inside they use uh neur networks like Transformers but we actually will not go into that so in this course we will not cover the theory behind llms we will not try to look inside the llms and we are going to treat them as black boxes so to our purposes this is a super smart thing that can figure out what you ask and give meaningful answers so this is how we are going to use it and what is inside it's for us in this particular course is secondary there are a lot of there are a lot of courses already a lot of books that talk about um the internals of llms but here we will not cover that and typically so LM receives an input so we will talk about the simple case when the input is text and we usually call this input prompt so this is what goes in inside the llm and the output is some answer so how a prompt may look like um for example this is what we're going to use in the course is a question like um so this is the course uh this is the prompt it starts with question um how do I enroll in the course context based on what we should answer the question and there will be some context and then answer and after that we may just leave it at that so answer colum and that's it and remember I talked about language models so the what language models do is they complete the input and they find the next logical term or the next logical uh word for that right so it is that answer column so expect it expects based on the context it expects an answer to the question or we expect it so it will try to to do that so this is a prompt uh an example of a prompt modern llms like chbd you don't actually need to do that like you don't need to write answer at the end uh but this is just an example right and the answer could be actual answer right everything that goes after after this so these are llms and the next thing we will talk about uh is Rock so rock stands for the Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       "  'id': '3b81512927800f5b8a8e8c94d6e7caf5'},\n",
       " {'vid_id': 'Q75JgLEXMsM',\n",
       "  'title': 'LLM Zoomcamp 1.1 - Introduction to LLM and RAG',\n",
       "  'timecode': '09:15',\n",
       "  'text': \"retrieval generation and I think I made it toway here augmented so what it means is generation there are two two things that are interesting here generation and retrieval so we use retrieval to augment generation so retrieval is nothing else but search and generation is llm so llms generate text and they use search to augment the generation of this text what exactly does it mean so I have a few examples why we need track why do we need to augment Our Generation search so let's see yeah so these are the example so I have asked a question uh to chpt and the question is how do I cook cell one and then this is a straightforward question CH GPT gives a very comprehensive answer right so um like there is a lot of uh ways to cook Salman like five quite a lot right so this is a perfect example when we don't need search like we have a question llm knows the answer and llm gives us the answer but what if we want to ask it about the course so ask is it too late for me to join the course right but the llm has no idea what we talk about like which course is it too late what what does it mean exactly right and then it says I can help you but I need to know more details right so can you tell me more about that and the reason we want to use search is actually to answer that to give it more information to give it more context such that the llm is able to give the answer right and this is another example like how do I enroll in the course again it has no clue what we ask about but like it just give some answer that is nonsense right um another good example where we use uh we use uh retrieval also in chbt is we can ask it uh look up in  how to enroll in data engineering Zoom camp and so here we give it explicitly an instruction to uh perform a search and then based on the results of the search give us the answer so this is an example of rack so we augment the generation with the context we get from search and this is similar more similar to what we are going to build um this instruction is quite good okay so let me go back here so search is the first component of rock so we have like some sort of knowledge base and then the other component is the actual llm and then um just remove that um so we have two components knowledge base andm and let's say we have a user the user the student of the course or I don't know just the user if if we talk about our example it could be a student if we talk about some other example could be just user and the user has a question like how do I enroll in the course so this question let me call it Q so in rck what we do first is we send this query or the question to the knowledge base and the knowledge base has some article so this is our FAQ is a knowledge base space right so it has um some articles some answers some questions and answers to these questions so it could be like how do I enroll in the course there are some uh entries they talk about that so we retrieve these entries and then later so let's say these are the documents uh I'll call them T1 uh D2 Etc D5 right so let's say we retrieve five documents so these documents now have the context so remember I showed you example how do I enroll in the course or how like is it too late to join the course the llm has no idea but now these documents they provide the context for llm to figure out the right answer so now using the documents we do a prompt we create a prompt so let me just reuse uh this prompt so now we put this prompt and the context becomes uh these documents D1 Etc D5 right so these are the documents we received and now we send this prompt to the llm so llm receives the prompt it has the question it has the context and now based on the question and the context it can generate answer which we now we return to the user so this is the answer we sent so this is what I call the raack framework like how exactly we use a database knowledge base we can just call it database and LM together in order to accomplish what we want here how do we add context to our queries right so this is the rack framework and actually this database and this llm can be anything so for example in the course in this particular module we first will use a toy search engine like it will be a super simple search engine just to illustrate the idea but then later in the same module we will replace this search engine with elastic search and later in the course we will replace uh we will not just use text search but we will use some other uh ways of searching in particular Vector search so this thing here it can be anything like it doesn't have to be one specific technology in this framework you can easily replace them with another tool and see what works better and the same goes with llm so in this module we will use open AI but it doesn't have to be open AI you can use some open source llm and you just replace one with another you will of course need to tweak The Prompt because different lamps expect the like they they want the prompts to be a little bit different and that's all so we have the framework we replace uh or we put we insert some particular database some particular uh llm we see how it works and then at the end of the course we will also put this in some nice UI like stream Le or something like that so this is what you are going to learn and this is what you're are going to do and then at the end of the course you will Implement something like that on your own knowledge base so I'm really looking forward to the course and uh yeah have fun and uh yeah next we'll talk about uh preparing the environment\",\n",
       "  'timecode_text': 'Exploring RAG',\n",
       "  'description': \"Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=Q75JgLEXMsM&t=555s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.1 - Introduction to LLM and RAG retrieval generation and I think I made it toway here augmented so what it means is generation there are two two things that are interesting here generation and retrieval so we use retrieval to augment generation so retrieval is nothing else but search and generation is llm so llms generate text and they use search to augment the generation of this text what exactly does it mean so I have a few examples why we need track why do we need to augment Our Generation search so let's see yeah so these are the example so I have asked a question uh to chpt and the question is how do I cook cell one and then this is a straightforward question CH GPT gives a very comprehensive answer right so um like there is a lot of uh ways to cook Salman like five quite a lot right so this is a perfect example when we don't need search like we have a question llm knows the answer and llm gives us the answer but what if we want to ask it about the course so ask is it too late for me to join the course right but the llm has no idea what we talk about like which course is it too late what what does it mean exactly right and then it says I can help you but I need to know more details right so can you tell me more about that and the reason we want to use search is actually to answer that to give it more information to give it more context such that the llm is able to give the answer right and this is another example like how do I enroll in the course again it has no clue what we ask about but like it just give some answer that is nonsense right um another good example where we use uh we use uh retrieval also in chbt is we can ask it uh look up in  how to enroll in data engineering Zoom camp and so here we give it explicitly an instruction to uh perform a search and then based on the results of the search give us the answer so this is an example of rack so we augment the generation with the context we get from search and this is similar more similar to what we are going to build um this instruction is quite good okay so let me go back here so search is the first component of rock so we have like some sort of knowledge base and then the other component is the actual llm and then um just remove that um so we have two components knowledge base andm and let's say we have a user the user the student of the course or I don't know just the user if if we talk about our example it could be a student if we talk about some other example could be just user and the user has a question like how do I enroll in the course so this question let me call it Q so in rck what we do first is we send this query or the question to the knowledge base and the knowledge base has some article so this is our FAQ is a knowledge base space right so it has um some articles some answers some questions and answers to these questions so it could be like how do I enroll in the course there are some uh entries they talk about that so we retrieve these entries and then later so let's say these are the documents uh I'll call them T1 uh D2 Etc D5 right so let's say we retrieve five documents so these documents now have the context so remember I showed you example how do I enroll in the course or how like is it too late to join the course the llm has no idea but now these documents they provide the context for llm to figure out the right answer so now using the documents we do a prompt we create a prompt so let me just reuse uh this prompt so now we put this prompt and the context becomes uh these documents D1 Etc D5 right so these are the documents we received and now we send this prompt to the llm so llm receives the prompt it has the question it has the context and now based on the question and the context it can generate answer which we now we return to the user so this is the answer we sent so this is what I call the raack framework like how exactly we use a database knowledge base we can just call it database and LM together in order to accomplish what we want here how do we add context to our queries right so this is the rack framework and actually this database and this llm can be anything so for example in the course in this particular module we first will use a toy search engine like it will be a super simple search engine just to illustrate the idea but then later in the same module we will replace this search engine with elastic search and later in the course we will replace uh we will not just use text search but we will use some other uh ways of searching in particular Vector search so this thing here it can be anything like it doesn't have to be one specific technology in this framework you can easily replace them with another tool and see what works better and the same goes with llm so in this module we will use open AI but it doesn't have to be open AI you can use some open source llm and you just replace one with another you will of course need to tweak The Prompt because different lamps expect the like they they want the prompts to be a little bit different and that's all so we have the framework we replace uh or we put we insert some particular database some particular uh llm we see how it works and then at the end of the course we will also put this in some nice UI like stream Le or something like that so this is what you are going to learn and this is what you're are going to do and then at the end of the course you will Implement something like that on your own knowledge base so I'm really looking forward to the course and uh yeah have fun and uh yeah next we'll talk about uh preparing the environment Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       "  'id': 'a2df1629ab86e0313977b228955bcea2'},\n",
       " {'vid_id': 'ozCpmkbJNJE',\n",
       "  'title': 'LLM Zoomcamp 1.2 - Configuring Your Environment',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hi everyone in this video I want to show you how to configure your environment in this particular video I want to use GitHub codes spaces to show how to configure it but you don't have to use it you can just follow along the video and do everything you need on your local machine I will not show how to install Docker you will need to do it yourself because Cod spaces already has Docker and we will focus on installing uh the libraries we need for this particular model or you can like you again as I said you don't have to use cod spaces it's a very convenient tool but you can also for this model you can use G collab Google collab you can use C Cloud uh you can use any other notebook provider like Sage maker or you can just run things locally for the uh second module we will need a GPU right now we don't need your GPU for the second module we will so if you have an access to a machine with GPU it's good you should use that if you don't uh then this is something we will talk about in the in the next module right now back to Cod spaces I will show you how to set up a simple uh environment from scratch and it's free so you should go to your GitHub account create a new repository uh give it some name so I'll call it llm Zoom camp and um it should be public because uh like you will um commit your notebooks uh your homeworks here um and then you will need to share the link to the homework in the answers uh in the answer for so make it private and then uh so license we don't care about the license G ignore should be python so let's create a repository and once the rep is created we click on code and there are two tabs Here Local and code spaces I just click on create a code space online right now it will start Visual Studio code in a browser so it is preparing the environment uh we don't want to use the browser so I already have Visual Studio desktop um on my computer you should also have it if you want to follow uh along uh this tutorial again you don't have to do this if you already have the tools we're going to install later you should be fine so now let me open it uh on Visual Studio code desktop so I clicked here and I click on open inv this code desktop and right now Visual Studio code if it's the first time you uh run Cod spaces it should after some time should ask you to to install an extension for COD spaces if it doesn't you will need to do this manually you go to extensions and you type code spaces here yeah so this one you will need to install it I already have it so I don't need to install it and actually we already have a code space enabled in accessible in our Visual Studio code desktop and I just pressed contrl Tilda to open the command line and here we have already have Docker can run hello world so Docker Docker compose uh are already here and we have Python and um I'll show you two ways of preparing the environment the first uh way would be just using the python that we already have here we will need to install the libraries we need and then in the second part of the video I'll show you how to install an aond with\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, I guide you through configuring your environment using GitHub Code Spaces, focusing on installing necessary libraries for a specific model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=ozCpmkbJNJE&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.2 - Configuring Your Environment hi everyone in this video I want to show you how to configure your environment in this particular video I want to use GitHub codes spaces to show how to configure it but you don't have to use it you can just follow along the video and do everything you need on your local machine I will not show how to install Docker you will need to do it yourself because Cod spaces already has Docker and we will focus on installing uh the libraries we need for this particular model or you can like you again as I said you don't have to use cod spaces it's a very convenient tool but you can also for this model you can use G collab Google collab you can use C Cloud uh you can use any other notebook provider like Sage maker or you can just run things locally for the uh second module we will need a GPU right now we don't need your GPU for the second module we will so if you have an access to a machine with GPU it's good you should use that if you don't uh then this is something we will talk about in the in the next module right now back to Cod spaces I will show you how to set up a simple uh environment from scratch and it's free so you should go to your GitHub account create a new repository uh give it some name so I'll call it llm Zoom camp and um it should be public because uh like you will um commit your notebooks uh your homeworks here um and then you will need to share the link to the homework in the answers uh in the answer for so make it private and then uh so license we don't care about the license G ignore should be python so let's create a repository and once the rep is created we click on code and there are two tabs Here Local and code spaces I just click on create a code space online right now it will start Visual Studio code in a browser so it is preparing the environment uh we don't want to use the browser so I already have Visual Studio desktop um on my computer you should also have it if you want to follow uh along uh this tutorial again you don't have to do this if you already have the tools we're going to install later you should be fine so now let me open it uh on Visual Studio code desktop so I clicked here and I click on open inv this code desktop and right now Visual Studio code if it's the first time you uh run Cod spaces it should after some time should ask you to to install an extension for COD spaces if it doesn't you will need to do this manually you go to extensions and you type code spaces here yeah so this one you will need to install it I already have it so I don't need to install it and actually we already have a code space enabled in accessible in our Visual Studio code desktop and I just pressed contrl Tilda to open the command line and here we have already have Docker can run hello world so Docker Docker compose uh are already here and we have Python and um I'll show you two ways of preparing the environment the first uh way would be just using the python that we already have here we will need to install the libraries we need and then in the second part of the video I'll show you how to install an aond with In this video, I guide you through configuring your environment using GitHub Code Spaces, focusing on installing necessary libraries for a specific model.\",\n",
       "  'id': 'db6058ddeee3cd31cb9bcc0aa8db80ea'},\n",
       " {'vid_id': 'ozCpmkbJNJE',\n",
       "  'title': 'LLM Zoomcamp 1.2 - Configuring Your Environment',\n",
       "  'timecode': '03:50',\n",
       "  'text': \"anakonda you don't need to install any libraries you can just download in AA and that's all you need so uh for installing libraries we'll use peep in in install and then there's a bunch of libraries we need to install tqdm which is a progress manager it's very convenient uh then uh well we will not need tqdm actually for this particular module we will need it later uh then we will need jupyter notebook I'll specify the version uh the version will will be 7.1.2 then we will need open ey to speak to uh the open AI API and also we will need um elastic search so Arch right at the end as I said we will first we will use a simple toy search engine but then at the end of the module we will replace it with elastic search and so that's why we need to install it um I don't know if we need to install anything else so also I think P learn uh yeah and pandas so that's Jupiter notebook open elastic s second learn pandas when later we use Anaconda we will need to install tqdm openi and elastic search it already has Jupiter it already has Psy it already has pandas and then I made a type elastic search ltic search hopefully it should work now so many T sorry about so did I make a typo did I make it\",\n",
       "  'timecode_text': 'Anaconda Installation',\n",
       "  'description': 'In this video, I guide you through configuring your environment using GitHub Code Spaces, focusing on installing necessary libraries for a specific model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=ozCpmkbJNJE&t=230s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.2 - Configuring Your Environment anakonda you don't need to install any libraries you can just download in AA and that's all you need so uh for installing libraries we'll use peep in in install and then there's a bunch of libraries we need to install tqdm which is a progress manager it's very convenient uh then uh well we will not need tqdm actually for this particular module we will need it later uh then we will need jupyter notebook I'll specify the version uh the version will will be 7.1.2 then we will need open ey to speak to uh the open AI API and also we will need um elastic search so Arch right at the end as I said we will first we will use a simple toy search engine but then at the end of the module we will replace it with elastic search and so that's why we need to install it um I don't know if we need to install anything else so also I think P learn uh yeah and pandas so that's Jupiter notebook open elastic s second learn pandas when later we use Anaconda we will need to install tqdm openi and elastic search it already has Jupiter it already has Psy it already has pandas and then I made a type elastic search ltic search hopefully it should work now so many T sorry about so did I make a typo did I make it In this video, I guide you through configuring your environment using GitHub Code Spaces, focusing on installing necessary libraries for a specific model.\",\n",
       "  'id': '31e622970a5811ff4d9508bdccd20511'},\n",
       " {'vid_id': 'ozCpmkbJNJE',\n",
       "  'title': 'LLM Zoomcamp 1.2 - Configuring Your Environment',\n",
       "  'timecode': '05:55',\n",
       "  'text': \"typo again no this time it was actually good and while we are installing it uh we will need to have a key for open AI because we will use it it's llm GPD so I will go to uh platform. open.com you should uh register there if you haven't and this is sort of semi optional we will also show how to run it without openi but for Simplicity and also for this model you should have an uh you should have an account there then we need to go to API keys and I'm going to create a new API key I'll call it um course and I'll create a secret key I will remove this key of course after recording this video I don't want you to use it of course so but right now you will see my key but this should never ever happen like you do not you have to watch your key and never ever expose it commit it to G or uh show it to anyone show it to people you don't trust so now we need to take the key and now I write expert open AI API key and then I copy the key so now we have the key in our environment and I just start Jitter notbook so notbook is starting and if you run in code spaces like me you should notice that this port ports um tap got one so it means that uh it detected that there is a port\",\n",
       "  'timecode_text': 'Elasticsearch Configuration',\n",
       "  'description': 'In this video, I guide you through configuring your environment using GitHub Code Spaces, focusing on installing necessary libraries for a specific model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=ozCpmkbJNJE&t=355s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.2 - Configuring Your Environment typo again no this time it was actually good and while we are installing it uh we will need to have a key for open AI because we will use it it's llm GPD so I will go to uh platform. open.com you should uh register there if you haven't and this is sort of semi optional we will also show how to run it without openi but for Simplicity and also for this model you should have an uh you should have an account there then we need to go to API keys and I'm going to create a new API key I'll call it um course and I'll create a secret key I will remove this key of course after recording this video I don't want you to use it of course so but right now you will see my key but this should never ever happen like you do not you have to watch your key and never ever expose it commit it to G or uh show it to anyone show it to people you don't trust so now we need to take the key and now I write expert open AI API key and then I copy the key so now we have the key in our environment and I just start Jitter notbook so notbook is starting and if you run in code spaces like me you should notice that this port ports um tap got one so it means that uh it detected that there is a port In this video, I guide you through configuring your environment using GitHub Code Spaces, focusing on installing necessary libraries for a specific model.\",\n",
       "  'id': '2640bd291048944e7315e82b96bd0465'},\n",
       " {'vid_id': 'ozCpmkbJNJE',\n",
       "  'title': 'LLM Zoomcamp 1.2 - Configuring Your Environment',\n",
       "  'timecode': '07:57',\n",
       "  'text': \"8888 that is running on the Cod spaces and it forwarded automatically forwarded this port to our local machine which means that now we can use our browser to access the jupyter notebook I'll just type Local Host 8888 and yeah it asks for token we can just copy this whole thing or if the port is different for you you can copy this token so maybe you already have Jupiter running and then the would be 8888 9 something like that I don't know if I how many eights I said anyways um so we have Jupiter now I just click new Python 3 it will automatically open it and let me make it large I'll hide H here and then we can just test that things work so I'll import open AI so works and now we can send a simple request to open AI um so check the code actually we do it from open AI import open AI create a [Music] client and here when we create a client we can provide the API key explicitly here by saying API um key and then this is the key that I have is the key I think I should start with SK whatever um uh I'm just wondering what did I put here CU typically the key starts with um SK something um yeah I will not try to find it here anyways so we put the key like that or if we don't provide it it will read it from the uh from the environmental variable that we created um we can quickly check if actually I did that so we import Os Os M creates see I think it's s in environment yeah so here we have um we should have open I key yeah so this is the key so this is the right key and we can put the key directly or if we set up this um um this environmental variable we don't need to do anything so we just do Cent client initialize the client it looks up the key in the environmental variables and uses it right so we have the client and now we can say that we want to use chat um chat API uh like that and we can specify the model we want to use so for example if we go here to playground can see what kind of models they have right so we can just say we want to use GPT 40 GPT for all and message messages is what we want to write to the client so let's say yeah so we need to specify the role uh there are multiple roles like user and system we will not go into details right now so user is like when in Char GPT I type something so the role is user for me so this is exactly what I did when I typed uh when I opened gbt in the intro video chbt and typed um how do I cook salmon and then content is uh let me write um something like is it too late to join the course then let's save it to response and then see what is the reply and the reply is yeah there's a bunch of things but we can say choices so there are there is only one choice right and then the message inside the choices is this one or we can just show the [Music] content doing it this way so and then this is the answer right so of course it has no idea what course I'm referring to therefore it provides this answer now I'll show you a different way to prepare the environment so this is kind of enough but if you want to see how to do it with Anaconda it's a little bit easier Anaconda install\",\n",
       "  'timecode_text': 'Accessing Jupyter Notebook',\n",
       "  'description': 'In this video, I guide you through configuring your environment using GitHub Code Spaces, focusing on installing necessary libraries for a specific model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=ozCpmkbJNJE&t=477s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.2 - Configuring Your Environment 8888 that is running on the Cod spaces and it forwarded automatically forwarded this port to our local machine which means that now we can use our browser to access the jupyter notebook I'll just type Local Host 8888 and yeah it asks for token we can just copy this whole thing or if the port is different for you you can copy this token so maybe you already have Jupiter running and then the would be 8888 9 something like that I don't know if I how many eights I said anyways um so we have Jupiter now I just click new Python 3 it will automatically open it and let me make it large I'll hide H here and then we can just test that things work so I'll import open AI so works and now we can send a simple request to open AI um so check the code actually we do it from open AI import open AI create a [Music] client and here when we create a client we can provide the API key explicitly here by saying API um key and then this is the key that I have is the key I think I should start with SK whatever um uh I'm just wondering what did I put here CU typically the key starts with um SK something um yeah I will not try to find it here anyways so we put the key like that or if we don't provide it it will read it from the uh from the environmental variable that we created um we can quickly check if actually I did that so we import Os Os M creates see I think it's s in environment yeah so here we have um we should have open I key yeah so this is the key so this is the right key and we can put the key directly or if we set up this um um this environmental variable we don't need to do anything so we just do Cent client initialize the client it looks up the key in the environmental variables and uses it right so we have the client and now we can say that we want to use chat um chat API uh like that and we can specify the model we want to use so for example if we go here to playground can see what kind of models they have right so we can just say we want to use GPT 40 GPT for all and message messages is what we want to write to the client so let's say yeah so we need to specify the role uh there are multiple roles like user and system we will not go into details right now so user is like when in Char GPT I type something so the role is user for me so this is exactly what I did when I typed uh when I opened gbt in the intro video chbt and typed um how do I cook salmon and then content is uh let me write um something like is it too late to join the course then let's save it to response and then see what is the reply and the reply is yeah there's a bunch of things but we can say choices so there are there is only one choice right and then the message inside the choices is this one or we can just show the [Music] content doing it this way so and then this is the answer right so of course it has no idea what course I'm referring to therefore it provides this answer now I'll show you a different way to prepare the environment so this is kind of enough but if you want to see how to do it with Anaconda it's a little bit easier Anaconda install In this video, I guide you through configuring your environment using GitHub Code Spaces, focusing on installing necessary libraries for a specific model.\",\n",
       "  'id': 'd76fbf0f473ee608c1d28bfb1bdef826'},\n",
       " {'vid_id': 'ozCpmkbJNJE',\n",
       "  'title': 'LLM Zoomcamp 1.2 - Configuring Your Environment',\n",
       "  'timecode': '13:26',\n",
       "  'text': \"Linux then uh we have this link docs andaconda free andaconda install Linux and then there is a URL that we need to execute to to have and we can see the full list of installers available at this link and I'll just pick this is the probably the latest one notice that this is actually quite large so it's almost 1 gab uh cuz like there's a lot of stuff in Anaconda and most of it we frankly don't need but for Simplicity like it's very simple we just download unpack it and can use it um so I copy the link and I'll stop right now Jupiter and I'll do W get um I'll go one one level up so I don't download an AA in my workspace uh I do W get and then this path so now I download it and I will now unpack it you can also instead of using Anaconda you can use miniconda maybe I'll show you too how to do this mini uh install Linux and this is actually the same we just need to find um any installer for Linux we just need to find where yeah this is the latest so they also have this repo this is the same URL as we used previously um so let's use this one with python 3.10 for Linux uh this architecture so will copy the link and notice that it's way smaller this link so it's way smaller than what we had so just do all we yet and let's install miniconda but for Anaconda it's the same you do bash minond and then you read the agreement it's becoming longer and longer yeah come on yeah I accept because I read very carefully so now we install uh minondo and with an Aon would be the same just would take a bit little bit longer we initialize it and now when I create a new shell we have base here here meaning that we Ed the base environment and now we can use which python to see to see where the python comes from so the python comes from minond and when we do python minus V so this is a python from miniconda so this could be convenient if you're are not running on U Cod spaces um but let's now when we do which pip we can install the same things we installed previously and if it's miniconda or if it's anakonda you don't need to install Jupiter you don't need to install um second Lear you don't need to install pandas just elastic search open Ai and tqdm I think that's enough for this so now\",\n",
       "  'timecode_text': 'Alternative Environment Setup',\n",
       "  'description': 'In this video, I guide you through configuring your environment using GitHub Code Spaces, focusing on installing necessary libraries for a specific model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=ozCpmkbJNJE&t=806s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.2 - Configuring Your Environment Linux then uh we have this link docs andaconda free andaconda install Linux and then there is a URL that we need to execute to to have and we can see the full list of installers available at this link and I'll just pick this is the probably the latest one notice that this is actually quite large so it's almost 1 gab uh cuz like there's a lot of stuff in Anaconda and most of it we frankly don't need but for Simplicity like it's very simple we just download unpack it and can use it um so I copy the link and I'll stop right now Jupiter and I'll do W get um I'll go one one level up so I don't download an AA in my workspace uh I do W get and then this path so now I download it and I will now unpack it you can also instead of using Anaconda you can use miniconda maybe I'll show you too how to do this mini uh install Linux and this is actually the same we just need to find um any installer for Linux we just need to find where yeah this is the latest so they also have this repo this is the same URL as we used previously um so let's use this one with python 3.10 for Linux uh this architecture so will copy the link and notice that it's way smaller this link so it's way smaller than what we had so just do all we yet and let's install miniconda but for Anaconda it's the same you do bash minond and then you read the agreement it's becoming longer and longer yeah come on yeah I accept because I read very carefully so now we install uh minondo and with an Aon would be the same just would take a bit little bit longer we initialize it and now when I create a new shell we have base here here meaning that we Ed the base environment and now we can use which python to see to see where the python comes from so the python comes from minond and when we do python minus V so this is a python from miniconda so this could be convenient if you're are not running on U Cod spaces um but let's now when we do which pip we can install the same things we installed previously and if it's miniconda or if it's anakonda you don't need to install Jupiter you don't need to install um second Lear you don't need to install pandas just elastic search open Ai and tqdm I think that's enough for this so now In this video, I guide you through configuring your environment using GitHub Code Spaces, focusing on installing necessary libraries for a specific model.\",\n",
       "  'id': 'e8f648db82162a6e9f95ea76e167ad7a'},\n",
       " {'vid_id': 'ozCpmkbJNJE',\n",
       "  'title': 'LLM Zoomcamp 1.2 - Configuring Your Environment',\n",
       "  'timecode': '17:00',\n",
       "  'text': \"you know how to prepare the environment uh maybe just last thing um I'll show you how to commit your changes so this is the file we worked on let's say this is your homework of course give it a meaningful name uh we can create a new like create a proper directory for this uh 0 one intro think I made inro and then put it there and then give it a good name like um I know homework and then we put it I created a file instead of a directory yeah so I'll create a folder 01 intro put it there and do G at now we have this there and just do get commit work one and push it to get so now it's there uh we pushed it to get uh we have prepared the environment now I'll just stop the Cod space should be stopped I'll also show you how to delete it it's not necessary but like if you just want if you know that you're not going to use it you can just delete it and that's how you use cod spaces for preparing the environment so yeah have fun and in the uh next unit we will see how to prepare how to index our documents with the search engine see soon\",\n",
       "  'timecode_text': 'Final Steps',\n",
       "  'description': 'In this video, I guide you through configuring your environment using GitHub Code Spaces, focusing on installing necessary libraries for a specific model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=ozCpmkbJNJE&t=1020s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.2 - Configuring Your Environment you know how to prepare the environment uh maybe just last thing um I'll show you how to commit your changes so this is the file we worked on let's say this is your homework of course give it a meaningful name uh we can create a new like create a proper directory for this uh 0 one intro think I made inro and then put it there and then give it a good name like um I know homework and then we put it I created a file instead of a directory yeah so I'll create a folder 01 intro put it there and do G at now we have this there and just do get commit work one and push it to get so now it's there uh we pushed it to get uh we have prepared the environment now I'll just stop the Cod space should be stopped I'll also show you how to delete it it's not necessary but like if you just want if you know that you're not going to use it you can just delete it and that's how you use cod spaces for preparing the environment so yeah have fun and in the uh next unit we will see how to prepare how to index our documents with the search engine see soon In this video, I guide you through configuring your environment using GitHub Code Spaces, focusing on installing necessary libraries for a specific model.\",\n",
       "  'id': '295a30e9d02b94789b51df76dcbdb43b'},\n",
       " {'vid_id': 'olvem333Bqo',\n",
       "  'title': 'LLM Zoomcamp 1.3 - Retrieval and Search',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"welcome back in this video we are going to talk about retrieval and I want to start with talking again about the rock framework remember in the r framework we have two components the database and llm so here for the database we will use a simple SE search engine uh the search engine that we implemented as a in one of the pre courses uh pre-course workshops and if you go to the uh course repo you can see the workshop here implementing a search engine there's a video that that you can hold follow along and there is a GitHub rep so in that Workshop we implemented a search engine it was a very simple inmemory search engine it's not production ready it was just for illustrating how search engines Works work and right now we are going to use this simple search engine and later later in the model we will replace it with another one uh with elastic search but here I just want to illustrate the principle so right now what we're going to do is we're going to take this search engine and take the data from the FAQ documents and put the data to the search engine and perform a simple search and later we will use the results to put I can just open it here we'll get the results put them into an nlm and we will get answers to our questions so this is the first step and we are going to use this um so there is an an implementation that we're going to use this is this minan search this is the python file that implements the the search functionality so I'm going to get the r uh implementation um I just want to download the file the the r python file I just want to download it it's not people install able so I just download the file and use it as a as a package so I already have it um I\",\n",
       "  'timecode_text': 'Introduction to Retrieval',\n",
       "  'description': \"In this video, I delve into the concept of retrieval, focusing on the rack framework's components - the database and the LLM. We start by using a simple search engine to search FAQ documents, illustrating search engine principles. Action: Watch the process of putting data into the search engine and performing a search to get answers.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=olvem333Bqo&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.3 - Retrieval and Search welcome back in this video we are going to talk about retrieval and I want to start with talking again about the rock framework remember in the r framework we have two components the database and llm so here for the database we will use a simple SE search engine uh the search engine that we implemented as a in one of the pre courses uh pre-course workshops and if you go to the uh course repo you can see the workshop here implementing a search engine there's a video that that you can hold follow along and there is a GitHub rep so in that Workshop we implemented a search engine it was a very simple inmemory search engine it's not production ready it was just for illustrating how search engines Works work and right now we are going to use this simple search engine and later later in the model we will replace it with another one uh with elastic search but here I just want to illustrate the principle so right now what we're going to do is we're going to take this search engine and take the data from the FAQ documents and put the data to the search engine and perform a simple search and later we will use the results to put I can just open it here we'll get the results put them into an nlm and we will get answers to our questions so this is the first step and we are going to use this um so there is an an implementation that we're going to use this is this minan search this is the python file that implements the the search functionality so I'm going to get the r uh implementation um I just want to download the file the the r python file I just want to download it it's not people install able so I just download the file and use it as a as a package so I already have it um I In this video, I delve into the concept of retrieval, focusing on the rack framework's components - the database and the LLM. We start by using a simple search engine to search FAQ documents, illustrating search engine principles. Action: Watch the process of putting data into the search engine and performing a search to get answers.\",\n",
       "  'id': '64ad4cbcb952d38e16e49b1fa7e2c5b0'},\n",
       " {'vid_id': 'olvem333Bqo',\n",
       "  'title': 'LLM Zoomcamp 1.3 - Retrieval and Search',\n",
       "  'timecode': '02:06',\n",
       "  'text': \"already have um my Jupiter running here I'll start a new notebook I will call it we can call it Ro intro for example and the first step here I will download uh this uh search engine the this implementation using W get command and now I can just import it import main search so this is Library uh we implemented it took a bit of time for some reasons um so this is the search engine we implemented and right now we are going to get the fq documents the fq documents um I already passed them so they are in adjacent form it look like that looks like that so we have for each course we have adjacent object and inside the Json object we have different documents like the question the section of the document and the text is the answer to the question if you're wondering how exactly I extracted these documents there is another uh another IPython notebook that uh you can see how I did that so this code I used for extracting the documents so this is something you can use for your own projects later if you want if you have a also a Google Document which you want to use for extracting the data this is what you can use I will not go into details of how exactly it works although if there's a lot of interest maybe I'll record another video of how it works but for now we just get the dat this Json file and we will need to load this data and we will need to put it into our search engine so for that I'll import the Json built-in library and I will open the the documents yeah documents oops um so we open it in read text mode as F in so this is the anation I usually use fcor in is file input so we open file for reading and then doc I'll call it DOC ra is Json Lo F so this is um this is document maybe I should have not printed it um and the structure is as I said for each of the courses we have like nested structure with documents now I want to put everything into um so what I want to have is to get rid of this nestedness so I just want to have a list with dictionaries a list with documents so I want to create um so keep this as this just add another field course and use this data gen camp or other courses so let me call it docment um so this will contain the result and we just go for course in document s then for I Doc in course we can just go say document course equals course course maybe it's course dictionary something like that um yeah and actually here we go over docs right course document and then document do something like that so now our documents should contain um documents records that so we have course we have question we have session where to which the the part of the document belongs the the actual question and the answer to this question so this is what we have and we\",\n",
       "  'timecode_text': 'Preparing the Documents',\n",
       "  'description': \"In this video, I delve into the concept of retrieval, focusing on the rack framework's components - the database and the LLM. We start by using a simple search engine to search FAQ documents, illustrating search engine principles. Action: Watch the process of putting data into the search engine and performing a search to get answers.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=olvem333Bqo&t=126s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.3 - Retrieval and Search already have um my Jupiter running here I'll start a new notebook I will call it we can call it Ro intro for example and the first step here I will download uh this uh search engine the this implementation using W get command and now I can just import it import main search so this is Library uh we implemented it took a bit of time for some reasons um so this is the search engine we implemented and right now we are going to get the fq documents the fq documents um I already passed them so they are in adjacent form it look like that looks like that so we have for each course we have adjacent object and inside the Json object we have different documents like the question the section of the document and the text is the answer to the question if you're wondering how exactly I extracted these documents there is another uh another IPython notebook that uh you can see how I did that so this code I used for extracting the documents so this is something you can use for your own projects later if you want if you have a also a Google Document which you want to use for extracting the data this is what you can use I will not go into details of how exactly it works although if there's a lot of interest maybe I'll record another video of how it works but for now we just get the dat this Json file and we will need to load this data and we will need to put it into our search engine so for that I'll import the Json built-in library and I will open the the documents yeah documents oops um so we open it in read text mode as F in so this is the anation I usually use fcor in is file input so we open file for reading and then doc I'll call it DOC ra is Json Lo F so this is um this is document maybe I should have not printed it um and the structure is as I said for each of the courses we have like nested structure with documents now I want to put everything into um so what I want to have is to get rid of this nestedness so I just want to have a list with dictionaries a list with documents so I want to create um so keep this as this just add another field course and use this data gen camp or other courses so let me call it docment um so this will contain the result and we just go for course in document s then for I Doc in course we can just go say document course equals course course maybe it's course dictionary something like that um yeah and actually here we go over docs right course document and then document do something like that so now our documents should contain um documents records that so we have course we have question we have session where to which the the part of the document belongs the the actual question and the answer to this question so this is what we have and we In this video, I delve into the concept of retrieval, focusing on the rack framework's components - the database and the LLM. We start by using a simple search engine to search FAQ documents, illustrating search engine principles. Action: Watch the process of putting data into the search engine and performing a search to get answers.\",\n",
       "  'id': '20990df4b235db0addfecdb25971d4e8'},\n",
       " {'vid_id': 'olvem333Bqo',\n",
       "  'title': 'LLM Zoomcamp 1.3 - Retrieval and Search',\n",
       "  'timecode': '06:42',\n",
       "  'text': \"passed this document now we want to index it so for that in this mean search Library we have um the index and here I'm going to just uh go to where it is I'm just going to use an example here instead of typing because I think the example uses the same thing that the same data set so now we create an index and we specify the we need to specify two things we need to specify which fields are text fields and which fields are keywords Fields so for keywords Fields uh we can just do filtering so for example what if we want to select to restrict our search only to the data engineering Zoom Camp courses so it would be something similar to a SQL query where we do select uh where course course equals to data engineering Zoom cap right so this is um just in lastic Search terms and for now this toy search engine these are the keywords so this are the things that we can use for exact filtering so right now we only want to keep the documents for data engineer Z camp or for machine learning Zoom camp or for mlops Zoom Camp right so these are the filters the keyword fields we do filtering and text Fields is the fields for which which we use for performance search uh so this would be the question the text and session fields right so here when um let's say user query is um who how do I enroll um let me ask something uh more elaborate like the course has already started can I still enroll right so this is the question and\",\n",
       "  'timecode_text': 'Indexing Documents with min-search Library',\n",
       "  'description': \"In this video, I delve into the concept of retrieval, focusing on the rack framework's components - the database and the LLM. We start by using a simple search engine to search FAQ documents, illustrating search engine principles. Action: Watch the process of putting data into the search engine and performing a search to get answers.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=olvem333Bqo&t=402s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.3 - Retrieval and Search passed this document now we want to index it so for that in this mean search Library we have um the index and here I'm going to just uh go to where it is I'm just going to use an example here instead of typing because I think the example uses the same thing that the same data set so now we create an index and we specify the we need to specify two things we need to specify which fields are text fields and which fields are keywords Fields so for keywords Fields uh we can just do filtering so for example what if we want to select to restrict our search only to the data engineering Zoom Camp courses so it would be something similar to a SQL query where we do select uh where course course equals to data engineering Zoom cap right so this is um just in lastic Search terms and for now this toy search engine these are the keywords so this are the things that we can use for exact filtering so right now we only want to keep the documents for data engineer Z camp or for machine learning Zoom camp or for mlops Zoom Camp right so these are the filters the keyword fields we do filtering and text Fields is the fields for which which we use for performance search uh so this would be the question the text and session fields right so here when um let's say user query is um who how do I enroll um let me ask something uh more elaborate like the course has already started can I still enroll right so this is the question and In this video, I delve into the concept of retrieval, focusing on the rack framework's components - the database and the LLM. We start by using a simple search engine to search FAQ documents, illustrating search engine principles. Action: Watch the process of putting data into the search engine and performing a search to get answers.\",\n",
       "  'id': '365d4966652d1e9e5db732cf4996207f'},\n",
       " {'vid_id': 'olvem333Bqo',\n",
       "  'title': 'LLM Zoomcamp 1.3 - Retrieval and Search',\n",
       "  'timecode': '08:53',\n",
       "  'text': \"now what we want to do is we want to retrieve documents that potentially can contain answers to these questions to this question so what we want to do is we want to um look up in this database all the uh questions texts and sections that contain relevant information right and this Index this search engine can help us with that so I'll call it index and now with index we can do fit and fit is if you are familiar with the psyit learn um API how they usually do things is uh yeah they have this fit method and this is exactly what we're going to use and fit it to the documents so right now it has analyzed all the documents it knows what are the the key the text Fields what are the keywords so now we can use this search engine to perform uh to execute this query and see what documents are relevant for this query so now let's say we perform the search and here we have a bunch of things a bunch of arguments so the actual query this is um the query that I provided the course has already started can I still enroll then we have um boost so boost is um when we think that one of the text Fields is more important than the other we can boost this um this field we can say that if the question field contains the word course started en roll then it's more important than if the answer contains these words right so then we want to give more importance to the the question term and this is we do with boosting and actually these are all terms from elastic search so we just reimplemented it uh with secondy LA and pandas in the workshop but all these terms come from lastic search or actually from Lucine and Lucine is the search engine that elastic search uses under the hood so we give a boost of five to let's give it three a boost of three to the teex to the question field meaning that the question field is three times more important than uh the text field and for example let's say we want to have section uh less important to give section less importance then we can say that um it's by default it's one like all Fields have the importance of one boost of one and we can give less importance by giving zero .5 or more importance by giving like more than one like three then so boost dictionary is this boost and what else do we have there is a third parameter which is number results so this is the amount of results it returns let's say five this is results so now we perform a search and this is what the results are and and the top relevant documents are yes you can you won't be able to submit some of the homeworks um so actually like this is exactly the same question that we uh try to find the answer to the course has already started can I still join um it's a little bit different because like we used enroll not join but still we were able to retrieve this document um by the way so we also noticed that um here we actually retrieve so let's say if we want to build a Q&A system for the data engineering course so we want to be able to restrict our answers only to the data engineering uh Records so here we retrieved a record from another FAQ not from the data engineer fq so which means we also need to add a filter which is another parameter here so we can say that look we are only interested in documents for the data engineering engineering Zoom Camp so only give me the documents that are about um the course and now the results should be different so now we only have um the data engineering Zoom Camp answers and again the first answer is quite relevant can I still join the course after the start date and this is the answer right so now we implemented the first step we implemented we have a query and now we have indexed our uh knowledge base so now we can ask this uh knowledge base for the context right so now we think okay for this query these are the most relevant documents and what we are going to do next is we are going to put these documents to an uh llm and use this as context so we want to build a prompt that looks like this and this is what we are going to do in the next video see you soon\",\n",
       "  'timecode_text': 'Retrieving Documents for a Query',\n",
       "  'description': \"In this video, I delve into the concept of retrieval, focusing on the rack framework's components - the database and the LLM. We start by using a simple search engine to search FAQ documents, illustrating search engine principles. Action: Watch the process of putting data into the search engine and performing a search to get answers.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=olvem333Bqo&t=533s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.3 - Retrieval and Search now what we want to do is we want to retrieve documents that potentially can contain answers to these questions to this question so what we want to do is we want to um look up in this database all the uh questions texts and sections that contain relevant information right and this Index this search engine can help us with that so I'll call it index and now with index we can do fit and fit is if you are familiar with the psyit learn um API how they usually do things is uh yeah they have this fit method and this is exactly what we're going to use and fit it to the documents so right now it has analyzed all the documents it knows what are the the key the text Fields what are the keywords so now we can use this search engine to perform uh to execute this query and see what documents are relevant for this query so now let's say we perform the search and here we have a bunch of things a bunch of arguments so the actual query this is um the query that I provided the course has already started can I still enroll then we have um boost so boost is um when we think that one of the text Fields is more important than the other we can boost this um this field we can say that if the question field contains the word course started en roll then it's more important than if the answer contains these words right so then we want to give more importance to the the question term and this is we do with boosting and actually these are all terms from elastic search so we just reimplemented it uh with secondy LA and pandas in the workshop but all these terms come from lastic search or actually from Lucine and Lucine is the search engine that elastic search uses under the hood so we give a boost of five to let's give it three a boost of three to the teex to the question field meaning that the question field is three times more important than uh the text field and for example let's say we want to have section uh less important to give section less importance then we can say that um it's by default it's one like all Fields have the importance of one boost of one and we can give less importance by giving zero .5 or more importance by giving like more than one like three then so boost dictionary is this boost and what else do we have there is a third parameter which is number results so this is the amount of results it returns let's say five this is results so now we perform a search and this is what the results are and and the top relevant documents are yes you can you won't be able to submit some of the homeworks um so actually like this is exactly the same question that we uh try to find the answer to the course has already started can I still join um it's a little bit different because like we used enroll not join but still we were able to retrieve this document um by the way so we also noticed that um here we actually retrieve so let's say if we want to build a Q&A system for the data engineering course so we want to be able to restrict our answers only to the data engineering uh Records so here we retrieved a record from another FAQ not from the data engineer fq so which means we also need to add a filter which is another parameter here so we can say that look we are only interested in documents for the data engineering engineering Zoom Camp so only give me the documents that are about um the course and now the results should be different so now we only have um the data engineering Zoom Camp answers and again the first answer is quite relevant can I still join the course after the start date and this is the answer right so now we implemented the first step we implemented we have a query and now we have indexed our uh knowledge base so now we can ask this uh knowledge base for the context right so now we think okay for this query these are the most relevant documents and what we are going to do next is we are going to put these documents to an uh llm and use this as context so we want to build a prompt that looks like this and this is what we are going to do in the next video see you soon In this video, I delve into the concept of retrieval, focusing on the rack framework's components - the database and the LLM. We start by using a simple search engine to search FAQ documents, illustrating search engine principles. Action: Watch the process of putting data into the search engine and performing a search to get answers.\",\n",
       "  'id': '8bb8fdd5f7ee287974c32614b442dac1'},\n",
       " {'vid_id': 'qz316T3U49Q',\n",
       "  'title': 'LLM Zoomcamp 1.4 - Generating Answers with gpt-4o',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"welcome back in this video we will talk about generation so previously we indexed our documents with a search engine now we can perform a search using a user user query and let me just quickly show you what we built so far so we have a query the course has already started can I still enroll and from our knowledge base we can retrieve the answers right now what we want to do is take this uh this potential this document from our knowledge base the documents that may or may not contain the answer we want to put them into an llm as a context to the question that the user has asked right so we will need now uh form a prompt and send it to open AI or some other llm so in this in this particular video we will use openi as an as an llm we'll use DPT 40 the newest one which is also reasonably fast and quite cheap compared to G pt4 so this is the llm we are going to use and the database is this mean search the toy search engine implementation um so I should already have configured um I have already configured the key so now I can just do for open AI import open AI so this is our client uh open AI so I don't don't need to do anything cuz I you put the key in the environmental variable like I showed in the preparing the configuration video and now we will use again chat completion API chat completion API and we will use uh GPT 40 and um so the pro the the question we will use is actually the same so this Q the course already started can I still enroll so this is what we will put right now for now in messages um so roll user content q and yeah we will put this to response\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': \"In this video, I demonstrate how we integrate OpenAI's gpt-4.o into our search engine to enhance user queries. We utilize documents from our knowledge base to create a prompt for the LLM, resulting in more accurate responses.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=qz316T3U49Q&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.4 - Generating Answers with gpt-4o welcome back in this video we will talk about generation so previously we indexed our documents with a search engine now we can perform a search using a user user query and let me just quickly show you what we built so far so we have a query the course has already started can I still enroll and from our knowledge base we can retrieve the answers right now what we want to do is take this uh this potential this document from our knowledge base the documents that may or may not contain the answer we want to put them into an llm as a context to the question that the user has asked right so we will need now uh form a prompt and send it to open AI or some other llm so in this in this particular video we will use openi as an as an llm we'll use DPT 40 the newest one which is also reasonably fast and quite cheap compared to G pt4 so this is the llm we are going to use and the database is this mean search the toy search engine implementation um so I should already have configured um I have already configured the key so now I can just do for open AI import open AI so this is our client uh open AI so I don't don't need to do anything cuz I you put the key in the environmental variable like I showed in the preparing the configuration video and now we will use again chat completion API chat completion API and we will use uh GPT 40 and um so the pro the the question we will use is actually the same so this Q the course already started can I still enroll so this is what we will put right now for now in messages um so roll user content q and yeah we will put this to response In this video, I demonstrate how we integrate OpenAI's gpt-4.o into our search engine to enhance user queries. We utilize documents from our knowledge base to create a prompt for the LLM, resulting in more accurate responses.\",\n",
       "  'id': '429b1b0a281f5ab1fde3fe498bbaf846'},\n",
       " {'vid_id': 'qz316T3U49Q',\n",
       "  'title': 'LLM Zoomcamp 1.4 - Generating Answers with gpt-4o',\n",
       "  'timecode': '02:27',\n",
       "  'text': \"and do I need to put um maybe minus here so it took a bit of time now let's see what response is and we'll look at choices then message content right so this is um the response again it just gives some stuff like very generic uh answer now we want to get this content context the answers we retrieved from the knowledge base and put it in the prompt uh so hopefully now our llm will include the the content with the context and give a better answer so our prompt uh can look like that prompt um I'll call Prompt template um so we will need to explain it what we do and usually a good practice uh for prompt engineering is we need to give it a rle um so we can say you're a course teaching assistant so this is the role that we give we assign into nlm and usually we need to do this for more advanced llms like gbt 4 gbt 3.5 uh for some smaller llms maybe we don't necessarily need that um so and this is U um it's more like Art and Science because you just need to do a lot of experimentation and see what works best for you um so first it's just Tri an\",\n",
       "  'timecode_text': 'Response Analysis',\n",
       "  'description': \"In this video, I demonstrate how we integrate OpenAI's gpt-4.o into our search engine to enhance user queries. We utilize documents from our knowledge base to create a prompt for the LLM, resulting in more accurate responses.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=qz316T3U49Q&t=147s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.4 - Generating Answers with gpt-4o and do I need to put um maybe minus here so it took a bit of time now let's see what response is and we'll look at choices then message content right so this is um the response again it just gives some stuff like very generic uh answer now we want to get this content context the answers we retrieved from the knowledge base and put it in the prompt uh so hopefully now our llm will include the the content with the context and give a better answer so our prompt uh can look like that prompt um I'll call Prompt template um so we will need to explain it what we do and usually a good practice uh for prompt engineering is we need to give it a rle um so we can say you're a course teaching assistant so this is the role that we give we assign into nlm and usually we need to do this for more advanced llms like gbt 4 gbt 3.5 uh for some smaller llms maybe we don't necessarily need that um so and this is U um it's more like Art and Science because you just need to do a lot of experimentation and see what works best for you um so first it's just Tri an In this video, I demonstrate how we integrate OpenAI's gpt-4.o into our search engine to enhance user queries. We utilize documents from our knowledge base to create a prompt for the LLM, resulting in more accurate responses.\",\n",
       "  'id': '3ed32f4e4527df471e489f84d811aa41'},\n",
       " {'vid_id': 'qz316T3U49Q',\n",
       "  'title': 'LLM Zoomcamp 1.4 - Generating Answers with gpt-4o',\n",
       "  'timecode': '04:28',\n",
       "  'text': \"error but then later later in the course we will also talk about different evaluation metrics so you will actually be able to numerically evaluate the goodness uh how well your prompt performs right now we'll just uh put this so your course teaching assistant answer the question and who will give the question here um question I don't remember with templates and python I think it's just one one CBR so answer the question based on the content and we will also provide content context context context um so we'll have context here and we can also say if the context doesn't have the answer um or like let me say it use only the facts from the context context when answering the the question and we can also add uh add L breaks if the context doesn't contain the answer contain contain the answer um output no something like this so we have a prompt template um I don't think we need to uh put anything else um let's add from the F data database we can play with this forever ever but this is just a good template right so this is a template and now we can add the question from the user and also the context let's build the context so\",\n",
       "  'timecode_text': 'Building a Prompt Template',\n",
       "  'description': \"In this video, I demonstrate how we integrate OpenAI's gpt-4.o into our search engine to enhance user queries. We utilize documents from our knowledge base to create a prompt for the LLM, resulting in more accurate responses.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=qz316T3U49Q&t=268s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.4 - Generating Answers with gpt-4o error but then later later in the course we will also talk about different evaluation metrics so you will actually be able to numerically evaluate the goodness uh how well your prompt performs right now we'll just uh put this so your course teaching assistant answer the question and who will give the question here um question I don't remember with templates and python I think it's just one one CBR so answer the question based on the content and we will also provide content context context context um so we'll have context here and we can also say if the context doesn't have the answer um or like let me say it use only the facts from the context context when answering the the question and we can also add uh add L breaks if the context doesn't contain the answer contain contain the answer um output no something like this so we have a prompt template um I don't think we need to uh put anything else um let's add from the F data database we can play with this forever ever but this is just a good template right so this is a template and now we can add the question from the user and also the context let's build the context so In this video, I demonstrate how we integrate OpenAI's gpt-4.o into our search engine to enhance user queries. We utilize documents from our knowledge base to create a prompt for the LLM, resulting in more accurate responses.\",\n",
       "  'id': 'ddbfa1f72f32c387eb8c1e1c985edebb'},\n",
       " {'vid_id': 'qz316T3U49Q',\n",
       "  'title': 'LLM Zoomcamp 1.4 - Generating Answers with gpt-4o',\n",
       "  'timecode': '06:42',\n",
       "  'text': \"I'll um start with an empty string and we will just iterate over um our documents for Doc in Posite results think was results right so now we just need to iterate over them and build this content content content plus um what do we have here um so let's say we have a section and Doc section then new line section then we have question um talk question and the new line answer um it's doc text and then again let's add to new lines of course context I for some reasons always want to put to write content and let me just print yeah it should be an F string um and if I print I should have new lines here so this is the content now we can put everything together uh how do format and also question is q and context is context right so this is St it okay so we don't have any any extra line breaks um and I'll just also step here so we don't have any line braks before line line breaks before and after so this is our prompt prompt and this is how it looks like Pro yeah print\",\n",
       "  'timecode_text': 'Building the Context',\n",
       "  'description': \"In this video, I demonstrate how we integrate OpenAI's gpt-4.o into our search engine to enhance user queries. We utilize documents from our knowledge base to create a prompt for the LLM, resulting in more accurate responses.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=qz316T3U49Q&t=402s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.4 - Generating Answers with gpt-4o I'll um start with an empty string and we will just iterate over um our documents for Doc in Posite results think was results right so now we just need to iterate over them and build this content content content plus um what do we have here um so let's say we have a section and Doc section then new line section then we have question um talk question and the new line answer um it's doc text and then again let's add to new lines of course context I for some reasons always want to put to write content and let me just print yeah it should be an F string um and if I print I should have new lines here so this is the content now we can put everything together uh how do format and also question is q and context is context right so this is St it okay so we don't have any any extra line breaks um and I'll just also step here so we don't have any line braks before line line breaks before and after so this is our prompt prompt and this is how it looks like Pro yeah print In this video, I demonstrate how we integrate OpenAI's gpt-4.o into our search engine to enhance user queries. We utilize documents from our knowledge base to create a prompt for the LLM, resulting in more accurate responses.\",\n",
       "  'id': '5137d696a43fa92a6bf5245cb8650867'},\n",
       " {'vid_id': 'qz316T3U49Q',\n",
       "  'title': 'LLM Zoomcamp 1.4 - Generating Answers with gpt-4o',\n",
       "  'timecode': '08:54',\n",
       "  'text': \"actually so we have the prompt we have the question we have the context and now let's try to put it into our um GPT put everything in one line here and the content would be prompt so let's see what GPT answers and here it says yes even if the course has already started you can still enoll and the homeworks however be mindful of the deadlines for touring in the final project so what we just accomplished is we took the documents that we retrieved from our knowledge base this is our context and based on that context we buil a prompt and we sent that prompt to our llm which is what we did here and we got back the answer which we return to the user this is all for this video in the\",\n",
       "  'timecode_text': 'Getting the Answer',\n",
       "  'description': \"In this video, I demonstrate how we integrate OpenAI's gpt-4.o into our search engine to enhance user queries. We utilize documents from our knowledge base to create a prompt for the LLM, resulting in more accurate responses.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=qz316T3U49Q&t=534s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.4 - Generating Answers with gpt-4o actually so we have the prompt we have the question we have the context and now let's try to put it into our um GPT put everything in one line here and the content would be prompt so let's see what GPT answers and here it says yes even if the course has already started you can still enoll and the homeworks however be mindful of the deadlines for touring in the final project so what we just accomplished is we took the documents that we retrieved from our knowledge base this is our context and based on that context we buil a prompt and we sent that prompt to our llm which is what we did here and we got back the answer which we return to the user this is all for this video in the In this video, I demonstrate how we integrate OpenAI's gpt-4.o into our search engine to enhance user queries. We utilize documents from our knowledge base to create a prompt for the LLM, resulting in more accurate responses.\",\n",
       "  'id': 'a2c76743dca1b5bae8dffcbc75275c6e'},\n",
       " {'vid_id': 'qz316T3U49Q',\n",
       "  'title': 'LLM Zoomcamp 1.4 - Generating Answers with gpt-4o',\n",
       "  'timecode': '09:54',\n",
       "  'text': \"next video I want to put everything together clean it a little bit um have a like put all the logic inside different functions and have it more modular so later when we want to replace database this mean uh mean search with elastic search it's easy to do and also if we want to replace an llm uh open llm with something else it's also easy to do see you soon\",\n",
       "  'timecode_text': 'Conclusions',\n",
       "  'description': \"In this video, I demonstrate how we integrate OpenAI's gpt-4.o into our search engine to enhance user queries. We utilize documents from our knowledge base to create a prompt for the LLM, resulting in more accurate responses.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=qz316T3U49Q&t=594s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.4 - Generating Answers with gpt-4o next video I want to put everything together clean it a little bit um have a like put all the logic inside different functions and have it more modular so later when we want to replace database this mean uh mean search with elastic search it's easy to do and also if we want to replace an llm uh open llm with something else it's also easy to do see you soon In this video, I demonstrate how we integrate OpenAI's gpt-4.o into our search engine to enhance user queries. We utilize documents from our knowledge base to create a prompt for the LLM, resulting in more accurate responses.\",\n",
       "  'id': 'ee7d1816cc8ace1091ac0bd95b8a7a77'},\n",
       " {'vid_id': 'HObjFso2UJE',\n",
       "  'title': 'LLM Zoomcamp 1.4.2 - Exploring Alternatives to OpenAI',\n",
       "  'timecode': '0:00',\n",
       "  'text': \"a very common question people I see people ask in slack is if we can use anything else apart from open ey yes and we will cover open source models in module two but there are other services that we can use instead of open AI that provide very similar kind of uh service like an API too that you don't need to host yourself you can just send a prompt and get back a response very similar to open Ai and I want in this video to talk about Alternatives and I want to ask you to provide a list of different Alternatives I already saw some in chat in slack and let's build a list of Alternatives together and for that I created a markdown page in our uh first module and\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, I discuss various alternatives to OpenAI for our project. I highlight services like Mistral AI and mention the availability of free trials and credits. I request viewers to contribute to a list of alternatives by sending pull requests. Action requested: Provide a list of different alternatives.',\n",
       "  'link': 'https://www.youtube.com/watch?v=HObjFso2UJE&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.4.2 - Exploring Alternatives to OpenAI a very common question people I see people ask in slack is if we can use anything else apart from open ey yes and we will cover open source models in module two but there are other services that we can use instead of open AI that provide very similar kind of uh service like an API too that you don't need to host yourself you can just send a prompt and get back a response very similar to open Ai and I want in this video to talk about Alternatives and I want to ask you to provide a list of different Alternatives I already saw some in chat in slack and let's build a list of Alternatives together and for that I created a markdown page in our uh first module and In this video, I discuss various alternatives to OpenAI for our project. I highlight services like Mistral AI and mention the availability of free trials and credits. I request viewers to contribute to a list of alternatives by sending pull requests. Action requested: Provide a list of different alternatives.\",\n",
       "  'id': '15cd7cc957963f3f08cb0af0af367959'},\n",
       " {'vid_id': 'HObjFso2UJE',\n",
       "  'title': 'LLM Zoomcamp 1.4.2 - Exploring Alternatives to OpenAI',\n",
       "  'timecode': '0:51',\n",
       "  'text': \"this is uh open AI Alternatives I already added one mistal AI in mistal um yeah I need to add HTTP but let me just I'll fix that but if you go to mistal there is this La platform sorry for my French and you can sign up for a free trial they will give you like five EUR uh on your plan and there are many many other services like mistal that we can also use instead of open so if for some reasons you don't want to sign out for uh to create an account with open AI or there are Ser services that provide some free credit credit that you can use uh during the course um meaning that you will be able to just do the course for free let's build a list here uh send your pool requests and you're free to explore and try other things um I've already seen in slack uh some other platform I don't remember the name and and somebody said that um I think it was aradi uh he said that you can also use for example a yes uh it was Bedrock I think the service is called they have a bunch of llms too and then it's fairly easy to find online um Ed yes credits like $25 for example um yeah so let's build a list like that and we don't have to restrict ourselves to just open a\",\n",
       "  'timecode_text': 'Mistral AI, AWS and Bedrock',\n",
       "  'description': 'In this video, I discuss various alternatives to OpenAI for our project. I highlight services like Mistral AI and mention the availability of free trials and credits. I request viewers to contribute to a list of alternatives by sending pull requests. Action requested: Provide a list of different alternatives.',\n",
       "  'link': 'https://www.youtube.com/watch?v=HObjFso2UJE&t=51s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.4.2 - Exploring Alternatives to OpenAI this is uh open AI Alternatives I already added one mistal AI in mistal um yeah I need to add HTTP but let me just I'll fix that but if you go to mistal there is this La platform sorry for my French and you can sign up for a free trial they will give you like five EUR uh on your plan and there are many many other services like mistal that we can also use instead of open so if for some reasons you don't want to sign out for uh to create an account with open AI or there are Ser services that provide some free credit credit that you can use uh during the course um meaning that you will be able to just do the course for free let's build a list here uh send your pool requests and you're free to explore and try other things um I've already seen in slack uh some other platform I don't remember the name and and somebody said that um I think it was aradi uh he said that you can also use for example a yes uh it was Bedrock I think the service is called they have a bunch of llms too and then it's fairly easy to find online um Ed yes credits like $25 for example um yeah so let's build a list like that and we don't have to restrict ourselves to just open a In this video, I discuss various alternatives to OpenAI for our project. I highlight services like Mistral AI and mention the availability of free trials and credits. I request viewers to contribute to a list of alternatives by sending pull requests. Action requested: Provide a list of different alternatives.\",\n",
       "  'id': 'db120ba3f514f112f5602a642fa76685'},\n",
       " {'vid_id': 'vkTiVwwch6A',\n",
       "  'title': 'LLM Zoomcamp 1.5 - The RAG Flow Cleaning and Modularizing Code',\n",
       "  'timecode': '0:00',\n",
       "  'text': \"welcome back in this video I want to show you how to clean the code we wrot so far make it modular make it easier to use and then replace different components later as we go along the course so this is what we this is the rec framework we send a query to our index uh to our search which is a Min search implementation then we build the prompt send it to an llm in our case GPT 4 40 and then get the answer and this is the implementation that we made in the last couple of videos so here we index everything we retrieve the results uh we send we built the prompt and then we send the prompt to open a so now I want to put everything together so first the first function I want to clean it make it more modular put everything together and break it down into simple functions so clean uh the code we have wrot so far so I'll first start\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, I demonstrate how to clean and modularize our existing code, making it more manageable and preparing it for component replacements. I break down the process into simple functions, starting with cleaning the search function and then structuring the prompt creation.',\n",
       "  'link': 'https://www.youtube.com/watch?v=vkTiVwwch6A&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.5 - The RAG Flow Cleaning and Modularizing Code welcome back in this video I want to show you how to clean the code we wrot so far make it modular make it easier to use and then replace different components later as we go along the course so this is what we this is the rec framework we send a query to our index uh to our search which is a Min search implementation then we build the prompt send it to an llm in our case GPT 4 40 and then get the answer and this is the implementation that we made in the last couple of videos so here we index everything we retrieve the results uh we send we built the prompt and then we send the prompt to open a so now I want to put everything together so first the first function I want to clean it make it more modular put everything together and break it down into simple functions so clean uh the code we have wrot so far so I'll first start In this video, I demonstrate how to clean and modularize our existing code, making it more manageable and preparing it for component replacements. I break down the process into simple functions, starting with cleaning the search function and then structuring the prompt creation.\",\n",
       "  'id': '5a16b9f3bf2cb1b94f3db39d4aa0d095'},\n",
       " {'vid_id': 'vkTiVwwch6A',\n",
       "  'title': 'LLM Zoomcamp 1.5 - The RAG Flow Cleaning and Modularizing Code',\n",
       "  'timecode': '1:05',\n",
       "  'text': \"with a function called search and here we will have query um so here I will basically take this piece of code and put it here so this is what we have and I'll just return the results and of course here instead of Q we have query and let me just quickly check that it works how do I run Kafka and then it retrieves some questions about Kafka and here yeah uh we limit our answers only to dat engineering zom Camp course so this is our search function then the next function is um we want to\",\n",
       "  'timecode_text': 'Search',\n",
       "  'description': 'In this video, I demonstrate how to clean and modularize our existing code, making it more manageable and preparing it for component replacements. I break down the process into simple functions, starting with cleaning the search function and then structuring the prompt creation.',\n",
       "  'link': 'https://www.youtube.com/watch?v=vkTiVwwch6A&t=65s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.5 - The RAG Flow Cleaning and Modularizing Code with a function called search and here we will have query um so here I will basically take this piece of code and put it here so this is what we have and I'll just return the results and of course here instead of Q we have query and let me just quickly check that it works how do I run Kafka and then it retrieves some questions about Kafka and here yeah uh we limit our answers only to dat engineering zom Camp course so this is our search function then the next function is um we want to In this video, I demonstrate how to clean and modularize our existing code, making it more manageable and preparing it for component replacements. I break down the process into simple functions, starting with cleaning the search function and then structuring the prompt creation.\",\n",
       "  'id': '15080ba748b4187a132801cd0d7ff038'},\n",
       " {'vid_id': 'vkTiVwwch6A',\n",
       "  'title': 'LLM Zoomcamp 1.5 - The RAG Flow Cleaning and Modularizing Code',\n",
       "  'timecode': '2:12',\n",
       "  'text': \"turn let make sure put it back so we want to turn this documents uh results we want to turn these documents into a prompt so we have we have some code here for that I'll just put it together [Music] and over here they built prompt so here we have we need to have a query and we need to have the context right so we have the prom plate we have the um let's actually call it search results and then yeah the content then we have the prompt and this is what we return prompt let's see if it works so I'll put this into query query so this one works this one query and search results so we have the The Prompt prompt and now we need to to put this prompt let it here I'll keep everything in one line now let me also put the logic for invoking the GPT I'll clean up a little bit uh so we don't need this anymore\",\n",
       "  'timecode_text': 'Building the Prompt',\n",
       "  'description': 'In this video, I demonstrate how to clean and modularize our existing code, making it more manageable and preparing it for component replacements. I break down the process into simple functions, starting with cleaning the search function and then structuring the prompt creation.',\n",
       "  'link': 'https://www.youtube.com/watch?v=vkTiVwwch6A&t=132s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.5 - The RAG Flow Cleaning and Modularizing Code turn let make sure put it back so we want to turn this documents uh results we want to turn these documents into a prompt so we have we have some code here for that I'll just put it together [Music] and over here they built prompt so here we have we need to have a query and we need to have the context right so we have the prom plate we have the um let's actually call it search results and then yeah the content then we have the prompt and this is what we return prompt let's see if it works so I'll put this into query query so this one works this one query and search results so we have the The Prompt prompt and now we need to to put this prompt let it here I'll keep everything in one line now let me also put the logic for invoking the GPT I'll clean up a little bit uh so we don't need this anymore In this video, I demonstrate how to clean and modularize our existing code, making it more manageable and preparing it for component replacements. I break down the process into simple functions, starting with cleaning the search function and then structuring the prompt creation.\",\n",
       "  'id': 'b1184877d681acb4d10c24a526f989ac'},\n",
       " {'vid_id': 'vkTiVwwch6A',\n",
       "  'title': 'LLM Zoomcamp 1.5 - The RAG Flow Cleaning and Modularizing Code',\n",
       "  'timecode': '3:57',\n",
       "  'text': \"um so I'll take this one so this will be llm so llm Gets In The Prompt and gives back some answer so this is what we do so now answer LM\",\n",
       "  'timecode_text': 'LLM',\n",
       "  'description': 'In this video, I demonstrate how to clean and modularize our existing code, making it more manageable and preparing it for component replacements. I break down the process into simple functions, starting with cleaning the search function and then structuring the prompt creation.',\n",
       "  'link': 'https://www.youtube.com/watch?v=vkTiVwwch6A&t=237s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.5 - The RAG Flow Cleaning and Modularizing Code um so I'll take this one so this will be llm so llm Gets In The Prompt and gives back some answer so this is what we do so now answer LM In this video, I demonstrate how to clean and modularize our existing code, making it more manageable and preparing it for component replacements. I break down the process into simple functions, starting with cleaning the search function and then structuring the prompt creation.\",\n",
       "  'id': '0e54cba5c95e2d13cc14211cc9dfbbd3'},\n",
       " {'vid_id': 'vkTiVwwch6A',\n",
       "  'title': 'LLM Zoomcamp 1.5 - The RAG Flow Cleaning and Modularizing Code',\n",
       "  'timecode': '4:30',\n",
       "  'text': \"so we get in the user query we perform the search we build a prompt from the search results and we get the answer from the LM and this is nothing else but an an implementation of this flow so now each of the steps we expressed in functions and let us try to execute that so what does answer contain yeah well probably there is no answer to that question uh but um let's try how do I enroll the course just started how do I enroll\",\n",
       "  'timecode_text': 'The RAG flow',\n",
       "  'description': 'In this video, I demonstrate how to clean and modularize our existing code, making it more manageable and preparing it for component replacements. I break down the process into simple functions, starting with cleaning the search function and then structuring the prompt creation.',\n",
       "  'link': 'https://www.youtube.com/watch?v=vkTiVwwch6A&t=270s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.5 - The RAG Flow Cleaning and Modularizing Code so we get in the user query we perform the search we build a prompt from the search results and we get the answer from the LM and this is nothing else but an an implementation of this flow so now each of the steps we expressed in functions and let us try to execute that so what does answer contain yeah well probably there is no answer to that question uh but um let's try how do I enroll the course just started how do I enroll In this video, I demonstrate how to clean and modularize our existing code, making it more manageable and preparing it for component replacements. I break down the process into simple functions, starting with cleaning the search function and then structuring the prompt creation.\",\n",
       "  'id': '325e4b1ca606548dc5599206c07a4691'},\n",
       " {'vid_id': 'vkTiVwwch6A',\n",
       "  'title': 'LLM Zoomcamp 1.5 - The RAG Flow Cleaning and Modularizing Code',\n",
       "  'timecode': '5:16',\n",
       "  'text': \"okay I don't know maybe let's play with this a little bit I'll change the prompt slightly okay yeah well it's not correct I don't know we probably need to deac let's see if the search results actually contain um the questions what can I do before the course can I fold the course after it finishes yeah the course already started can I enroll yeah good it works well as you see there is still some room for improvement so probably if we played with this and added a little bit more uh results we can actually do that right now by changing the number of results to 10 um and what I wanted to ask like how do I run Kafka I don't understand yeah let me see I think we have a we have made a mistake somewhere results uh yeah I think this is the error this is the problem so query should be not q but query context is context and let us execute things one more time yeah well now the the the the mystery solved I was just not careful we can we can play with this parameters to so let me see if five answers is actually enough yeah this is enough so now what we can do is put everything into a simple\",\n",
       "  'timecode_text': 'Debugging',\n",
       "  'description': 'In this video, I demonstrate how to clean and modularize our existing code, making it more manageable and preparing it for component replacements. I break down the process into simple functions, starting with cleaning the search function and then structuring the prompt creation.',\n",
       "  'link': 'https://www.youtube.com/watch?v=vkTiVwwch6A&t=316s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.5 - The RAG Flow Cleaning and Modularizing Code okay I don't know maybe let's play with this a little bit I'll change the prompt slightly okay yeah well it's not correct I don't know we probably need to deac let's see if the search results actually contain um the questions what can I do before the course can I fold the course after it finishes yeah the course already started can I enroll yeah good it works well as you see there is still some room for improvement so probably if we played with this and added a little bit more uh results we can actually do that right now by changing the number of results to 10 um and what I wanted to ask like how do I run Kafka I don't understand yeah let me see I think we have a we have made a mistake somewhere results uh yeah I think this is the error this is the problem so query should be not q but query context is context and let us execute things one more time yeah well now the the the the mystery solved I was just not careful we can we can play with this parameters to so let me see if five answers is actually enough yeah this is enough so now what we can do is put everything into a simple In this video, I demonstrate how to clean and modularize our existing code, making it more manageable and preparing it for component replacements. I break down the process into simple functions, starting with cleaning the search function and then structuring the prompt creation.\",\n",
       "  'id': '4fbb4d5cb5d1d473ebbdc2ad5950a9d4'},\n",
       " {'vid_id': 'vkTiVwwch6A',\n",
       "  'title': 'LLM Zoomcamp 1.5 - The RAG Flow Cleaning and Modularizing Code',\n",
       "  'timecode': '7:35',\n",
       "  'text': \"function that we can call Ro and then now it's even simpler we just do rock query and then it should give us the answers uh the core has already started can I still en roll yeah so now we have put everything together and you can see that now it's pretty straightforward to play with different components so now for example if instead of uh Min search we want to use elastic search what we need to do is just replace this function with something else and we will have elastic search or if we want to have a different llm we would replace this function we'll probably need to change this one a little bit but now we can put the this rack framework into code and now it's way easier to understand what EXA exactly is happening and each of these um functions is independent so we can plug and play how we want okay so that's all for now and in the next video I want to show you how to replace Min search with elastic search\",\n",
       "  'timecode_text': 'The RAG flow function',\n",
       "  'description': 'In this video, I demonstrate how to clean and modularize our existing code, making it more manageable and preparing it for component replacements. I break down the process into simple functions, starting with cleaning the search function and then structuring the prompt creation.',\n",
       "  'link': 'https://www.youtube.com/watch?v=vkTiVwwch6A&t=455s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.5 - The RAG Flow Cleaning and Modularizing Code function that we can call Ro and then now it's even simpler we just do rock query and then it should give us the answers uh the core has already started can I still en roll yeah so now we have put everything together and you can see that now it's pretty straightforward to play with different components so now for example if instead of uh Min search we want to use elastic search what we need to do is just replace this function with something else and we will have elastic search or if we want to have a different llm we would replace this function we'll probably need to change this one a little bit but now we can put the this rack framework into code and now it's way easier to understand what EXA exactly is happening and each of these um functions is independent so we can plug and play how we want okay so that's all for now and in the next video I want to show you how to replace Min search with elastic search In this video, I demonstrate how to clean and modularize our existing code, making it more manageable and preparing it for component replacements. I break down the process into simple functions, starting with cleaning the search function and then structuring the prompt creation.\",\n",
       "  'id': 'a1c245146102682a39539caeeb499503'},\n",
       " {'vid_id': '1lgbR5wMvsI',\n",
       "  'title': 'LLM Zoomcamp 1.6 - Search with Elasticsearch',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"welcome back this is llm zoom camp and in this video we are going to replace the toy search engine we used previously with elastic search quick recap of what we have done so far we have a knowledge base which we index with a small uh Library search engine and now with user query we can retrieve the relevant document build the prompt and send this prompt to nlm and TM will give us an answer and we already have the code for that so this is what we implemented previously and now and one of these uh things like search uses a toy search engine that we implemented in a precourse workshop now we want to replace it with a a proper search system with elastic search uh but why did we actually use this toy search engine in the first place you may wonder and it's because later in the course we might use tools uh like sat Cloud where it's uh not easy to run elastic search so having something that runs in memory and gives good results is um a good thing plus we wanted to see how to implement the search engine so this is why we use it previously but now let's replace it with something real life something you will actually uh probably use in practice okay so this is the thing we want to replace the search with something else so let's do that\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, I demonstrate how we transition from a toy search engine to Elasticsearch for better search results. We discuss the importance of using Elasticsearch for efficient querying and indexing. Action requested: Watch the process of setting up Elasticsearch and creating an index for our data.',\n",
       "  'link': 'https://www.youtube.com/watch?v=1lgbR5wMvsI&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.6 - Search with Elasticsearch welcome back this is llm zoom camp and in this video we are going to replace the toy search engine we used previously with elastic search quick recap of what we have done so far we have a knowledge base which we index with a small uh Library search engine and now with user query we can retrieve the relevant document build the prompt and send this prompt to nlm and TM will give us an answer and we already have the code for that so this is what we implemented previously and now and one of these uh things like search uses a toy search engine that we implemented in a precourse workshop now we want to replace it with a a proper search system with elastic search uh but why did we actually use this toy search engine in the first place you may wonder and it's because later in the course we might use tools uh like sat Cloud where it's uh not easy to run elastic search so having something that runs in memory and gives good results is um a good thing plus we wanted to see how to implement the search engine so this is why we use it previously but now let's replace it with something real life something you will actually uh probably use in practice okay so this is the thing we want to replace the search with something else so let's do that In this video, I demonstrate how we transition from a toy search engine to Elasticsearch for better search results. We discuss the importance of using Elasticsearch for efficient querying and indexing. Action requested: Watch the process of setting up Elasticsearch and creating an index for our data.\",\n",
       "  'id': '716f38312bf1989e8eba7a48280a214d'},\n",
       " {'vid_id': '1lgbR5wMvsI',\n",
       "  'title': 'LLM Zoomcamp 1.6 - Search with Elasticsearch',\n",
       "  'timecode': '01:30',\n",
       "  'text': \"first of all we want to run elastic search right now I'm in my code spaces and I have this command for running elastic search in Docker so this is running elastic search of this version I don't know if it's the most recent or not now I'll open a new terminal and I will execute this comment so I just copy it here so elastic search will run on these two parts and uh now it will download the docker image and start elastic search and okay I don't [Music] know yeah whatever we just need to check that our ports the port that we need are forwarded and these are the two ports that we actually care about so we will use uh this port 9,200 so I'll create another uh terminal and execute a simple HTTP get request uh to Local Host uh this so this is the URL I don't know why I cut it it's Local Host uh 92000 this is where elastic search is running and we see a response which means that it works and actually we don't really care about uh this port being forward to our local machine to our host machine because we are going to use elastic search only within Cod spaces but anyways it forwards it okay and now we can see that it works so now let us um first we will index these documents that we have here let me remind you these are the documents we have so now we want to index them with\",\n",
       "  'timecode_text': 'Running Elasticsearch',\n",
       "  'description': 'In this video, I demonstrate how we transition from a toy search engine to Elasticsearch for better search results. We discuss the importance of using Elasticsearch for efficient querying and indexing. Action requested: Watch the process of setting up Elasticsearch and creating an index for our data.',\n",
       "  'link': 'https://www.youtube.com/watch?v=1lgbR5wMvsI&t=90s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.6 - Search with Elasticsearch first of all we want to run elastic search right now I'm in my code spaces and I have this command for running elastic search in Docker so this is running elastic search of this version I don't know if it's the most recent or not now I'll open a new terminal and I will execute this comment so I just copy it here so elastic search will run on these two parts and uh now it will download the docker image and start elastic search and okay I don't [Music] know yeah whatever we just need to check that our ports the port that we need are forwarded and these are the two ports that we actually care about so we will use uh this port 9,200 so I'll create another uh terminal and execute a simple HTTP get request uh to Local Host uh this so this is the URL I don't know why I cut it it's Local Host uh 92000 this is where elastic search is running and we see a response which means that it works and actually we don't really care about uh this port being forward to our local machine to our host machine because we are going to use elastic search only within Cod spaces but anyways it forwards it okay and now we can see that it works so now let us um first we will index these documents that we have here let me remind you these are the documents we have so now we want to index them with In this video, I demonstrate how we transition from a toy search engine to Elasticsearch for better search results. We discuss the importance of using Elasticsearch for efficient querying and indexing. Action requested: Watch the process of setting up Elasticsearch and creating an index for our data.\",\n",
       "  'id': 'c459c28180d0de87257d163d956b0a1d'},\n",
       " {'vid_id': '1lgbR5wMvsI',\n",
       "  'title': 'LLM Zoomcamp 1.6 - Search with Elasticsearch',\n",
       "  'timecode': '03:43',\n",
       "  'text': \"elastic search and later use this for searching the thing about elastic search um is that it's actually persistent meaning that unlike this toy search engine uh when our machine stops when we finish this Jupiter notebook process we need to basically next time we start we need to rebuild the whole index but when it comes to elastic search it saves all the data on disk next time we start elastic search it will have all the data we need so we will not need to ex to executed of course it depends on how exactly we run it so we might need to do volume Ming for that this is outside of the scope for this course if you want to learn Docker in more detail you can check our data engineering course module one where we talk about Docker in a lot of detail right now let us see how we can use it from uh from our Jupiter so we want to replace this line here so let's do that first I'll import elastic search um so we should have installed it so from elastic search import elastic search I hope I typed it correctly elastic search yeah so I did from the second attempt so now let's create elastic search client which can be elastic search and then the URL which in our case HTTP Local Host uh 92 0 if you deploy elastic search somewhere in the cloud uh then you can just use the the Euro you get but um yeah this is the local deployment and we can check that things work by executing this info function so this is the same response as we got as we saw before in the terminal okay so it works we can connect from our Jupiter to uh elastic search that we run Docker and now we need to create a an index so an index in elastic search is like a table in a relational database and it's very similar to what we have created previously with M search and usually the Syntax for that is rather complicated so I already prepared this query here so this is our index settings and here the interesting thing is this the properties so course is keyword we will do filtering and the rest are the type R of the type text so this is um very similar to what uh we have done with u mean search right so this first three were text and of course was keyword and now we need to so these are the settings so now we need to give the name of the index so let's call it course questions and last thing is we will need to use elastic search client to create an index so need to do something like indices create and then index will be index name and uh the the settings will be this index settings um so we execute that and yeah from what we saw it went well the index was created and we can now index the data with this with this Index right and um the way it works is we again use elastic search client then we have uh the index function and then we need to specify the index name which will be our course questions and then the document that we want to index so the document that we want to index the look like that right so we basically need now to iterate over all the documents we have and for each of the document index it with plastic search um we have rather small data set so this should be very fast for document in documents uh that's all we need to do to to index our data set and I also like to add progress bar to that so this operation might take some time so want to see what is the progress and that's why we installed this tqdm Library so the way I usually import it is tqdm Auto Import tqdm yeah well apparently I have not installed some library but whatever we will still have a progress bar yeah it's not just offensive progress bar if I installed python IPI widgets it would look nicer but we can still see the progress that uh we index at roughly at the speed of roughly 30 documents per second which is quite good so yeah almost the indexing is\",\n",
       "  'timecode_text': 'Indexing Data',\n",
       "  'description': 'In this video, I demonstrate how we transition from a toy search engine to Elasticsearch for better search results. We discuss the importance of using Elasticsearch for efficient querying and indexing. Action requested: Watch the process of setting up Elasticsearch and creating an index for our data.',\n",
       "  'link': 'https://www.youtube.com/watch?v=1lgbR5wMvsI&t=223s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.6 - Search with Elasticsearch elastic search and later use this for searching the thing about elastic search um is that it's actually persistent meaning that unlike this toy search engine uh when our machine stops when we finish this Jupiter notebook process we need to basically next time we start we need to rebuild the whole index but when it comes to elastic search it saves all the data on disk next time we start elastic search it will have all the data we need so we will not need to ex to executed of course it depends on how exactly we run it so we might need to do volume Ming for that this is outside of the scope for this course if you want to learn Docker in more detail you can check our data engineering course module one where we talk about Docker in a lot of detail right now let us see how we can use it from uh from our Jupiter so we want to replace this line here so let's do that first I'll import elastic search um so we should have installed it so from elastic search import elastic search I hope I typed it correctly elastic search yeah so I did from the second attempt so now let's create elastic search client which can be elastic search and then the URL which in our case HTTP Local Host uh 92 0 if you deploy elastic search somewhere in the cloud uh then you can just use the the Euro you get but um yeah this is the local deployment and we can check that things work by executing this info function so this is the same response as we got as we saw before in the terminal okay so it works we can connect from our Jupiter to uh elastic search that we run Docker and now we need to create a an index so an index in elastic search is like a table in a relational database and it's very similar to what we have created previously with M search and usually the Syntax for that is rather complicated so I already prepared this query here so this is our index settings and here the interesting thing is this the properties so course is keyword we will do filtering and the rest are the type R of the type text so this is um very similar to what uh we have done with u mean search right so this first three were text and of course was keyword and now we need to so these are the settings so now we need to give the name of the index so let's call it course questions and last thing is we will need to use elastic search client to create an index so need to do something like indices create and then index will be index name and uh the the settings will be this index settings um so we execute that and yeah from what we saw it went well the index was created and we can now index the data with this with this Index right and um the way it works is we again use elastic search client then we have uh the index function and then we need to specify the index name which will be our course questions and then the document that we want to index so the document that we want to index the look like that right so we basically need now to iterate over all the documents we have and for each of the document index it with plastic search um we have rather small data set so this should be very fast for document in documents uh that's all we need to do to to index our data set and I also like to add progress bar to that so this operation might take some time so want to see what is the progress and that's why we installed this tqdm Library so the way I usually import it is tqdm Auto Import tqdm yeah well apparently I have not installed some library but whatever we will still have a progress bar yeah it's not just offensive progress bar if I installed python IPI widgets it would look nicer but we can still see the progress that uh we index at roughly at the speed of roughly 30 documents per second which is quite good so yeah almost the indexing is In this video, I demonstrate how we transition from a toy search engine to Elasticsearch for better search results. We discuss the importance of using Elasticsearch for efficient querying and indexing. Action requested: Watch the process of setting up Elasticsearch and creating an index for our data.\",\n",
       "  'id': '564fa875f34aa279091f99cc802221a3'},\n",
       " {'vid_id': '1lgbR5wMvsI',\n",
       "  'title': 'LLM Zoomcamp 1.6 - Search with Elasticsearch',\n",
       "  'timecode': '09:23',\n",
       "  'text': \"almost over um and now we can query this uh this data and again the query I have I had to prepare it in advance because it's rather complicated to type I'll explain what's happening here um let's call it search query and um do we already have a query defined query yeah how do run cka let's use the same example as we did previously is how um I just disced the course can I still join it right so this will be our query and um this syntax is rather complicated and then let's look at what we have here so there are two components the first component is the filtering component so this is where it's very similar it's actually the same as we did before so it's very similar it's analogous to this select query where the course is data engineering Zoom Camp so this is what the filter is doing so we limit ourselves only to the questions from the data engine cam course and then this part is when we do perform text matching and here notice this hat so we say with this hat three power three means that question is three times more important than text or section field right so essentially what we're doing here is we're giving more importance to the question field we're giving it a post this is exactly the same thing as we did before in our M search uh and the size is five so we will get only five uh only five answers in the result okay so this is what we do so this is the query this is the search query we sent to elastic search and the way we do the way we use it is we invoke the client using the search method and we need to specify the index name and we need to specify the query search query and we get the results so let me now write into the response variable and response would be so we need to actually uh somehow get the relevant part here so they it is hidden under like a lot of um like it's quite nested so I think first we need to go to hits and then inside hits we need to have hits again and yeah so this is our um documents and I can see that for each hit we actually have to go inside and get to the source right so actually we need to have a loop for heat in hits so let me result looks we have a variable here uh append uh hit Source now result dox contains the documents yeah I think this is exactly the same as we had before uh well I'm not sure about uh I think we are asking about Kafka right uh let me just execute it one more time so the questions are different the answers are different um so yeah this is the answers we get from elastic search\",\n",
       "  'timecode_text': 'Querying Data',\n",
       "  'description': 'In this video, I demonstrate how we transition from a toy search engine to Elasticsearch for better search results. We discuss the importance of using Elasticsearch for efficient querying and indexing. Action requested: Watch the process of setting up Elasticsearch and creating an index for our data.',\n",
       "  'link': 'https://www.youtube.com/watch?v=1lgbR5wMvsI&t=563s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.6 - Search with Elasticsearch almost over um and now we can query this uh this data and again the query I have I had to prepare it in advance because it's rather complicated to type I'll explain what's happening here um let's call it search query and um do we already have a query defined query yeah how do run cka let's use the same example as we did previously is how um I just disced the course can I still join it right so this will be our query and um this syntax is rather complicated and then let's look at what we have here so there are two components the first component is the filtering component so this is where it's very similar it's actually the same as we did before so it's very similar it's analogous to this select query where the course is data engineering Zoom Camp so this is what the filter is doing so we limit ourselves only to the questions from the data engine cam course and then this part is when we do perform text matching and here notice this hat so we say with this hat three power three means that question is three times more important than text or section field right so essentially what we're doing here is we're giving more importance to the question field we're giving it a post this is exactly the same thing as we did before in our M search uh and the size is five so we will get only five uh only five answers in the result okay so this is what we do so this is the query this is the search query we sent to elastic search and the way we do the way we use it is we invoke the client using the search method and we need to specify the index name and we need to specify the query search query and we get the results so let me now write into the response variable and response would be so we need to actually uh somehow get the relevant part here so they it is hidden under like a lot of um like it's quite nested so I think first we need to go to hits and then inside hits we need to have hits again and yeah so this is our um documents and I can see that for each hit we actually have to go inside and get to the source right so actually we need to have a loop for heat in hits so let me result looks we have a variable here uh append uh hit Source now result dox contains the documents yeah I think this is exactly the same as we had before uh well I'm not sure about uh I think we are asking about Kafka right uh let me just execute it one more time so the questions are different the answers are different um so yeah this is the answers we get from elastic search In this video, I demonstrate how we transition from a toy search engine to Elasticsearch for better search results. We discuss the importance of using Elasticsearch for efficient querying and indexing. Action requested: Watch the process of setting up Elasticsearch and creating an index for our data.\",\n",
       "  'id': '528076e97249949a1611062ee62acf5c'},\n",
       " {'vid_id': '1lgbR5wMvsI',\n",
       "  'title': 'LLM Zoomcamp 1.6 - Search with Elasticsearch',\n",
       "  'timecode': '13:38',\n",
       "  'text': \"and yeah like we don't actually need to do much now so now let's just put this in I'll put it in function I'll call it search uh maybe elastic search right and then we have a one perameter query and then we just put everything in this function right and then we just return the result documents atend so let's quickly test it uh elastic search I'll test it here this query so we get some results and now let's quickly adjust our rck function so I'll just replace elastics I'll just replace search with lastic search and now when I execute Rock we should go like go through the same rack flow and then get the answer okay so this is how we replace um one component here like we replace our toy search engine with a proper search engine and you can see that this is highly modular so we can easily swap like all the components and later in the course we will also show you how to replace this llm the open AI one that we used with an Open Source One okay that's it for this video and I don't know for this week we will have a video where we show how to run it locally most likely we will save it for the next week so have fun and see you around\",\n",
       "  'timecode_text': 'Updating the RAG flow',\n",
       "  'description': 'In this video, I demonstrate how we transition from a toy search engine to Elasticsearch for better search results. We discuss the importance of using Elasticsearch for efficient querying and indexing. Action requested: Watch the process of setting up Elasticsearch and creating an index for our data.',\n",
       "  'link': 'https://www.youtube.com/watch?v=1lgbR5wMvsI&t=818s',\n",
       "  'text_vector': \"LLM Zoomcamp 1.6 - Search with Elasticsearch and yeah like we don't actually need to do much now so now let's just put this in I'll put it in function I'll call it search uh maybe elastic search right and then we have a one perameter query and then we just put everything in this function right and then we just return the result documents atend so let's quickly test it uh elastic search I'll test it here this query so we get some results and now let's quickly adjust our rck function so I'll just replace elastics I'll just replace search with lastic search and now when I execute Rock we should go like go through the same rack flow and then get the answer okay so this is how we replace um one component here like we replace our toy search engine with a proper search engine and you can see that this is highly modular so we can easily swap like all the components and later in the course we will also show you how to replace this llm the open AI one that we used with an Open Source One okay that's it for this video and I don't know for this week we will have a video where we show how to run it locally most likely we will save it for the next week so have fun and see you around In this video, I demonstrate how we transition from a toy search engine to Elasticsearch for better search results. We discuss the importance of using Elasticsearch for efficient querying and indexing. Action requested: Watch the process of setting up Elasticsearch and creating an index for our data.\",\n",
       "  'id': '678ea6e250b314c3c07aa7e5c68c2478'},\n",
       " {'vid_id': 'ATchkIRsH4g',\n",
       "  'title': 'LLM Zoomcamp 2.1 - Open Source LLMs',\n",
       "  'timecode': '0:00',\n",
       "  'text': \"welcome back this is module two of llm Zoom camp and in this module we are going to explore alternatives to open Ai and in particular we're going to talk about open source llms that we can host ourselves on our own computers or somewhere but we have full control over the model we can execute this model on our Hardware so we will explore different options of different models there are a lot of open source\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': \"In this module of LLM Zoomcamp, we'll look into open source LLMs that offer full control over the model execution. We explore various alternatives to GPT, including models like LLAMA from Meta, and Mistral. The focus is on setting up environments, running models, and accessing them from platforms like HuggingFace hub.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=ATchkIRsH4g&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.1 - Open Source LLMs welcome back this is module two of llm Zoom camp and in this module we are going to explore alternatives to open Ai and in particular we're going to talk about open source llms that we can host ourselves on our own computers or somewhere but we have full control over the model we can execute this model on our Hardware so we will explore different options of different models there are a lot of open source In this module of LLM Zoomcamp, we'll look into open source LLMs that offer full control over the model execution. We explore various alternatives to GPT, including models like LLAMA from Meta, and Mistral. The focus is on setting up environments, running models, and accessing them from platforms like HuggingFace hub.\",\n",
       "  'id': '8b583ffd5ce6885a420c094a6a53adad'},\n",
       " {'vid_id': 'ATchkIRsH4g',\n",
       "  'title': 'LLM Zoomcamp 2.1 - Open Source LLMs',\n",
       "  'timecode': '0:29',\n",
       "  'text': \"models actually and I asked chbt what are the well-known ones there are a lot more and actually like even though it's a I asked for well-known ones I have not heard about many of them so there are I know hundreds if not thousands of them already available so here is the list um maybe you have heard you must have heard llama already so this is a model from meta from Facebook um and in this module I think we will we will use this flan T5 or just T5 I don't remember but like one of those developed by Google maybe you've heard about mistal and they also have an API that you can use we talked about that in the previous moduel so there are many of them and what we are going to cover in this module is how do we take this mod models and how we want to run it running these models usually requires a GPU so we will need a proper environment this is Will proper environment for that this is what we will also talk about so what we will do is uh if you remember we\",\n",
       "  'timecode_text': 'Open-Source LLMs',\n",
       "  'description': \"In this module of LLM Zoomcamp, we'll look into open source LLMs that offer full control over the model execution. We explore various alternatives to GPT, including models like LLAMA from Meta, and Mistral. The focus is on setting up environments, running models, and accessing them from platforms like HuggingFace hub.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=ATchkIRsH4g&t=29s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.1 - Open Source LLMs models actually and I asked chbt what are the well-known ones there are a lot more and actually like even though it's a I asked for well-known ones I have not heard about many of them so there are I know hundreds if not thousands of them already available so here is the list um maybe you have heard you must have heard llama already so this is a model from meta from Facebook um and in this module I think we will we will use this flan T5 or just T5 I don't remember but like one of those developed by Google maybe you've heard about mistal and they also have an API that you can use we talked about that in the previous moduel so there are many of them and what we are going to cover in this module is how do we take this mod models and how we want to run it running these models usually requires a GPU so we will need a proper environment this is Will proper environment for that this is what we will also talk about so what we will do is uh if you remember we In this module of LLM Zoomcamp, we'll look into open source LLMs that offer full control over the model execution. We explore various alternatives to GPT, including models like LLAMA from Meta, and Mistral. The focus is on setting up environments, running models, and accessing them from platforms like HuggingFace hub.\",\n",
       "  'id': '25ea9592c53f0bcbcfa87e08bcc06d8c'},\n",
       " {'vid_id': 'ATchkIRsH4g',\n",
       "  'title': 'LLM Zoomcamp 2.1 - Open Source LLMs',\n",
       "  'timecode': '1:47',\n",
       "  'text': \"talked about this rack flow so what we're going to do is we're going to see how we can replace this box with something else and this will be the focus of this model\",\n",
       "  'timecode_text': 'Replacing the LLM box in the RAG flow',\n",
       "  'description': \"In this module of LLM Zoomcamp, we'll look into open source LLMs that offer full control over the model execution. We explore various alternatives to GPT, including models like LLAMA from Meta, and Mistral. The focus is on setting up environments, running models, and accessing them from platforms like HuggingFace hub.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=ATchkIRsH4g&t=107s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.1 - Open Source LLMs talked about this rack flow so what we're going to do is we're going to see how we can replace this box with something else and this will be the focus of this model In this module of LLM Zoomcamp, we'll look into open source LLMs that offer full control over the model execution. We explore various alternatives to GPT, including models like LLAMA from Meta, and Mistral. The focus is on setting up environments, running models, and accessing them from platforms like HuggingFace hub.\",\n",
       "  'id': '623588b07a6d852722fa5424026b8837'},\n",
       " {'vid_id': 'ATchkIRsH4g',\n",
       "  'title': 'LLM Zoomcamp 2.1 - Open Source LLMs',\n",
       "  'timecode': '2:00',\n",
       "  'text': \"and in particular we will see how to set up an environment and we will see how to use different LMS okay I think uh so this is where we talk about flan T5 I think so we're going to cover we will look at different modules uh with different models how to run them and then how to access uh models from hin hgin face hub and think like things like that so uh have fun this is going to be an interesting model and in the next model we will see how to get a notebook with the GPU where you can run uh all these models see you soon\",\n",
       "  'timecode_text': 'Module Overview',\n",
       "  'description': \"In this module of LLM Zoomcamp, we'll look into open source LLMs that offer full control over the model execution. We explore various alternatives to GPT, including models like LLAMA from Meta, and Mistral. The focus is on setting up environments, running models, and accessing them from platforms like HuggingFace hub.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=ATchkIRsH4g&t=120s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.1 - Open Source LLMs and in particular we will see how to set up an environment and we will see how to use different LMS okay I think uh so this is where we talk about flan T5 I think so we're going to cover we will look at different modules uh with different models how to run them and then how to access uh models from hin hgin face hub and think like things like that so uh have fun this is going to be an interesting model and in the next model we will see how to get a notebook with the GPU where you can run uh all these models see you soon In this module of LLM Zoomcamp, we'll look into open source LLMs that offer full control over the model execution. We explore various alternatives to GPT, including models like LLAMA from Meta, and Mistral. The focus is on setting up environments, running models, and accessing them from platforms like HuggingFace hub.\",\n",
       "  'id': '9d12cee39991af2ca114c1622d3c25f7'},\n",
       " {'vid_id': 'E0cAqBWfJYY',\n",
       "  'title': 'LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks',\n",
       "  'timecode': '0:00',\n",
       "  'text': 'welcome back this is Alm Zoom camp and in this video we are going to see how to use saton cloud to get a machine a notebook with a GPU cuz for open source llms most of them require a GPU um and sat Cloud can give us an environment',\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.',\n",
       "  'link': 'https://www.youtube.com/watch?v=E0cAqBWfJYY&t=0s',\n",
       "  'text_vector': 'LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks welcome back this is Alm Zoom camp and in this video we are going to see how to use saton cloud to get a machine a notebook with a GPU cuz for open source llms most of them require a GPU um and sat Cloud can give us an environment In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.',\n",
       "  'id': '35897c3247e8873ae7baa63bad360b16'},\n",
       " {'vid_id': 'E0cAqBWfJYY',\n",
       "  'title': 'LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks',\n",
       "  'timecode': '0:20',\n",
       "  'text': \"where we can do this it's only it's one of the possible options you can use uh for example Google collab then in Google Cloud platform there is something you can get a machine on AWS I think with a s maker you can get a notebook with um a GPU there are many many options in this video we will look at using Saturn Cloud so here I have Saturn Cloud um so\",\n",
       "  'timecode_text': 'Environments with GPUs',\n",
       "  'description': 'In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.',\n",
       "  'link': 'https://www.youtube.com/watch?v=E0cAqBWfJYY&t=20s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks where we can do this it's only it's one of the possible options you can use uh for example Google collab then in Google Cloud platform there is something you can get a machine on AWS I think with a s maker you can get a notebook with um a GPU there are many many options in this video we will look at using Saturn Cloud so here I have Saturn Cloud um so In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.\",\n",
       "  'id': '42a8b50368670287377bc0c00af40c87'},\n",
       " {'vid_id': 'E0cAqBWfJYY',\n",
       "  'title': 'LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks',\n",
       "  'timecode': '0:50',\n",
       "  'text': \"what we need to do is log in if you already have an account if you do not have an account you have two options so if you're watching this uh before this uh module actually started you can go and fill in the form in our course management platform and then you will get access to the platform with already some extra GP hours if you're watching later you can click on get a technical demo and then mention that you are from uh I am a student uh or I'm a participant oh from LM Zoom Camp something like that right so just mention that you're coming from the course uh and then you should get the demo okay so I already have an account so I'll just log in um I think I should uh be able to do it with Google and now I have access to the\",\n",
       "  'timecode_text': 'Signing up for SaturnCloud',\n",
       "  'description': 'In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.',\n",
       "  'link': 'https://www.youtube.com/watch?v=E0cAqBWfJYY&t=50s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks what we need to do is log in if you already have an account if you do not have an account you have two options so if you're watching this uh before this uh module actually started you can go and fill in the form in our course management platform and then you will get access to the platform with already some extra GP hours if you're watching later you can click on get a technical demo and then mention that you are from uh I am a student uh or I'm a participant oh from LM Zoom Camp something like that right so just mention that you're coming from the course uh and then you should get the demo okay so I already have an account so I'll just log in um I think I should uh be able to do it with Google and now I have access to the In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.\",\n",
       "  'id': '2538cdf5c6d565584c3f351a2442a6af'},\n",
       " {'vid_id': 'E0cAqBWfJYY',\n",
       "  'title': 'LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks',\n",
       "  'timecode': '2:00',\n",
       "  'text': \"platform um I will show you how to create a notebook in C Cloud before that I want to show you a few other things so first uh of all is secrets so this is where we can add tokens like open AI token or um huging face token right so this is how you do this so you can say that this is hugen face token and then it's huging face blah blah right so this is where you put your token um yeah so this is how you create um Secrets then one other thing is\",\n",
       "  'timecode_text': 'SaturnCloud Secrets',\n",
       "  'description': 'In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.',\n",
       "  'link': 'https://www.youtube.com/watch?v=E0cAqBWfJYY&t=120s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks platform um I will show you how to create a notebook in C Cloud before that I want to show you a few other things so first uh of all is secrets so this is where we can add tokens like open AI token or um huging face token right so this is how you do this so you can say that this is hugen face token and then it's huging face blah blah right so this is where you put your token um yeah so this is how you create um Secrets then one other thing is In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.\",\n",
       "  'id': 'f09cf83d4c0b57e2553e12bbe3f51a24'},\n",
       " {'vid_id': 'E0cAqBWfJYY',\n",
       "  'title': 'LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks',\n",
       "  'timecode': '2:40',\n",
       "  'text': 'um setting up kit access so you go to the users and you have this public uh SSH Keys uh section right and I will replace the this after this video but you can go to GitHub up and I will not show it here but basically you can take this key put to GitHub and then you will have access from the the machines you create here on sou Cloud to get okay so now the interesting part is creating a',\n",
       "  'timecode_text': 'Git SSH key',\n",
       "  'description': 'In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.',\n",
       "  'link': 'https://www.youtube.com/watch?v=E0cAqBWfJYY&t=160s',\n",
       "  'text_vector': 'LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks um setting up kit access so you go to the users and you have this public uh SSH Keys uh section right and I will replace the this after this video but you can go to GitHub up and I will not show it here but basically you can take this key put to GitHub and then you will have access from the the machines you create here on sou Cloud to get okay so now the interesting part is creating a In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.',\n",
       "  'id': '1fbd84ee1ce44caa5ddba8f1728c7172'},\n",
       " {'vid_id': 'E0cAqBWfJYY',\n",
       "  'title': 'LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks',\n",
       "  'timecode': '3:20',\n",
       "  'text': \"notebook so it will be new notebook server I will also uh create like later if you watch this course uh later you can go check uh data do Club llm Zoom cam Southern Cloud so we already have here a starter notebook I also want to have here a button uh lounge with saton Cloud I'll put a link here also let me add a link to sat sat Cloud here okay and this is actually the notebook we are going to use uh the starter code and um yeah here let me call it LM Zoom cam HP and this is the only option I think I have and then here we choose a Saturn python llm and this is an okay version uh so I know that southern Cloud team right now is working on creating a new image so later if you see a new version here um just use a newer version for now we will use this one and now we need to install extra packages in particular we need to run this command so we need to install the latest version of the Transformers Library accelerator library and bits and bytes Library I will copy this thing and put it here and this is what SAT Cloud will execute when it starts and I will also add our G repo and this one llm zom Camp sou Cloud uh this one okay so I don't don't think I need to do anything else yeah let's just start it did it click yeah I did not okay and now we\",\n",
       "  'timecode_text': 'Creating a Notebook',\n",
       "  'description': 'In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.',\n",
       "  'link': 'https://www.youtube.com/watch?v=E0cAqBWfJYY&t=200s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks notebook so it will be new notebook server I will also uh create like later if you watch this course uh later you can go check uh data do Club llm Zoom cam Southern Cloud so we already have here a starter notebook I also want to have here a button uh lounge with saton Cloud I'll put a link here also let me add a link to sat sat Cloud here okay and this is actually the notebook we are going to use uh the starter code and um yeah here let me call it LM Zoom cam HP and this is the only option I think I have and then here we choose a Saturn python llm and this is an okay version uh so I know that southern Cloud team right now is working on creating a new image so later if you see a new version here um just use a newer version for now we will use this one and now we need to install extra packages in particular we need to run this command so we need to install the latest version of the Transformers Library accelerator library and bits and bytes Library I will copy this thing and put it here and this is what SAT Cloud will execute when it starts and I will also add our G repo and this one llm zom Camp sou Cloud uh this one okay so I don't don't think I need to do anything else yeah let's just start it did it click yeah I did not okay and now we In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.\",\n",
       "  'id': '6414abbbd6d4c8c8cb6064cd19bebcea'},\n",
       " {'vid_id': 'E0cAqBWfJYY',\n",
       "  'title': 'LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks',\n",
       "  'timecode': '5:43',\n",
       "  'text': \"need to go to secrets and roles to add the token we previously created in case you need but yeah this is how we do it and we just click Start and it's going to take some time so I will put it on pause and so let me check time so it's 8 uh 53 now so I'll be back when it's ready well I was away so I don't know when it actually finished uh preparing I checked it a few times most of the time it spent uh pulling the image but now it's uh it was roughly 9 minutes so I I think it was much faster than that okay anyways it's ready and now we can use\",\n",
       "  'timecode_text': 'Creating a Secret',\n",
       "  'description': 'In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.',\n",
       "  'link': 'https://www.youtube.com/watch?v=E0cAqBWfJYY&t=343s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks need to go to secrets and roles to add the token we previously created in case you need but yeah this is how we do it and we just click Start and it's going to take some time so I will put it on pause and so let me check time so it's 8 uh 53 now so I'll be back when it's ready well I was away so I don't know when it actually finished uh preparing I checked it a few times most of the time it spent uh pulling the image but now it's uh it was roughly 9 minutes so I I think it was much faster than that okay anyways it's ready and now we can use In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.\",\n",
       "  'id': 'ab3134eb00ba244029b4b1d32d6da68f'},\n",
       " {'vid_id': 'E0cAqBWfJYY',\n",
       "  'title': 'LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks',\n",
       "  'timecode': '6:40',\n",
       "  'text': \"Jupiter lab to start uh The Notebook I prefer to run Jupiter notebook it's up to you what you want to use um yeah and then here we have the starter code let me make it larger and I'll also remove yeah usually also do this and this is the starter code code so this is all all the code we created previously in module one we already have it here and we just executed like in a usual notebook and I want to quickly check if our and uh variable is set yeah here you go so this is how you configure your environment and yeah in the next video we will see how to use this environment to use our first open source LM it will be a Google's model fl5 and yeah is a GPU Nvidia SMI so there is a GPU this model will fit on this GPU okay so that is going to be fun see on\",\n",
       "  'timecode_text': 'Starting the Notebook',\n",
       "  'description': 'In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.',\n",
       "  'link': 'https://www.youtube.com/watch?v=E0cAqBWfJYY&t=400s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.2  - Using SaturnCloud for GPU Notebooks Jupiter lab to start uh The Notebook I prefer to run Jupiter notebook it's up to you what you want to use um yeah and then here we have the starter code let me make it larger and I'll also remove yeah usually also do this and this is the starter code code so this is all all the code we created previously in module one we already have it here and we just executed like in a usual notebook and I want to quickly check if our and uh variable is set yeah here you go so this is how you configure your environment and yeah in the next video we will see how to use this environment to use our first open source LM it will be a Google's model fl5 and yeah is a GPU Nvidia SMI so there is a GPU this model will fit on this GPU okay so that is going to be fun see on In this video, I guide you through using SaturnCloud to access a GPU notebook. I explain the options available, demonstrate how to log in, set up secrets, configure git access, and create a notebook.\",\n",
       "  'id': 'b97d1b0f032a1d3202df9ff52295fc7c'},\n",
       " {'vid_id': 'a86iTyxnFE4',\n",
       "  'title': 'LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5',\n",
       "  'timecode': '00:00',\n",
       "  'text': 'welcome back and in this video we are going to run our first llm open source llm this will be an llm from Google and we will use a huging face library for that um and uh if you words about huging',\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=a86iTyxnFE4&t=0s',\n",
       "  'text_vector': 'LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5 welcome back and in this video we are going to run our first llm open source llm this will be an llm from Google and we will use a huging face library for that um and uh if you words about huging In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.',\n",
       "  'id': '8aac2b529d5cf7ce6891928b0d0d58b2'},\n",
       " {'vid_id': 'a86iTyxnFE4',\n",
       "  'title': 'LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5',\n",
       "  'timecode': '00:16',\n",
       "  'text': \"face so they have um so Hing face is the way I see it this is a place for that hosts model so you can upload any model there and this is kind of a model repository in a way so if you want to uh find a model usually you go there and many companies uh like for example Google publish their models on Hing face so there's a place called Kagen face Hub and this is the place for models and we can see that um like right now on their website and we see that there are trending models and I think um this quen 2 this is an llm and they have like not it's not only llm there are many different models and this what you have chpt 40 I don't know what is that it's probably okay I will not attempt to uh go there the important thing to know that uh the library we will use will actually go to hgen face Hub to fetch um the model files okay so we have that and we install actually these transformers this is a library by hgen phase and when we use Transformers and when we download models using trans the Transformers Library we actually go to huging face to fetch this library to to fetch these models and um there we will use this model Google flan T5 XL so this will be\",\n",
       "  'timecode_text': 'Hugging Face',\n",
       "  'description': 'In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=a86iTyxnFE4&t=16s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5 face so they have um so Hing face is the way I see it this is a place for that hosts model so you can upload any model there and this is kind of a model repository in a way so if you want to uh find a model usually you go there and many companies uh like for example Google publish their models on Hing face so there's a place called Kagen face Hub and this is the place for models and we can see that um like right now on their website and we see that there are trending models and I think um this quen 2 this is an llm and they have like not it's not only llm there are many different models and this what you have chpt 40 I don't know what is that it's probably okay I will not attempt to uh go there the important thing to know that uh the library we will use will actually go to hgen face Hub to fetch um the model files okay so we have that and we install actually these transformers this is a library by hgen phase and when we use Transformers and when we download models using trans the Transformers Library we actually go to huging face to fetch this library to to fetch these models and um there we will use this model Google flan T5 XL so this will be In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.\",\n",
       "  'id': '63586c54f15f120dbc3ffac75f1a43a3'},\n",
       " {'vid_id': 'a86iTyxnFE4',\n",
       "  'title': 'LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5',\n",
       "  'timecode': '02:00',\n",
       "  'text': \"the first model and right now I am just going to Google this model and this is the first result so we are now in on hgen face in the hgen face Hub on the model description of um this model right and so this link describes the model I will put it now here in the link in in the links session you can check it out later and the interesting part usually this is like rme or GitHub projects except this rme is for models and usually there is a very good um examples of how you can run the model so there is like how you run it on a CPU we have a GPU machine or at least I'm showing it on the GPU machine you can try it on CPU but usually it takes some time to do that and I just copy this piece of code and I will put it here so I'll split it into multiple blocks so first we do the import and here we need to import two things the generator and tokenizer so the difference between them well it's actually kind of self-explanatory if you look at the code so there is one thing that is in the model this is the actual model the actual llm and the token is something that turns text for example English text or any text in any language and turns it into a representation that the model can understand so let me execute that right now it's going to download and actually let me let me stop it stop interrupt because like I didn't do one important thing is right now it is downloading\",\n",
       "  'timecode_text': 'Google FLAN T5 XL',\n",
       "  'description': 'In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=a86iTyxnFE4&t=120s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5 the first model and right now I am just going to Google this model and this is the first result so we are now in on hgen face in the hgen face Hub on the model description of um this model right and so this link describes the model I will put it now here in the link in in the links session you can check it out later and the interesting part usually this is like rme or GitHub projects except this rme is for models and usually there is a very good um examples of how you can run the model so there is like how you run it on a CPU we have a GPU machine or at least I'm showing it on the GPU machine you can try it on CPU but usually it takes some time to do that and I just copy this piece of code and I will put it here so I'll split it into multiple blocks so first we do the import and here we need to import two things the generator and tokenizer so the difference between them well it's actually kind of self-explanatory if you look at the code so there is one thing that is in the model this is the actual model the actual llm and the token is something that turns text for example English text or any text in any language and turns it into a representation that the model can understand so let me execute that right now it's going to download and actually let me let me stop it stop interrupt because like I didn't do one important thing is right now it is downloading In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.\",\n",
       "  'id': '6c4b59e03d15ffb7cf15f6efc72f5d97'},\n",
       " {'vid_id': 'a86iTyxnFE4',\n",
       "  'title': 'LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5',\n",
       "  'timecode': '04:00',\n",
       "  'text': \"um the files into a particular location um this location is specified with a variable HF home which we can set this is an environmental variable and by default it uses um cache folder in your home directory uh on this machine where I am running this and if you run this in Saturn Cloud you might also have this situation um you do not have enough space in your home directory uh so let's see how much space I have so in my home directory I have only not so much right not so much space but I have a lot of space in this SL run directory so what we want to do is we want to say hey hien face please use the slash run directory for your stuff right and for that we will use this line of code I'll include this uh in the beginning I will need to execute to restart the entire thing so this is the same as I as if I wrote export uh h have home and then this value run cache and then run D notebook I can also first set this environmental variable and then import the library I it and this Library will pick the the variable from from this location okay now I will just redo everything I did previously and we will use um slash run\",\n",
       "  'timecode_text': 'HF_HOME for Storing Model Files',\n",
       "  'description': 'In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=a86iTyxnFE4&t=240s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5 um the files into a particular location um this location is specified with a variable HF home which we can set this is an environmental variable and by default it uses um cache folder in your home directory uh on this machine where I am running this and if you run this in Saturn Cloud you might also have this situation um you do not have enough space in your home directory uh so let's see how much space I have so in my home directory I have only not so much right not so much space but I have a lot of space in this SL run directory so what we want to do is we want to say hey hien face please use the slash run directory for your stuff right and for that we will use this line of code I'll include this uh in the beginning I will need to execute to restart the entire thing so this is the same as I as if I wrote export uh h have home and then this value run cache and then run D notebook I can also first set this environmental variable and then import the library I it and this Library will pick the the variable from from this location okay now I will just redo everything I did previously and we will use um slash run In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.\",\n",
       "  'id': 'edebe7b5f8b4ff34161627baab77a6e0'},\n",
       " {'vid_id': 'a86iTyxnFE4',\n",
       "  'title': 'LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5',\n",
       "  'timecode': '06:00',\n",
       "  'text': \"um now SL run directory for downloading the files and now let us execute this one more time so we're downloading the file uh the model files they're pretty large this is not the largest model if we just if we we look up Google plan5 we will see that there are many different models um so yeah so this is the list let me also put it in the in documents um so there's a bunch of them and we use Excel so Excel is something that will fit our GPU if you have a more powerful GPU you can go to you can use a larger version if you have a less powerful GPU or if you have a CPU you can use a smaller version okay we have seen this already uh it's still being downloaded yeah so it will take some time to download the files and to load the files into memory to so right now it's downloading the files um so while it's doing that let's take a look at um what we do so as I\",\n",
       "  'timecode_text': 'Downloading Model Files',\n",
       "  'description': 'In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=a86iTyxnFE4&t=360s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5 um now SL run directory for downloading the files and now let us execute this one more time so we're downloading the file uh the model files they're pretty large this is not the largest model if we just if we we look up Google plan5 we will see that there are many different models um so yeah so this is the list let me also put it in the in documents um so there's a bunch of them and we use Excel so Excel is something that will fit our GPU if you have a more powerful GPU you can go to you can use a larger version if you have a less powerful GPU or if you have a CPU you can use a smaller version okay we have seen this already uh it's still being downloaded yeah so it will take some time to download the files and to load the files into memory to so right now it's downloading the files um so while it's doing that let's take a look at um what we do so as I In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.\",\n",
       "  'id': 'cc9096b78b4db57dd74c6e5759a7116a'},\n",
       " {'vid_id': 'a86iTyxnFE4',\n",
       "  'title': 'LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5',\n",
       "  'timecode': '07:18',\n",
       "  'text': \"mentioned there are two things tokenizer and the model so the tokenizer takes the text and turns it into some representation and then we feed this representation into the model I think it will be interesting to now look to split this into two parts so first we will look at the input IDs so this is the representation of uh this text translate English to German how old are you and then we will actually run the model okay so right now what is happening is uh the files uh the model files are already there downloaded and right now we are loading them to Memory and while we're doing this I will rename this to let's say um huging face plan was it f T5 or what was the name F5 yeah okay so it finished and so this is the internal representation of this I supect this is an ID for each token so translate is uh this is translate this is English this is two this is German um probably colon how old are you question mark end of sentence maybe probably I don't know uh I assume this is uh this is how it works right so for each toing for each word we have um an ID but I'm just guessing and then uh yeah actually we get the answer right so well there is a bit of um like this is I guess technical stuff that the model has uh perhaps we need to filter something but uh so the actually actual thing is so this is what the model is generating so this is the output right again a bunch of um ads and then we need to turn this back into text and this is what we use uh with tokenizer and we can see that token zero is this one toker one token one is this one so perhaps if we need to do if we need to filter out this filter out we just remove uh these tokens or do some simple text processing okay so this is how we use this model and let's uh\",\n",
       "  'timecode_text': 'Models and Tokenizers',\n",
       "  'description': 'In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=a86iTyxnFE4&t=438s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5 mentioned there are two things tokenizer and the model so the tokenizer takes the text and turns it into some representation and then we feed this representation into the model I think it will be interesting to now look to split this into two parts so first we will look at the input IDs so this is the representation of uh this text translate English to German how old are you and then we will actually run the model okay so right now what is happening is uh the files uh the model files are already there downloaded and right now we are loading them to Memory and while we're doing this I will rename this to let's say um huging face plan was it f T5 or what was the name F5 yeah okay so it finished and so this is the internal representation of this I supect this is an ID for each token so translate is uh this is translate this is English this is two this is German um probably colon how old are you question mark end of sentence maybe probably I don't know uh I assume this is uh this is how it works right so for each toing for each word we have um an ID but I'm just guessing and then uh yeah actually we get the answer right so well there is a bit of um like this is I guess technical stuff that the model has uh perhaps we need to filter something but uh so the actually actual thing is so this is what the model is generating so this is the output right again a bunch of um ads and then we need to turn this back into text and this is what we use uh with tokenizer and we can see that token zero is this one toker one token one is this one so perhaps if we need to do if we need to filter out this filter out we just remove uh these tokens or do some simple text processing okay so this is how we use this model and let's uh In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.\",\n",
       "  'id': '2495c9b639d4f549adcf68153275a71a'},\n",
       " {'vid_id': 'a86iTyxnFE4',\n",
       "  'title': 'LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5',\n",
       "  'timecode': '10:10',\n",
       "  'text': \"put everything together so I will just uh um put everything now in one line again like it used to be and I'll call this result right uh end result is uh the alt z z okay right so this is how we do it now let us change this part uh I'll take the entire thing here build prompt cuz we will also need to replace the prompt um okay llm is this one right so I'll put this thing here and return result return result so let's try this rock um I just discover the course can I still join it um yeah of course we need to replace uh yes even if you don't register you're still eligible to submit there um yeah and now probably we need to make it larger I don't know how to do\",\n",
       "  'timecode_text': 'Rewriting the LLM function',\n",
       "  'description': 'In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=a86iTyxnFE4&t=610s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5 put everything together so I will just uh um put everything now in one line again like it used to be and I'll call this result right uh end result is uh the alt z z okay right so this is how we do it now let us change this part uh I'll take the entire thing here build prompt cuz we will also need to replace the prompt um okay llm is this one right so I'll put this thing here and return result return result so let's try this rock um I just discover the course can I still join it um yeah of course we need to replace uh yes even if you don't register you're still eligible to submit there um yeah and now probably we need to make it larger I don't know how to do In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.\",\n",
       "  'id': '0bd7e6af4c9fa2eb4aebdae2048d7f6c'},\n",
       " {'vid_id': 'a86iTyxnFE4',\n",
       "  'title': 'LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5',\n",
       "  'timecode': '11:48',\n",
       "  'text': \"it uh there is probably Max um max token length or something something like I I need to look it up let's see model generate longer responses uh I'll generate hi can face longer responses probably there is um some simple parameter that we can use for that I don't remember this perimeter okay I'll check it later for now I see that it works and perhaps we can tweak The Prompt but like it actually looks quite fine um we can try so usually what helps like this is an uh completion model right so usually what helps is um just explicitly say okay this is question this is context and this is answer so I can try to see if this changes anything um no it's actually but yeah we can experiment with um The Prompt so probably for this one it just works um I'll put it on pause now I want to look up the parameter for making longer answers and then I'll come back yeah of course I use chpt to ask what to do and U yeah it told me which parameters I need to use and I simply will copy these parameters and put them here so the parameter I was looking for was max length parameter I will also like there is some explanation of parameters which probably I should copy I will put this uh here and edit it a little bit um so you can read what the they are and let us actually test it but I was looking for this specific max length right so if I do that and three execute Rock function yeah it says yes even if you don't register you're still eligible submit the homework and now we have our Rock flow and we replaced uh the call to open AI with a local instance of a GPU in particular it was uh flant T5 Excel and now we can do everything locally without having to call open ey okay uh this flan uh T5 is just one of the possible models there are many other models and in this video I want to explore another model from Microsoft called P3 so see you soon\",\n",
       "  'timecode_text': 'Making the Output Longer',\n",
       "  'description': 'In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=a86iTyxnFE4&t=708s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.3 - HuggingFace and Google FLAN T5 it uh there is probably Max um max token length or something something like I I need to look it up let's see model generate longer responses uh I'll generate hi can face longer responses probably there is um some simple parameter that we can use for that I don't remember this perimeter okay I'll check it later for now I see that it works and perhaps we can tweak The Prompt but like it actually looks quite fine um we can try so usually what helps like this is an uh completion model right so usually what helps is um just explicitly say okay this is question this is context and this is answer so I can try to see if this changes anything um no it's actually but yeah we can experiment with um The Prompt so probably for this one it just works um I'll put it on pause now I want to look up the parameter for making longer answers and then I'll come back yeah of course I use chpt to ask what to do and U yeah it told me which parameters I need to use and I simply will copy these parameters and put them here so the parameter I was looking for was max length parameter I will also like there is some explanation of parameters which probably I should copy I will put this uh here and edit it a little bit um so you can read what the they are and let us actually test it but I was looking for this specific max length right so if I do that and three execute Rock function yeah it says yes even if you don't register you're still eligible submit the homework and now we have our Rock flow and we replaced uh the call to open AI with a local instance of a GPU in particular it was uh flant T5 Excel and now we can do everything locally without having to call open ey okay uh this flan uh T5 is just one of the possible models there are many other models and in this video I want to explore another model from Microsoft called P3 so see you soon In this video, we explore the Google Flan T5XL model, discussing the tokenizer and generator, and setting up the environment variables for model.\",\n",
       "  'id': 'be435889f102abd4a3223a64b194a422'},\n",
       " {'vid_id': '8KH6AS2PqWk',\n",
       "  'title': 'LLM Zoomcamp 2.4 - Phi 3 Mini',\n",
       "  'timecode': '0:00',\n",
       "  'text': \"welcome back in this video we are going to try another open source model also available on haging face Hub in the previous video we already used uh our first open source llm model on a GPU it was Google's model flan T5 XL and now we will use a model from Microsoft this model is called fe3 uh this is rather new model and um I think uh this um plant T5 is an older one and what we will do is just again look it up and Google the first page at least in my search is again a page on hgen phas and here we see the description so I think they have like different um uh again different sizes like with uh other models uh so this one is a small one so it doesn't have a lot of um parameters that's why I think it's called mini and uh probably there are more details if we check this block um yeah you can check it out so we saw this mini and small uh the small one should still fit the GPU that we have although not true but usually 7B or less fits a GPU like the one we have and by the way I\",\n",
       "  'timecode_text': \"Introduction to Microsoft's Phi3\",\n",
       "  'description': \"In this video, we use Microsoft's Phi 3 model, comparing it with Google's Flan T5 XL. I discuss the model's parameters, GPU compatibility, and interface differences.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=8KH6AS2PqWk&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.4 - Phi 3 Mini welcome back in this video we are going to try another open source model also available on haging face Hub in the previous video we already used uh our first open source llm model on a GPU it was Google's model flan T5 XL and now we will use a model from Microsoft this model is called fe3 uh this is rather new model and um I think uh this um plant T5 is an older one and what we will do is just again look it up and Google the first page at least in my search is again a page on hgen phas and here we see the description so I think they have like different um uh again different sizes like with uh other models uh so this one is a small one so it doesn't have a lot of um parameters that's why I think it's called mini and uh probably there are more details if we check this block um yeah you can check it out so we saw this mini and small uh the small one should still fit the GPU that we have although not true but usually 7B or less fits a GPU like the one we have and by the way I In this video, we use Microsoft's Phi 3 model, comparing it with Google's Flan T5 XL. I discuss the model's parameters, GPU compatibility, and interface differences.\",\n",
       "  'id': '421a954648aea7615aac0e6daca4c2df'},\n",
       " {'vid_id': '8KH6AS2PqWk',\n",
       "  'title': 'LLM Zoomcamp 2.4 - Phi 3 Mini',\n",
       "  'timecode': '1:37',\n",
       "  'text': \"don't think we actually discussed that so maybe we can quickly check so I'm right now I'm going to open a terminal so this machine is machine a machine with a GPU and I'm going to do Nvidia SMI and this lets us tells us what kind of GPU we have so in our GPU we have U this amount of memory and um it's like almost 16 like 15 gigs um and usually oops it's a wrong link um so usually these 7B models fit on such gpus but they might not so depending on on what kind of GPU you have you might have to try smaller or you can try larger Gus and I think um so right now what I also sometimes do is just to watch this so then we can see like a dashboard and what we also need to do is we need to stop shut down the previous model so it doesn't take any GPU even though like it was actually using zero megabytes of memory I don't know why but typically when you start running a model um I I be honest like I just started this instance of jupter notebook but if you contined uh from the old instance probably you would have seen something else like there is a process that takes the GPU and then you see that more RAM more memory of GPU is used so that's why I stopped it so now the memory is free and now I'm going to duplicate it and we will call\",\n",
       "  'timecode_text': 'nvidia-smi and Model Size',\n",
       "  'description': \"In this video, we use Microsoft's Phi 3 model, comparing it with Google's Flan T5 XL. I discuss the model's parameters, GPU compatibility, and interface differences.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=8KH6AS2PqWk&t=97s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.4 - Phi 3 Mini don't think we actually discussed that so maybe we can quickly check so I'm right now I'm going to open a terminal so this machine is machine a machine with a GPU and I'm going to do Nvidia SMI and this lets us tells us what kind of GPU we have so in our GPU we have U this amount of memory and um it's like almost 16 like 15 gigs um and usually oops it's a wrong link um so usually these 7B models fit on such gpus but they might not so depending on on what kind of GPU you have you might have to try smaller or you can try larger Gus and I think um so right now what I also sometimes do is just to watch this so then we can see like a dashboard and what we also need to do is we need to stop shut down the previous model so it doesn't take any GPU even though like it was actually using zero megabytes of memory I don't know why but typically when you start running a model um I I be honest like I just started this instance of jupter notebook but if you contined uh from the old instance probably you would have seen something else like there is a process that takes the GPU and then you see that more RAM more memory of GPU is used so that's why I stopped it so now the memory is free and now I'm going to duplicate it and we will call In this video, we use Microsoft's Phi 3 model, comparing it with Google's Flan T5 XL. I discuss the model's parameters, GPU compatibility, and interface differences.\",\n",
       "  'id': 'ae4d964ae270a6bf758eec3f6c1039cb'},\n",
       " {'vid_id': '8KH6AS2PqWk',\n",
       "  'title': 'LLM Zoomcamp 2.4 - Phi 3 Mini',\n",
       "  'timecode': '3:35',\n",
       "  'text': \"it I'll rename it so I'll use it um P3 right so let me run it again and I will run it only up to this point so I'll create empty else here so what I did is I just pressed B and then I will remove this one and where is this uh yeah we're going to close it so we have this um model uh read me file from this V3 and I think we have an example here so this is an example again like last time I'm just going to copy it and and then execute it one line by line by line I don't think we need this one um okay so I'll also set it here so this one just sets the random seat to zero so our what we do is reproducible and now we download the F3 model and the tokenizer for the model okay it's downloading this right now and I think if we compare it with this flant T5 I think T3 is actually smaller um yeah so this one is 10 gigs let's see how large is P3 so we have the first uh part is yeah so it's like five plus it's like 7 gigs versus um like 11 gigs I guess okay so let me close it um so the way we use it is as you see the interface is slightly different so\",\n",
       "  'timecode_text': 'Using Phi3',\n",
       "  'description': \"In this video, we use Microsoft's Phi 3 model, comparing it with Google's Flan T5 XL. I discuss the model's parameters, GPU compatibility, and interface differences.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=8KH6AS2PqWk&t=215s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.4 - Phi 3 Mini it I'll rename it so I'll use it um P3 right so let me run it again and I will run it only up to this point so I'll create empty else here so what I did is I just pressed B and then I will remove this one and where is this uh yeah we're going to close it so we have this um model uh read me file from this V3 and I think we have an example here so this is an example again like last time I'm just going to copy it and and then execute it one line by line by line I don't think we need this one um okay so I'll also set it here so this one just sets the random seat to zero so our what we do is reproducible and now we download the F3 model and the tokenizer for the model okay it's downloading this right now and I think if we compare it with this flant T5 I think T3 is actually smaller um yeah so this one is 10 gigs let's see how large is P3 so we have the first uh part is yeah so it's like five plus it's like 7 gigs versus um like 11 gigs I guess okay so let me close it um so the way we use it is as you see the interface is slightly different so In this video, we use Microsoft's Phi 3 model, comparing it with Google's Flan T5 XL. I discuss the model's parameters, GPU compatibility, and interface differences.\",\n",
       "  'id': '7c783275a4c0fc59e3fde87bb6427b5c'},\n",
       " {'vid_id': '8KH6AS2PqWk',\n",
       "  'title': 'LLM Zoomcamp 2.4 - Phi 3 Mini',\n",
       "  'timecode': '5:45',\n",
       "  'text': \"actually here we have more like a chatbot interface so previously it was like a completion interface so we had a prompt and then the task of anlm was to complete this but here it looks like we actually have a chat um so we have a bunch of messages and then we send this to pipeline pipeline I think combines um model and tokenizer into one so let me create let me just put it in a separate so we create a pipeline which combines both and what we have here is messages some parameters and it this is similar to what we did previously we passed some parameters remember I needed to I think I still have it here so yeah I needed to talk to chpt to learn about parameters so this model also have some parameters and these are the parameters and now so these are the messages we sent I will um edit them and and by the way this is very similar to open AI uh interface remember we used before how do I like I just discover the course and I still join and let's see what happens yes you can still join the course however please not yeah so it gives us a generic answer\",\n",
       "  'timecode_text': 'Interface Comparison',\n",
       "  'description': \"In this video, we use Microsoft's Phi 3 model, comparing it with Google's Flan T5 XL. I discuss the model's parameters, GPU compatibility, and interface differences.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=8KH6AS2PqWk&t=345s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.4 - Phi 3 Mini actually here we have more like a chatbot interface so previously it was like a completion interface so we had a prompt and then the task of anlm was to complete this but here it looks like we actually have a chat um so we have a bunch of messages and then we send this to pipeline pipeline I think combines um model and tokenizer into one so let me create let me just put it in a separate so we create a pipeline which combines both and what we have here is messages some parameters and it this is similar to what we did previously we passed some parameters remember I needed to I think I still have it here so yeah I needed to talk to chpt to learn about parameters so this model also have some parameters and these are the parameters and now so these are the messages we sent I will um edit them and and by the way this is very similar to open AI uh interface remember we used before how do I like I just discover the course and I still join and let's see what happens yes you can still join the course however please not yeah so it gives us a generic answer In this video, we use Microsoft's Phi 3 model, comparing it with Google's Flan T5 XL. I discuss the model's parameters, GPU compatibility, and interface differences.\",\n",
       "  'id': '4c2799a78c6943debf8576ffa14591d4'},\n",
       " {'vid_id': '8KH6AS2PqWk',\n",
       "  'title': 'LLM Zoomcamp 2.4 - Phi 3 Mini',\n",
       "  'timecode': '7:30',\n",
       "  'text': \"and now let us use that I will just take this part and put it to our llm some messages would be just give it the prompt as the first message and this is what we return so I think I removed more than I should have let me quickly check how it looks like content right okay so okay let's see what happens if we don't even change the prompt okay looks cool I don't know why like this one like the models typically we saw in the previous one also time to include a space here at the beginning I'll just add strip so then we don't have a space at the beginning yes you can still join the course even if you discover it after the start date cool so this is another model also on hien phas and this model was a model from Microsoft and in the next video we will will see yet another model it will be a model from mistal maybe you heard about mistal um so yeah we'll see that we will talk about that in the next video so see you soon\",\n",
       "  'timecode_text': 'Using Phi3 for RAG',\n",
       "  'description': \"In this video, we use Microsoft's Phi 3 model, comparing it with Google's Flan T5 XL. I discuss the model's parameters, GPU compatibility, and interface differences.\",\n",
       "  'link': 'https://www.youtube.com/watch?v=8KH6AS2PqWk&t=450s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.4 - Phi 3 Mini and now let us use that I will just take this part and put it to our llm some messages would be just give it the prompt as the first message and this is what we return so I think I removed more than I should have let me quickly check how it looks like content right okay so okay let's see what happens if we don't even change the prompt okay looks cool I don't know why like this one like the models typically we saw in the previous one also time to include a space here at the beginning I'll just add strip so then we don't have a space at the beginning yes you can still join the course even if you discover it after the start date cool so this is another model also on hien phas and this model was a model from Microsoft and in the next video we will will see yet another model it will be a model from mistal maybe you heard about mistal um so yeah we'll see that we will talk about that in the next video so see you soon In this video, we use Microsoft's Phi 3 model, comparing it with Google's Flan T5 XL. I discuss the model's parameters, GPU compatibility, and interface differences.\",\n",
       "  'id': '0dbd36deb8505ce37cb51a6057c4acaa'},\n",
       " {'vid_id': 'TdVEOzSoUCs',\n",
       "  'title': 'LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hi welcome back in we in this module we are looking at open source llms we have already looked at Google's fl5 we have looked at uh Microsoft's F3 and now we're looking at Mistral 7B model so again like previously I just copy it go to Google paste it and end up on the card page so let me add the link here yeah I think it's already here and you can see that we don't actually have a lot of information here uh meaning like there is some information but previously we saw that there is always a piece of code an example of how to use a model here we don't have this information right so probably we can just follow the same uh approach like here with um like creating a model creating tokenizer luckily we don't need to of course we can also ask Char PT it probably knows but also actually the the way I found out about this particular model is I Googled uh\",\n",
       "  'timecode_text': 'Introduction to Mistral 7b Model',\n",
       "  'description': 'In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.',\n",
       "  'link': 'https://www.youtube.com/watch?v=TdVEOzSoUCs&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication hi welcome back in we in this module we are looking at open source llms we have already looked at Google's fl5 we have looked at uh Microsoft's F3 and now we're looking at Mistral 7B model so again like previously I just copy it go to Google paste it and end up on the card page so let me add the link here yeah I think it's already here and you can see that we don't actually have a lot of information here uh meaning like there is some information but previously we saw that there is always a piece of code an example of how to use a model here we don't have this information right so probably we can just follow the same uh approach like here with um like creating a model creating tokenizer luckily we don't need to of course we can also ask Char PT it probably knows but also actually the the way I found out about this particular model is I Googled uh In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.\",\n",
       "  'id': 'f3bea681d065d7d747336057b021e0fb'},\n",
       " {'vid_id': 'TdVEOzSoUCs',\n",
       "  'title': 'LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication',\n",
       "  'timecode': '01:25',\n",
       "  'text': \"hugging hugging face where can I type llm models and yeah I visit it often so this is the the link I wanted to share with you and in actually in day tutorial they start with uh this mistal model so let us also let us take the code from here and I will create a copy so first I will shut down and actually like if you watch this after watching after after watching the previous video you can see that F3 takes this amount of memory so 8 gigs uh let's stop both of them 532 and now yeah zero memory I will now create copy again and we will call this\",\n",
       "  'timecode_text': 'LLM Tutorial',\n",
       "  'description': 'In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.',\n",
       "  'link': 'https://www.youtube.com/watch?v=TdVEOzSoUCs&t=85s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication hugging hugging face where can I type llm models and yeah I visit it often so this is the the link I wanted to share with you and in actually in day tutorial they start with uh this mistal model so let us also let us take the code from here and I will create a copy so first I will shut down and actually like if you watch this after watching after after watching the previous video you can see that F3 takes this amount of memory so 8 gigs uh let's stop both of them 532 and now yeah zero memory I will now create copy again and we will call this In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.\",\n",
       "  'id': '0f87cc441cebd9b2292d4ae66f06ff88'},\n",
       " {'vid_id': 'TdVEOzSoUCs',\n",
       "  'title': 'LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication',\n",
       "  'timecode': '02:37',\n",
       "  'text': \"um mral 7 B again execute so this one I don't execute and let us take this piece of code and also we will need the tokenizer right so I will put UTS here separately and then here we have um a model and here we have a tokenizer and I will put model inputs here um okay so let us execute that and we see that it doesn't work so there is client error unauthorized uh access so actually we cannot access this model and the reason for that is this model requires us to accept the agreement I have already accepted it but if you visit it for the first time you actually need to be logged in and\",\n",
       "  'timecode_text': 'Using Mistral 7B',\n",
       "  'description': 'In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.',\n",
       "  'link': 'https://www.youtube.com/watch?v=TdVEOzSoUCs&t=157s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication um mral 7 B again execute so this one I don't execute and let us take this piece of code and also we will need the tokenizer right so I will put UTS here separately and then here we have um a model and here we have a tokenizer and I will put model inputs here um okay so let us execute that and we see that it doesn't work so there is client error unauthorized uh access so actually we cannot access this model and the reason for that is this model requires us to accept the agreement I have already accepted it but if you visit it for the first time you actually need to be logged in and In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.\",\n",
       "  'id': '1981c39ce1afffe6b3c5449328223814'},\n",
       " {'vid_id': 'TdVEOzSoUCs',\n",
       "  'title': 'LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication',\n",
       "  'timecode': '03:56',\n",
       "  'text': \"you will see um like an an aru an agreement which you will need to accept and only then you can use the model so unlike the previous two models here you actually need to say that uh you will not use it for something that is not intended by the by the developers okay so you will need to accept that and it means that you will need to somehow get access an authorized access to the model so the way you do it you need to have an account in hgen face then you go to settings and then you go to access tokens and then you create a new token so let's call it course uh LM course um yeah read access generate token so this is the token of course you should not show it to anyone I will not show I will disable this token after recording this video and we will now just remember this token typically we would put this Tok actually in an environmental variable uh I will just pretend we actually did this I'll call it HF H can face token and put it here so we kind of did this outside we can pretend that we did this outside of jupyter notebook um like for example if you use S Cloud you can put it to to Secrets if you use um Google collab you also have collab Secrets there um so typically if you have some hosted notebook solution you have a secret management component or if you run it locally you just put this into your terminal or use a end file whatever but typically you don't put it in um code like I do here and um I will also now yeah get it so I think um the code is uh from hugging face Hub import login yeah um we call this login yeah we need to provide um this token token and this yeah so token is valid your token has been saved login successful okay so now we can access the model CU I already\",\n",
       "  'timecode_text': 'Authorization for HuggingFace Hub',\n",
       "  'description': 'In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.',\n",
       "  'link': 'https://www.youtube.com/watch?v=TdVEOzSoUCs&t=236s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication you will see um like an an aru an agreement which you will need to accept and only then you can use the model so unlike the previous two models here you actually need to say that uh you will not use it for something that is not intended by the by the developers okay so you will need to accept that and it means that you will need to somehow get access an authorized access to the model so the way you do it you need to have an account in hgen face then you go to settings and then you go to access tokens and then you create a new token so let's call it course uh LM course um yeah read access generate token so this is the token of course you should not show it to anyone I will not show I will disable this token after recording this video and we will now just remember this token typically we would put this Tok actually in an environmental variable uh I will just pretend we actually did this I'll call it HF H can face token and put it here so we kind of did this outside we can pretend that we did this outside of jupyter notebook um like for example if you use S Cloud you can put it to to Secrets if you use um Google collab you also have collab Secrets there um so typically if you have some hosted notebook solution you have a secret management component or if you run it locally you just put this into your terminal or use a end file whatever but typically you don't put it in um code like I do here and um I will also now yeah get it so I think um the code is uh from hugging face Hub import login yeah um we call this login yeah we need to provide um this token token and this yeah so token is valid your token has been saved login successful okay so now we can access the model CU I already In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.\",\n",
       "  'id': '5ff15dc3f44a0c366aabd106dd758de8'},\n",
       " {'vid_id': 'TdVEOzSoUCs',\n",
       "  'title': 'LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication',\n",
       "  'timecode': '06:46',\n",
       "  'text': \"accepted the agreement if you have not accepted the agreement you would see another error and while when it downloads um so we already have an example so while it's downloading let us check the the rest of the tutorial so this is how we execute it what else do we have here yeah well it's probably very interesting um you should check it definitely I included the link in the here in the link link links session uh let's see if it already yeah so this is probably so far the largest model we used so it's larger even than this flant T5 it should still fit our GPU we will see now when we load uh how much it actually takes okay now it will Lo it uh let me see if there's anything that you need to know okay this is another interesting link open llm leaderboard and open llm performance leaderboard let me close that uh this is something we will see in the next video I'll keep the links I'll keep the tutorial open so we can come back to this in the the next video so this leaderboard contains different uh models and their performance okay take some time and I'll just put it on pause and wait till it uh it loads the model okay it\",\n",
       "  'timecode_text': 'Loading the Model',\n",
       "  'description': 'In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.',\n",
       "  'link': 'https://www.youtube.com/watch?v=TdVEOzSoUCs&t=406s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication accepted the agreement if you have not accepted the agreement you would see another error and while when it downloads um so we already have an example so while it's downloading let us check the the rest of the tutorial so this is how we execute it what else do we have here yeah well it's probably very interesting um you should check it definitely I included the link in the here in the link link links session uh let's see if it already yeah so this is probably so far the largest model we used so it's larger even than this flant T5 it should still fit our GPU we will see now when we load uh how much it actually takes okay now it will Lo it uh let me see if there's anything that you need to know okay this is another interesting link open llm leaderboard and open llm performance leaderboard let me close that uh this is something we will see in the next video I'll keep the links I'll keep the tutorial open so we can come back to this in the the next video so this leaderboard contains different uh models and their performance okay take some time and I'll just put it on pause and wait till it uh it loads the model okay it In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.\",\n",
       "  'id': '68cb3bbe0b7c177e117ff72c4ec183ed'},\n",
       " {'vid_id': 'TdVEOzSoUCs',\n",
       "  'title': 'LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication',\n",
       "  'timecode': '09:18',\n",
       "  'text': \"finished so let's see how yeah it's actually like when we look at uh the memory consumption maybe when we run something it will take more R I don't know so let's see yeah so it's also from what I see is a completion model so we give it a list of colors red blue and then it completes so we can use it um so the problem here is that uh yeah it kind of will give us the entire input but we will see what to do\",\n",
       "  'timecode_text': 'Generating',\n",
       "  'description': 'In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.',\n",
       "  'link': 'https://www.youtube.com/watch?v=TdVEOzSoUCs&t=558s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication finished so let's see how yeah it's actually like when we look at uh the memory consumption maybe when we run something it will take more R I don't know so let's see yeah so it's also from what I see is a completion model so we give it a list of colors red blue and then it completes so we can use it um so the problem here is that uh yeah it kind of will give us the entire input but we will see what to do In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.\",\n",
       "  'id': '21e86a162e8ad14df9ac1a6c21cb0500'},\n",
       " {'vid_id': 'TdVEOzSoUCs',\n",
       "  'title': 'LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication',\n",
       "  'timecode': '10:00',\n",
       "  'text': \"with that now um so I will this and put again to our LM uh result Regal result yeah yeah of course we need to put brought here instead of that and put max length to I think it's here 300 or what's happening\",\n",
       "  'timecode_text': 'Adding it to the RAG flow',\n",
       "  'description': 'In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.',\n",
       "  'link': 'https://www.youtube.com/watch?v=TdVEOzSoUCs&t=600s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication with that now um so I will this and put again to our LM uh result Regal result yeah yeah of course we need to put brought here instead of that and put max length to I think it's here 300 or what's happening In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.\",\n",
       "  'id': '6ef5dd4a01ad43a0a0521cc78aac48e3'},\n",
       " {'vid_id': 'TdVEOzSoUCs',\n",
       "  'title': 'LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication',\n",
       "  'timecode': '11:00',\n",
       "  'text': \"um we can probably play with parameters uh more and yeah so it does not increase so probably It just fits uh it just uses less than six kicks and yeah I don't think I like the answer so let's see yeah it's kind of weird so I think with this we need to CU it's a completion model we need to tune to change the The Prompt so I'll just try to do this\",\n",
       "  'timecode_text': 'Making the Output Larger',\n",
       "  'description': 'In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.',\n",
       "  'link': 'https://www.youtube.com/watch?v=TdVEOzSoUCs&t=660s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication um we can probably play with parameters uh more and yeah so it does not increase so probably It just fits uh it just uses less than six kicks and yeah I don't think I like the answer so let's see yeah it's kind of weird so I think with this we need to CU it's a completion model we need to tune to change the The Prompt so I'll just try to do this In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.\",\n",
       "  'id': 'ffabc59c9b998a5d96667ba5ebbbdf30'},\n",
       " {'vid_id': 'TdVEOzSoUCs',\n",
       "  'title': 'LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication',\n",
       "  'timecode': '11:45',\n",
       "  'text': \"question context answer let's try that and see if it gives better results yeah it's also looks like it's a slowest model question yeah not very useful well that also happens H I will probably need to tune this model um I'll put it on pause now to try to figure it out in my experiments it actually worked so I don't know why the answers are weird but sometimes it happens like I don't know like when I Was preparing for this course I tried I don't know 15 different llms and the first two I showed and this one they were giving okay results uh so probably I'm missing something so I'll be right back um I cannot say I was very successful I of course asked CH PT\",\n",
       "  'timecode_text': 'Updating the Prompt',\n",
       "  'description': 'In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.',\n",
       "  'link': 'https://www.youtube.com/watch?v=TdVEOzSoUCs&t=705s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication question context answer let's try that and see if it gives better results yeah it's also looks like it's a slowest model question yeah not very useful well that also happens H I will probably need to tune this model um I'll put it on pause now to try to figure it out in my experiments it actually worked so I don't know why the answers are weird but sometimes it happens like I don't know like when I Was preparing for this course I tried I don't know 15 different llms and the first two I showed and this one they were giving okay results uh so probably I'm missing something so I'll be right back um I cannot say I was very successful I of course asked CH PT In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.\",\n",
       "  'id': 'af0f94c7d35ce08fbdfb1f312b84379e'},\n",
       " {'vid_id': 'TdVEOzSoUCs',\n",
       "  'title': 'LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication',\n",
       "  'timecode': '13:20',\n",
       "  'text': \"because it's a course about LM and yeah first it actually suggested to use pipeline we I think think we used pipeline in P3 and uh to create a generator which I think is a convenient way of doing that so instead of invoking tokenizer and model separately we just put it inside a generator so this is what I did here created the generator and then uh I tried a smaller prompt so it gave me a bunch of parameters that we can try but I think the main thing is it does not seem to like large prompts so I cut the um the prompt so I only kept two uh answers um and then it gave some response um yeah we can also tweak The Prompt let me take a copy yeah I think yeah it says yes you can still join the course um we can also tweak it a little bit so did I make a copy yeah so we can try to tweak The Prompt so instead of uh doing it this way maybe we can reduce the answer uh we can reduce the prompt uh we can make it smaller uh doing something like this so again we have question we have context we have uh we ask it to give the answer so let's see if it helps so while it's doing that uh we can go back and yeah I think yes you can still join the course yeah I wouldn't say it's great uh cuz like it continuous um generating some stuff right so maybe it's just too too long we need to experiment with this uh maybe max length could be just 200 um yeah okay and uh because we still get like the entire prompt we can avoid that\",\n",
       "  'timecode_text': 'Debugging and Making the Output Better',\n",
       "  'description': 'In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.',\n",
       "  'link': 'https://www.youtube.com/watch?v=TdVEOzSoUCs&t=800s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication because it's a course about LM and yeah first it actually suggested to use pipeline we I think think we used pipeline in P3 and uh to create a generator which I think is a convenient way of doing that so instead of invoking tokenizer and model separately we just put it inside a generator so this is what I did here created the generator and then uh I tried a smaller prompt so it gave me a bunch of parameters that we can try but I think the main thing is it does not seem to like large prompts so I cut the um the prompt so I only kept two uh answers um and then it gave some response um yeah we can also tweak The Prompt let me take a copy yeah I think yeah it says yes you can still join the course um we can also tweak it a little bit so did I make a copy yeah so we can try to tweak The Prompt so instead of uh doing it this way maybe we can reduce the answer uh we can reduce the prompt uh we can make it smaller uh doing something like this so again we have question we have context we have uh we ask it to give the answer so let's see if it helps so while it's doing that uh we can go back and yeah I think yes you can still join the course yeah I wouldn't say it's great uh cuz like it continuous um generating some stuff right so maybe it's just too too long we need to experiment with this uh maybe max length could be just 200 um yeah okay and uh because we still get like the entire prompt we can avoid that In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.\",\n",
       "  'id': '445c2d902f839b97c8415979c6998894'},\n",
       " {'vid_id': 'TdVEOzSoUCs',\n",
       "  'title': 'LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication',\n",
       "  'timecode': '15:45',\n",
       "  'text': \"by focusing only on um uh response let's print it from um uh so we kind of just cut the beginning and then we do strip so now it will also it will only know it will only show us the rest after completion and this is kind of yeah not great okay but I I think that's uh that's enough um yeah I'm a bit disappointed with this model um yeah and then after that we just put again we adjust uh\",\n",
       "  'timecode_text': 'Cutting the Output',\n",
       "  'description': 'In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.',\n",
       "  'link': 'https://www.youtube.com/watch?v=TdVEOzSoUCs&t=945s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication by focusing only on um uh response let's print it from um uh so we kind of just cut the beginning and then we do strip so now it will also it will only know it will only show us the rest after completion and this is kind of yeah not great okay but I I think that's uh that's enough um yeah I'm a bit disappointed with this model um yeah and then after that we just put again we adjust uh In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.\",\n",
       "  'id': '9587f57aba3ff79136eca809abb26480'},\n",
       " {'vid_id': 'TdVEOzSoUCs',\n",
       "  'title': 'LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication',\n",
       "  'timecode': '16:40',\n",
       "  'text': \"adjust this context uh we also need to adjust where like actually search is good I think the buil prompt yeah so need we will need to change here well let me do this since since I already started and then we do it like that and then generator I'll put it here and we just need tol that part um and instead of print we return it right wow response final okay let's see yeah okay uh cuz we did not adjust the we still get five top results so let's see if actually it can use five what's happening um so let's let me go back to this and update it to let's say three results and\",\n",
       "  'timecode_text': 'Putting it to the RAG Flow',\n",
       "  'description': 'In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.',\n",
       "  'link': 'https://www.youtube.com/watch?v=TdVEOzSoUCs&t=1000s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication adjust this context uh we also need to adjust where like actually search is good I think the buil prompt yeah so need we will need to change here well let me do this since since I already started and then we do it like that and then generator I'll put it here and we just need tol that part um and instead of print we return it right wow response final okay let's see yeah okay uh cuz we did not adjust the we still get five top results so let's see if actually it can use five what's happening um so let's let me go back to this and update it to let's say three results and In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.\",\n",
       "  'id': '7712e8e44c0ce86b2c08db25a2dabb5d'},\n",
       " {'vid_id': 'TdVEOzSoUCs',\n",
       "  'title': 'LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication',\n",
       "  'timecode': '18:45',\n",
       "  'text': \"500 probably it will now output some nonsense too okay while it's doing there is another thing I wanted to talk to you about and this is uh what I talked to to chpd about is um I asked it so we need to download um the model here where so we need to download the model here and we also require authentication right and this is a problem if we want to use this model later uh let's say to put it in a Docker image and serve it somehow we don't want to always rely on hugging face to First authenticate there and download the model so what we can do is take take the models download them like we did here in these videos and then save them save them locally and then just load them so this is how we do this I'll put this code also in um in the notes and this is how you can first download the models and then save them for later use and they even have some code for serving this with fast API so you can also use it so I will include all this code in the notes and with that I just want to see yeah actually at the end the results the result was good I guess I was just lucky I hit the right uh parameters but as you saw like it required some tuning unlike the other models that worked good out of the box that this one was a bit more like it required more tuning okay so that's all I wanted to talk about and in the next video we will talk about other open source models see you soon\",\n",
       "  'timecode_text': 'Serving HuggingFace models',\n",
       "  'description': 'In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.',\n",
       "  'link': 'https://www.youtube.com/watch?v=TdVEOzSoUCs&t=1125s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.5 - Mistral-7B and HuggingFace Hub Authentication 500 probably it will now output some nonsense too okay while it's doing there is another thing I wanted to talk to you about and this is uh what I talked to to chpd about is um I asked it so we need to download um the model here where so we need to download the model here and we also require authentication right and this is a problem if we want to use this model later uh let's say to put it in a Docker image and serve it somehow we don't want to always rely on hugging face to First authenticate there and download the model so what we can do is take take the models download them like we did here in these videos and then save them save them locally and then just load them so this is how we do this I'll put this code also in um in the notes and this is how you can first download the models and then save them for later use and they even have some code for serving this with fast API so you can also use it so I will include all this code in the notes and with that I just want to see yeah actually at the end the results the result was good I guess I was just lucky I hit the right uh parameters but as you saw like it required some tuning unlike the other models that worked good out of the box that this one was a bit more like it required more tuning okay so that's all I wanted to talk about and in the next video we will talk about other open source models see you soon In this video, I delve into the Mistral 7b model, showcasing how to access it and the necessary steps to take for authorization. Additionally, I introduce the LLM leaderboard, highlighting its significance and how it can be utilized for model performance evaluation. Viewers are encouraged to follow along as we navigate through these models and their functionalities.\",\n",
       "  'id': '237059bdae32c5a09b796d53372d4bc5'},\n",
       " {'vid_id': 'GzPV_HTmCkc',\n",
       "  'title': 'LLM Zoomcamp 2.6 - Exploring Open Source LLMs ',\n",
       "  'timecode': '0:00',\n",
       "  'text': 'welcome back this is llm zoom camp and in this module we are talking about open source llms we have already talked about uh multiple LMS such as FL T5 then we talked about V3 we talked about Mr l7b and there are many many many models so for example this one llm 360 Ember this is a model too and usually',\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, I look at open source LLMs, discussing various models like LAN v5, Mistral 7b, and more. I highlight the process of finding and using these models, emphasizing the importance of benchmarks and performance leaderboards. Viewers are encouraged to experiment with different queries, prompts, and models to find what works best for them.',\n",
       "  'link': 'https://www.youtube.com/watch?v=GzPV_HTmCkc&t=0s',\n",
       "  'text_vector': 'LLM Zoomcamp 2.6 - Exploring Open Source LLMs  welcome back this is llm zoom camp and in this module we are talking about open source llms we have already talked about uh multiple LMS such as FL T5 then we talked about V3 we talked about Mr l7b and there are many many many models so for example this one llm 360 Ember this is a model too and usually In this video, I look at open source LLMs, discussing various models like LAN v5, Mistral 7b, and more. I highlight the process of finding and using these models, emphasizing the importance of benchmarks and performance leaderboards. Viewers are encouraged to experiment with different queries, prompts, and models to find what works best for them.',\n",
       "  'id': 'd77ca650986910e135fd14862dc6e1b1'},\n",
       " {'vid_id': 'GzPV_HTmCkc',\n",
       "  'title': 'LLM Zoomcamp 2.6 - Exploring Open Source LLMs ',\n",
       "  'timecode': '0:29',\n",
       "  'text': \"you just look it up you'll find a huging face description there is um piece of code that you can use for um for using the model so you plug and play you test it and you see if it works well then you use it so where do we find all these models like there are many many many models um um we talked about three I mentioned December where do we find other models like that well usually if there is a good model you might see um people talking about it on Twitter other social media uh another good source and this is\",\n",
       "  'timecode_text': 'LLM360/Amber',\n",
       "  'description': 'In this video, I look at open source LLMs, discussing various models like LAN v5, Mistral 7b, and more. I highlight the process of finding and using these models, emphasizing the importance of benchmarks and performance leaderboards. Viewers are encouraged to experiment with different queries, prompts, and models to find what works best for them.',\n",
       "  'link': 'https://www.youtube.com/watch?v=GzPV_HTmCkc&t=29s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.6 - Exploring Open Source LLMs  you just look it up you'll find a huging face description there is um piece of code that you can use for um for using the model so you plug and play you test it and you see if it works well then you use it so where do we find all these models like there are many many many models um um we talked about three I mentioned December where do we find other models like that well usually if there is a good model you might see um people talking about it on Twitter other social media uh another good source and this is In this video, I look at open source LLMs, discussing various models like LAN v5, Mistral 7b, and more. I highlight the process of finding and using these models, emphasizing the importance of benchmarks and performance leaderboards. Viewers are encouraged to experiment with different queries, prompts, and models to find what works best for them.\",\n",
       "  'id': 'a8830233e7272b097b6bf814e1795276'},\n",
       " {'vid_id': 'GzPV_HTmCkc',\n",
       "  'title': 'LLM Zoomcamp 2.6 - Exploring Open Source LLMs ',\n",
       "  'timecode': '1:16',\n",
       "  'text': \"something we talked about in the last video is uh benchmarks so this is the tutorial we used in the uh last video uh I think in this tutorial we used Mr 7B model if if we scroll all the way down we will see two leaderboards this open llm leaderboard and open llm perf leaderboard so I am going to uh put these links here so then you can also access them and on this Le board on these lead boards um you can see how different models how different llms uh perform okay this one is still loading let's see this one right so there is a certain\",\n",
       "  'timecode_text': 'Leaderboards',\n",
       "  'description': 'In this video, I look at open source LLMs, discussing various models like LAN v5, Mistral 7b, and more. I highlight the process of finding and using these models, emphasizing the importance of benchmarks and performance leaderboards. Viewers are encouraged to experiment with different queries, prompts, and models to find what works best for them.',\n",
       "  'link': 'https://www.youtube.com/watch?v=GzPV_HTmCkc&t=76s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.6 - Exploring Open Source LLMs  something we talked about in the last video is uh benchmarks so this is the tutorial we used in the uh last video uh I think in this tutorial we used Mr 7B model if if we scroll all the way down we will see two leaderboards this open llm leaderboard and open llm perf leaderboard so I am going to uh put these links here so then you can also access them and on this Le board on these lead boards um you can see how different models how different llms uh perform okay this one is still loading let's see this one right so there is a certain In this video, I look at open source LLMs, discussing various models like LAN v5, Mistral 7b, and more. I highlight the process of finding and using these models, emphasizing the importance of benchmarks and performance leaderboards. Viewers are encouraged to experiment with different queries, prompts, and models to find what works best for them.\",\n",
       "  'id': '21ce9efcd30150f89b698306cbde2528'},\n",
       " {'vid_id': 'GzPV_HTmCkc',\n",
       "  'title': 'LLM Zoomcamp 2.6 - Exploring Open Source LLMs ',\n",
       "  'timecode': '2:06',\n",
       "  'text': \"score and then um the models are ranked on this score and this on this llm perf leaderboard uh the focus is on performance and you can see different models um well these large models you need to have a fairly large GPU to fit them um in order to hold in memory we're in general for smaller gpus uh the ones that we can access we can use on Souther cloud or collab or other similar platforms usually we look for 7B models so this 14b is rather large so we can scroll uh all all the way down so this 8B should also work so we can try this metal Lama 3 uh for example um yeah like or this quen 7B uh or this Desi 7B um so in general s or 8B models work well especially if you try with um this quantization quantization is when we uh reduce uh like the when we use um smaller data types like instead of using um f 64 FL bits for float we use less uh fewer bits like 16 bits for example so the model become less precise but they become faster right so you can this is one place where you can find um other models and then of course you click on the model and then you can see how to load the model right well in most cases for Mr L 7B for some reasons we don't have a description but usually you can find a description somewhere so that's one thing that's one place uh then we also like there was this other leaderboard right the same thing right so here I think the focus is not on performance\",\n",
       "  'timecode_text': 'Perf LLM Leaderboard',\n",
       "  'description': 'In this video, I look at open source LLMs, discussing various models like LAN v5, Mistral 7b, and more. I highlight the process of finding and using these models, emphasizing the importance of benchmarks and performance leaderboards. Viewers are encouraged to experiment with different queries, prompts, and models to find what works best for them.',\n",
       "  'link': 'https://www.youtube.com/watch?v=GzPV_HTmCkc&t=126s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.6 - Exploring Open Source LLMs  score and then um the models are ranked on this score and this on this llm perf leaderboard uh the focus is on performance and you can see different models um well these large models you need to have a fairly large GPU to fit them um in order to hold in memory we're in general for smaller gpus uh the ones that we can access we can use on Souther cloud or collab or other similar platforms usually we look for 7B models so this 14b is rather large so we can scroll uh all all the way down so this 8B should also work so we can try this metal Lama 3 uh for example um yeah like or this quen 7B uh or this Desi 7B um so in general s or 8B models work well especially if you try with um this quantization quantization is when we uh reduce uh like the when we use um smaller data types like instead of using um f 64 FL bits for float we use less uh fewer bits like 16 bits for example so the model become less precise but they become faster right so you can this is one place where you can find um other models and then of course you click on the model and then you can see how to load the model right well in most cases for Mr L 7B for some reasons we don't have a description but usually you can find a description somewhere so that's one thing that's one place uh then we also like there was this other leaderboard right the same thing right so here I think the focus is not on performance In this video, I look at open source LLMs, discussing various models like LAN v5, Mistral 7b, and more. I highlight the process of finding and using these models, emphasizing the importance of benchmarks and performance leaderboards. Viewers are encouraged to experiment with different queries, prompts, and models to find what works best for them.\",\n",
       "  'id': '3e26e33671e4d9acb146a15c7ba9b3db'},\n",
       " {'vid_id': 'GzPV_HTmCkc',\n",
       "  'title': 'LLM Zoomcamp 2.6 - Exploring Open Source LLMs ',\n",
       "  'timecode': '4:17',\n",
       "  'text': \"but on uh quality and that's why we have this 72b 70b models so they will not fit on uh gpus that we typically have okay so this uh this one should um should actually work but anyways so this is the place where you can find the model these two benchmarks um and also of course you can\",\n",
       "  'timecode_text': 'LLM Leaderboard',\n",
       "  'description': 'In this video, I look at open source LLMs, discussing various models like LAN v5, Mistral 7b, and more. I highlight the process of finding and using these models, emphasizing the importance of benchmarks and performance leaderboards. Viewers are encouraged to experiment with different queries, prompts, and models to find what works best for them.',\n",
       "  'link': 'https://www.youtube.com/watch?v=GzPV_HTmCkc&t=257s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.6 - Exploring Open Source LLMs  but on uh quality and that's why we have this 72b 70b models so they will not fit on uh gpus that we typically have okay so this uh this one should um should actually work but anyways so this is the place where you can find the model these two benchmarks um and also of course you can In this video, I look at open source LLMs, discussing various models like LAN v5, Mistral 7b, and more. I highlight the process of finding and using these models, emphasizing the importance of benchmarks and performance leaderboards. Viewers are encouraged to experiment with different queries, prompts, and models to find what works best for them.\",\n",
       "  'id': '381c1c388758e6b92157f27ec8bd6344'},\n",
       " {'vid_id': 'GzPV_HTmCkc',\n",
       "  'title': 'LLM Zoomcamp 2.6 - Exploring Open Source LLMs ',\n",
       "  'timecode': '4:45',\n",
       "  'text': \"just ask uh chat GPT so this is what I asked look um up the recent 7B llm open source models available on hen face Hub and then cuz I asked it specifically to look up then it performed this search using and then it summarized the results and then the model it found was uh GMA or Gemma from Google um this one all LM I have no idea what this is so M trial we covered and then like this and this I also have on test it so you can experiment with um similar queries similar prompts uh you can check the performance leaderboards and and test these things and see what works for you and if you like some models feel free to use them okay did I want to cover anything else leaderboards Google chpt so here what I meant by Google is you can just Google like open source llm 7B and then you will find different announcements and of course uh social media also is a good source and I think I already mentioned that so these are the places where you can find other open source models and for now that's all um when it comes to running larger model on gpus and in the next video we will run a an llm on a CPU so sometimes we don't have a GPU at our disposal so this is what we will do um when we don't have a GPU we will run them on the CPU and this is what we will talk about see you soon\",\n",
       "  'timecode_text': 'Using ChatGPT',\n",
       "  'description': 'In this video, I look at open source LLMs, discussing various models like LAN v5, Mistral 7b, and more. I highlight the process of finding and using these models, emphasizing the importance of benchmarks and performance leaderboards. Viewers are encouraged to experiment with different queries, prompts, and models to find what works best for them.',\n",
       "  'link': 'https://www.youtube.com/watch?v=GzPV_HTmCkc&t=285s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.6 - Exploring Open Source LLMs  just ask uh chat GPT so this is what I asked look um up the recent 7B llm open source models available on hen face Hub and then cuz I asked it specifically to look up then it performed this search using and then it summarized the results and then the model it found was uh GMA or Gemma from Google um this one all LM I have no idea what this is so M trial we covered and then like this and this I also have on test it so you can experiment with um similar queries similar prompts uh you can check the performance leaderboards and and test these things and see what works for you and if you like some models feel free to use them okay did I want to cover anything else leaderboards Google chpt so here what I meant by Google is you can just Google like open source llm 7B and then you will find different announcements and of course uh social media also is a good source and I think I already mentioned that so these are the places where you can find other open source models and for now that's all um when it comes to running larger model on gpus and in the next video we will run a an llm on a CPU so sometimes we don't have a GPU at our disposal so this is what we will do um when we don't have a GPU we will run them on the CPU and this is what we will talk about see you soon In this video, I look at open source LLMs, discussing various models like LAN v5, Mistral 7b, and more. I highlight the process of finding and using these models, emphasizing the importance of benchmarks and performance leaderboards. Viewers are encouraged to experiment with different queries, prompts, and models to find what works best for them.\",\n",
       "  'id': 'd4c808035b5e85ac5b0119458b932d4f'},\n",
       " {'vid_id': 'PVpBGs_iSjY',\n",
       "  'title': 'LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hi everyone in this video we want to run an llm locally on a machine without a GPU so I'll use my laptop we will also use cod spaces um and we will use a special tool for that a special service called orama so Alama is an open source\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=PVpBGs_iSjY&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama hi everyone in this video we want to run an llm locally on a machine without a GPU so I'll use my laptop we will also use cod spaces um and we will use a special tool for that a special service called orama so Alama is an open source In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.\",\n",
       "  'id': '2e0a7daae878c8bc834eeb266e77f72d'},\n",
       " {'vid_id': 'PVpBGs_iSjY',\n",
       "  'title': 'LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama',\n",
       "  'timecode': '00:18',\n",
       "  'text': \"project we can see it here um and this is a service running LM locally as I said and you can run it on Windows Linux um Mac OS actually I am on Windows and for Windows I use this um installer and with the installer it's the easiest way to get AMA on Windows but we will use it on Linux and what is more uh it serves as a drop drop in replacement for open AI API so previously in module one we used open AI the only thing we can change there the only thing we need to change there in order to start using all instead of open AI is this base URL so we can point uh the SDK or the API library to use Alama instead of open Ai and then we can just do the whole thing locally so let's start for that we will create a code\",\n",
       "  'timecode_text': 'Ollama',\n",
       "  'description': 'In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=PVpBGs_iSjY&t=18s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama project we can see it here um and this is a service running LM locally as I said and you can run it on Windows Linux um Mac OS actually I am on Windows and for Windows I use this um installer and with the installer it's the easiest way to get AMA on Windows but we will use it on Linux and what is more uh it serves as a drop drop in replacement for open AI API so previously in module one we used open AI the only thing we can change there the only thing we need to change there in order to start using all instead of open AI is this base URL so we can point uh the SDK or the API library to use Alama instead of open Ai and then we can just do the whole thing locally so let's start for that we will create a code In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.\",\n",
       "  'id': '65546345bc71de5b15aa0d712fabfad1'},\n",
       " {'vid_id': 'PVpBGs_iSjY',\n",
       "  'title': 'LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama',\n",
       "  'timecode': '01:21',\n",
       "  'text': \"space I will create like we in order to run LMS locally we need a more powerful machine than the default uh settings gives us so we start new with options and machine type is four course I think when you run this machine you use two hours of uh compute instead of one for one hour cuz it's two times more powerful and let me now open it locally so yeah it's open in Visual Studio desktop for some reasons I always need to click twice I don't know why maybe it's the same thing for you and so this is uh this is a Linux machine so we will need to follow the instructions for Linux which is this one so it's still setting up yeah it started now we just copy this command\",\n",
       "  'timecode_text': 'Configuring Codespaces',\n",
       "  'description': 'In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=PVpBGs_iSjY&t=81s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama space I will create like we in order to run LMS locally we need a more powerful machine than the default uh settings gives us so we start new with options and machine type is four course I think when you run this machine you use two hours of uh compute instead of one for one hour cuz it's two times more powerful and let me now open it locally so yeah it's open in Visual Studio desktop for some reasons I always need to click twice I don't know why maybe it's the same thing for you and so this is uh this is a Linux machine so we will need to follow the instructions for Linux which is this one so it's still setting up yeah it started now we just copy this command In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.\",\n",
       "  'id': 'f09b5652ed0e9a2bce78f06a146e400e'},\n",
       " {'vid_id': 'PVpBGs_iSjY',\n",
       "  'title': 'LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama',\n",
       "  'timecode': '02:45',\n",
       "  'text': \"this is an empty Cod space clean one I did not do anything and now we just do aama start so now we have the server and we open a new tab and do Lama run P3 so this is a model we used previously from Microsoft and now we will just run it locally on a machine without the GPU so this is a small P3 um so it's I think with we have a list here I think it's yeah with uh E3 mini this one so it's rather small one that's what we did in order to actually run it just do Alan P3 and we can ask how like our favorite my favorite question I just discovered the course can I still join it right and then it will give us some answer but of course we did not give it any context and now we can try to actually use use a prompt okay prepare the prompt so in this prompt we have context so we have a question we have the context and we have like we ask it for the answer and let me just put it and right now cuz like previously you saw it started answering right away cu the prompt was quite small but now it needs to analyze the prompt uh and then based on that answer the question so I'll it's not super fast even though um like it uses the four course it also takes some time for that is why I just want to use a llama on my local\",\n",
       "  'timecode_text': 'Running Ollama',\n",
       "  'description': 'In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=PVpBGs_iSjY&t=165s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama this is an empty Cod space clean one I did not do anything and now we just do aama start so now we have the server and we open a new tab and do Lama run P3 so this is a model we used previously from Microsoft and now we will just run it locally on a machine without the GPU so this is a small P3 um so it's I think with we have a list here I think it's yeah with uh E3 mini this one so it's rather small one that's what we did in order to actually run it just do Alan P3 and we can ask how like our favorite my favorite question I just discovered the course can I still join it right and then it will give us some answer but of course we did not give it any context and now we can try to actually use use a prompt okay prepare the prompt so in this prompt we have context so we have a question we have the context and we have like we ask it for the answer and let me just put it and right now cuz like previously you saw it started answering right away cu the prompt was quite small but now it needs to analyze the prompt uh and then based on that answer the question so I'll it's not super fast even though um like it uses the four course it also takes some time for that is why I just want to use a llama on my local In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.\",\n",
       "  'id': '615b2017c2035b5a2f0b9e2092b40ec3'},\n",
       " {'vid_id': 'PVpBGs_iSjY',\n",
       "  'title': 'LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama',\n",
       "  'timecode': '04:43',\n",
       "  'text': \"machine it should be a bit faster yeah you see it already started um answering something yeah I think right now locally I'm downloading the model or no I don't know but locally I have eight cores so it should be a bit faster than um on C spaces well we see that uh yeah it gives an answer it's a bit verbose and then um also the prompt probably needs some tweaking because it starts going into um answering these questions too so it started first correctly but then yeah I'll just stop it uh it's just it's not that it's a bad model we just need to do a bit of prompt engineering to give it good instructions what it should do and what it should not do for now we just stop uh it so it's still it's still doing something and um so this is how we do it and I wanted to also show you how we can actually use it as a dropin replace bement um let me actually check the plan um so yeah so right now I wanted to show you how to use it as a dropin replacement for open AI uh\",\n",
       "  'timecode_text': 'Executing the Model Locally',\n",
       "  'description': 'In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=PVpBGs_iSjY&t=283s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama machine it should be a bit faster yeah you see it already started um answering something yeah I think right now locally I'm downloading the model or no I don't know but locally I have eight cores so it should be a bit faster than um on C spaces well we see that uh yeah it gives an answer it's a bit verbose and then um also the prompt probably needs some tweaking because it starts going into um answering these questions too so it started first correctly but then yeah I'll just stop it uh it's just it's not that it's a bad model we just need to do a bit of prompt engineering to give it good instructions what it should do and what it should not do for now we just stop uh it so it's still it's still doing something and um so this is how we do it and I wanted to also show you how we can actually use it as a dropin replace bement um let me actually check the plan um so yeah so right now I wanted to show you how to use it as a dropin replacement for open AI uh In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.\",\n",
       "  'id': 'fe9266ed5993e2cf4e3a5b8441282a4c'},\n",
       " {'vid_id': 'PVpBGs_iSjY',\n",
       "  'title': 'LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama',\n",
       "  'timecode': '06:14',\n",
       "  'text': \"I'll do things locally cuz it's a bit faster than doing this on COD spaces but on COD spaces it's the same so I'm going to use uh we have this starter notebook so I'm going to start jupyter notebook here and use the starter notebook so let's open it and this is the code we used previously so I will uh we'll execute it now and this is the same thing we did previously right and if you remember this is we made a call to gbt 40 using um open AI client so now we will use this thing instead of that so now we are connecting to um the AMA server and when we do this let me just uh I'll change GPT 40 to P3 and then I'll just do LM right that that this is a test yeah because we saw that the answers are somewhat rebos for this model we need to tweak probably the parameters and also to tweak the The Prompt so right now it will take probably some time to uh to return an answer I'll just put it on pause okay [Music] so I actually let's see what it output yeah well it's kind of nonsense but like okay it's funny um yeah it definitely needs a bit of uh tuning when we want to use it um and I think actually we can do that that um when we customize a model yeah this where customize a model so we can uh change the parameters I will not do it here but you're free to experiment with this uh model files yeah so this is how we do it and\",\n",
       "  'timecode_text': 'Replacement for OpenAI API',\n",
       "  'description': 'In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=PVpBGs_iSjY&t=374s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama I'll do things locally cuz it's a bit faster than doing this on COD spaces but on COD spaces it's the same so I'm going to use uh we have this starter notebook so I'm going to start jupyter notebook here and use the starter notebook so let's open it and this is the code we used previously so I will uh we'll execute it now and this is the same thing we did previously right and if you remember this is we made a call to gbt 40 using um open AI client so now we will use this thing instead of that so now we are connecting to um the AMA server and when we do this let me just uh I'll change GPT 40 to P3 and then I'll just do LM right that that this is a test yeah because we saw that the answers are somewhat rebos for this model we need to tweak probably the parameters and also to tweak the The Prompt so right now it will take probably some time to uh to return an answer I'll just put it on pause okay [Music] so I actually let's see what it output yeah well it's kind of nonsense but like okay it's funny um yeah it definitely needs a bit of uh tuning when we want to use it um and I think actually we can do that that um when we customize a model yeah this where customize a model so we can uh change the parameters I will not do it here but you're free to experiment with this uh model files yeah so this is how we do it and In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.\",\n",
       "  'id': '2a5bad15e8ebcd332db623e5b73bfc76'},\n",
       " {'vid_id': 'PVpBGs_iSjY',\n",
       "  'title': 'LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama',\n",
       "  'timecode': '08:50',\n",
       "  'text': \"last thing I wanted to show you is running it in Docker so this is the comment we can use for running the docker um let me actually do it here in spaces so and then because I'm on Windows I want to stop the server so I had this small uh llama icon in my tray so I want to make sure that when I make a call to the AMA server uh I make a call to this Docker thing like did do I did not do p port forwarding I think because I no I did not forget actually what's the P this one let's add this and I want to make sure that it's it's running I'll do tet tet Local Host and then the boort yeah it's able to connect um so now I can just go to this thing and execute it one more time and and we here just yeah we just replac it uh cuz right\",\n",
       "  'timecode_text': 'Running Olama in Docker',\n",
       "  'description': 'In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=PVpBGs_iSjY&t=530s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama last thing I wanted to show you is running it in Docker so this is the comment we can use for running the docker um let me actually do it here in spaces so and then because I'm on Windows I want to stop the server so I had this small uh llama icon in my tray so I want to make sure that when I make a call to the AMA server uh I make a call to this Docker thing like did do I did not do p port forwarding I think because I no I did not forget actually what's the P this one let's add this and I want to make sure that it's it's running I'll do tet tet Local Host and then the boort yeah it's able to connect um so now I can just go to this thing and execute it one more time and and we here just yeah we just replac it uh cuz right In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.\",\n",
       "  'id': '0f1f0e3ebe562982051ddda726925b59'},\n",
       " {'vid_id': 'PVpBGs_iSjY',\n",
       "  'title': 'LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama',\n",
       "  'timecode': '10:34',\n",
       "  'text': \"now we run a Docker we need to download a model so previously when we did I think when we did um where's our other terminal so when we did all laran F3 it was downloading the model right so right now it's running inside Docker and we need to do a similar thing so I'll show you how to actually do it from uh with from from inside the docker container so I'll do Docker PS so this is the name that we're interested in the name of the container I will Docker exact to execute a command minus it to be in in the interactive mode bash the command I want to execute and then of course I forgot the name all Lama ex itama and then we're inside and I need to sayama pool E3 we just need to do it once cuz here when we run a Docker container we do volume mapping so we map a volume name with the name AMA to/ root. olama this is the place where AMA puts the models to so when we do it once when we pull a model once it stays there and next time we execute it will just uh use the model from the volume okay did it finish and now we can execute it so it will take some time cuz it's running in COD spaces I think I over complicated things a little bit uh I could have just run the whole thing think locally I don't know why I needed to run in Docker here on COD spaces instead of just running Docker locally but actually doesn't make any difference you can run just on COD spaces just locally doesn't make any difference except like if you want to run it locally you need to have your target like the the binary you're downloading or the installer should much your platform obviously okay did it give the answer no it's still uh it's still thinking okay and now it actually gave us T in bash python markdown okay that's very funny um so this is how we can run models locally with AMA we used the V3 model but you can use any other model from this list um also for example Llama Or GMA or whatever so yeah en enjoy that and yeah have fun and in the next video we will look at how to put everything together how we take elastic search how we take aama and how we put everything in Docker compost and run it a single thing see you soon\",\n",
       "  'timecode_text': 'Pulling a model inside Docker',\n",
       "  'description': 'In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.',\n",
       "  'link': 'https://www.youtube.com/watch?v=PVpBGs_iSjY&t=634s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.7 - Running LLMs Locally without a GPU with Ollama now we run a Docker we need to download a model so previously when we did I think when we did um where's our other terminal so when we did all laran F3 it was downloading the model right so right now it's running inside Docker and we need to do a similar thing so I'll show you how to actually do it from uh with from from inside the docker container so I'll do Docker PS so this is the name that we're interested in the name of the container I will Docker exact to execute a command minus it to be in in the interactive mode bash the command I want to execute and then of course I forgot the name all Lama ex itama and then we're inside and I need to sayama pool E3 we just need to do it once cuz here when we run a Docker container we do volume mapping so we map a volume name with the name AMA to/ root. olama this is the place where AMA puts the models to so when we do it once when we pull a model once it stays there and next time we execute it will just uh use the model from the volume okay did it finish and now we can execute it so it will take some time cuz it's running in COD spaces I think I over complicated things a little bit uh I could have just run the whole thing think locally I don't know why I needed to run in Docker here on COD spaces instead of just running Docker locally but actually doesn't make any difference you can run just on COD spaces just locally doesn't make any difference except like if you want to run it locally you need to have your target like the the binary you're downloading or the installer should much your platform obviously okay did it give the answer no it's still uh it's still thinking okay and now it actually gave us T in bash python markdown okay that's very funny um so this is how we can run models locally with AMA we used the V3 model but you can use any other model from this list um also for example Llama Or GMA or whatever so yeah en enjoy that and yeah have fun and in the next video we will look at how to put everything together how we take elastic search how we take aama and how we put everything in Docker compost and run it a single thing see you soon In this video, I demonstrate how to run LLMs locally on a machine without a GPU. We utilize Codespaces and Ollama to achieve this. By following the steps for Linux setup, we can switch from OpenAI to Olama for local processing. I showcase the process, including creating a codespace and executing the model.\",\n",
       "  'id': 'cf9f1afef299874d2c847e6dbb8c1c8b'},\n",
       " {'vid_id': '4juoo_jk96U',\n",
       "  'title': 'LLM Zoomcamp 2.8 - Ollama + Elastic in Docker Compose',\n",
       "  'timecode': '0:00',\n",
       "  'text': 'hey everyone in this video we are going to put a Lama and elastic search together in one Docker compost file we are going to and we are going to uh take the notebook from module one and run it completely locally with replacing uh open AI with a llama and putting elastic search in Doos uh in the previous video we discussed Lama we saw how to actually use it as a dropin replacement for uh open AI API when we run things locally now we will build on top of that and we will take the document uh the The Notebook this notebook and um adjust it so we will start with creating a Docker compost file so we will need these two commands uh so this one and this one and I will just ask uh chbt to',\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, I demonstrate how to integrate Ollama and Elasticsearch in a docker-compose file. We replace OpenAI with Ollama, set up a docker-compose yaml file, pull the necessary model, and answer user questions.',\n",
       "  'link': 'https://www.youtube.com/watch?v=4juoo_jk96U&t=0s',\n",
       "  'text_vector': 'LLM Zoomcamp 2.8 - Ollama + Elastic in Docker Compose hey everyone in this video we are going to put a Lama and elastic search together in one Docker compost file we are going to and we are going to uh take the notebook from module one and run it completely locally with replacing uh open AI with a llama and putting elastic search in Doos uh in the previous video we discussed Lama we saw how to actually use it as a dropin replacement for uh open AI API when we run things locally now we will build on top of that and we will take the document uh the The Notebook this notebook and um adjust it so we will start with creating a Docker compost file so we will need these two commands uh so this one and this one and I will just ask uh chbt to In this video, I demonstrate how to integrate Ollama and Elasticsearch in a docker-compose file. We replace OpenAI with Ollama, set up a docker-compose yaml file, pull the necessary model, and answer user questions.',\n",
       "  'id': '01589623c7c585bfa4867817590d25b5'},\n",
       " {'vid_id': '4juoo_jk96U',\n",
       "  'title': 'LLM Zoomcamp 2.8 - Ollama + Elastic in Docker Compose',\n",
       "  'timecode': '1:00',\n",
       "  'text': \"create me a Docker composto file I want to run it with Docker compos and this one too so now it hopefully should create a yo file that I while it's typing I'll just create uh Docker compose yammo and we will put this thing here so what it did is it just translated the docker uh commands we put in terminal to Docker compose yaml files uh yaml definitions right so this uh this and this are identical they do the same thing and this and this are identical they do the same thing so now we just run it with Docker compose so I do Docker compose up and it's stting elastic search it's starting AMA uh remember for AMA we need to pull a model locally if we if you haven't done that so I'll just\",\n",
       "  'timecode_text': 'Creating a Docker-Compose file',\n",
       "  'description': 'In this video, I demonstrate how to integrate Ollama and Elasticsearch in a docker-compose file. We replace OpenAI with Ollama, set up a docker-compose yaml file, pull the necessary model, and answer user questions.',\n",
       "  'link': 'https://www.youtube.com/watch?v=4juoo_jk96U&t=60s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.8 - Ollama + Elastic in Docker Compose create me a Docker composto file I want to run it with Docker compos and this one too so now it hopefully should create a yo file that I while it's typing I'll just create uh Docker compose yammo and we will put this thing here so what it did is it just translated the docker uh commands we put in terminal to Docker compose yaml files uh yaml definitions right so this uh this and this are identical they do the same thing and this and this are identical they do the same thing so now we just run it with Docker compose so I do Docker compose up and it's stting elastic search it's starting AMA uh remember for AMA we need to pull a model locally if we if you haven't done that so I'll just In this video, I demonstrate how to integrate Ollama and Elasticsearch in a docker-compose file. We replace OpenAI with Ollama, set up a docker-compose yaml file, pull the necessary model, and answer user questions.\",\n",
       "  'id': '697ac0baea66adfa73bdb09a43f5de68'},\n",
       " {'vid_id': '4juoo_jk96U',\n",
       "  'title': 'LLM Zoomcamp 2.8 - Ollama + Elastic in Docker Compose',\n",
       "  'timecode': '2:20',\n",
       "  'text': \"do I'll just do Docker PS to know the list of containers so this is what we need orama I'll just say Docker exac minus AMA bsh Umama V3 okay so it's pulling the model and both AMA and elastic s started and what we can do in the meantime is copy uh take the I'll just do crl c contrl v here so we will now uh rework this rck\",\n",
       "  'timecode_text': 'Pulling the Model',\n",
       "  'description': 'In this video, I demonstrate how to integrate Ollama and Elasticsearch in a docker-compose file. We replace OpenAI with Ollama, set up a docker-compose yaml file, pull the necessary model, and answer user questions.',\n",
       "  'link': 'https://www.youtube.com/watch?v=4juoo_jk96U&t=140s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.8 - Ollama + Elastic in Docker Compose do I'll just do Docker PS to know the list of containers so this is what we need orama I'll just say Docker exac minus AMA bsh Umama V3 okay so it's pulling the model and both AMA and elastic s started and what we can do in the meantime is copy uh take the I'll just do crl c contrl v here so we will now uh rework this rck In this video, I demonstrate how to integrate Ollama and Elasticsearch in a docker-compose file. We replace OpenAI with Ollama, set up a docker-compose yaml file, pull the necessary model, and answer user questions.\",\n",
       "  'id': '77dd6840c6981f3a12f288513c00b423'},\n",
       " {'vid_id': '4juoo_jk96U',\n",
       "  'title': 'LLM Zoomcamp 2.8 - Ollama + Elastic in Docker Compose',\n",
       "  'timecode': '3:10',\n",
       "  'text': \"intro IPython notebook jupyter notebook um and replace uh the call to open AI uh with a llama okay so uh now there's a lot of I think in this notebook there is a bunch of things we don't need um yeah we don't need all that we will need this and actually I need to uh we need to replace that right I think I'll just take it from here and we don't need that um when it comes to query yeah I think I will it's safe to just remove all these things and now we um yeah I don't have elastic search search have a fresh environment I should have installed it um so we will execute it now create an index like last time and um yeah index the documents create a query and then perform the search um while it's doing that I realized um that we still need documents right so in order to index the documents we need to put them um ex this create index uh we need to put them in memory to load that and uh previously we used this command uh we don't need mean search we just need to download the documents and I'll execute this line here so we have the documents now let index them with elastic search and while it's indexing I will just execute the rest of things we need to adjust uh yeah this llm right so we I will take it from here here do we have prompt no I also deleted it so I will take these two things and put them here right so build prompt and llm um yeah so right now it it's still indexing and notice that uh here the progress bar is a little bit different so previously I I uh run things on COD spaces that's why I had a different one but here I run things locally and I had a i widget remember it complained about IPI widget so this is installed that's why the tqdm widgets look differently so um yeah I don't think we\",\n",
       "  'timecode_text': 'Modifying Module 1 Notebook',\n",
       "  'description': 'In this video, I demonstrate how to integrate Ollama and Elasticsearch in a docker-compose file. We replace OpenAI with Ollama, set up a docker-compose yaml file, pull the necessary model, and answer user questions.',\n",
       "  'link': 'https://www.youtube.com/watch?v=4juoo_jk96U&t=190s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.8 - Ollama + Elastic in Docker Compose intro IPython notebook jupyter notebook um and replace uh the call to open AI uh with a llama okay so uh now there's a lot of I think in this notebook there is a bunch of things we don't need um yeah we don't need all that we will need this and actually I need to uh we need to replace that right I think I'll just take it from here and we don't need that um when it comes to query yeah I think I will it's safe to just remove all these things and now we um yeah I don't have elastic search search have a fresh environment I should have installed it um so we will execute it now create an index like last time and um yeah index the documents create a query and then perform the search um while it's doing that I realized um that we still need documents right so in order to index the documents we need to put them um ex this create index uh we need to put them in memory to load that and uh previously we used this command uh we don't need mean search we just need to download the documents and I'll execute this line here so we have the documents now let index them with elastic search and while it's indexing I will just execute the rest of things we need to adjust uh yeah this llm right so we I will take it from here here do we have prompt no I also deleted it so I will take these two things and put them here right so build prompt and llm um yeah so right now it it's still indexing and notice that uh here the progress bar is a little bit different so previously I I uh run things on COD spaces that's why I had a different one but here I run things locally and I had a i widget remember it complained about IPI widget so this is installed that's why the tqdm widgets look differently so um yeah I don't think we In this video, I demonstrate how to integrate Ollama and Elasticsearch in a docker-compose file. We replace OpenAI with Ollama, set up a docker-compose yaml file, pull the necessary model, and answer user questions.\",\n",
       "  'id': '7391c7a6989743f081436e38f6b8774b'},\n",
       " {'vid_id': '4juoo_jk96U',\n",
       "  'title': 'LLM Zoomcamp 2.8 - Ollama + Elastic in Docker Compose',\n",
       "  'timecode': '6:21',\n",
       "  'text': \"need that just in case I'll cut it uh so I have it in my buffer um clipboard so now let's um execute this so this is for searching in elastic search building prompt and then our Rock flow and yeah so this is our query let's run it so right now I think what happened it is not found model3 is not found but like why let us see ah it's still it's still downloading yeah when we do it on COD basis it's uh faster I think I don't know why it's blinking like that it's a bit annoying maybe because it's doing it inside Docker I I cannot look at that it's just too annoying okay so let us try one more time and well I don't see any errors let's check if we have any locks here yeah it's probably doing something right now yeah put it on Boss not to wait uh fory long for you\",\n",
       "  'timecode_text': 'RAG flow functions',\n",
       "  'description': 'In this video, I demonstrate how to integrate Ollama and Elasticsearch in a docker-compose file. We replace OpenAI with Ollama, set up a docker-compose yaml file, pull the necessary model, and answer user questions.',\n",
       "  'link': 'https://www.youtube.com/watch?v=4juoo_jk96U&t=381s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.8 - Ollama + Elastic in Docker Compose need that just in case I'll cut it uh so I have it in my buffer um clipboard so now let's um execute this so this is for searching in elastic search building prompt and then our Rock flow and yeah so this is our query let's run it so right now I think what happened it is not found model3 is not found but like why let us see ah it's still it's still downloading yeah when we do it on COD basis it's uh faster I think I don't know why it's blinking like that it's a bit annoying maybe because it's doing it inside Docker I I cannot look at that it's just too annoying okay so let us try one more time and well I don't see any errors let's check if we have any locks here yeah it's probably doing something right now yeah put it on Boss not to wait uh fory long for you In this video, I demonstrate how to integrate Ollama and Elasticsearch in a docker-compose file. We replace OpenAI with Ollama, set up a docker-compose yaml file, pull the necessary model, and answer user questions.\",\n",
       "  'id': '3347792c71510a48f96281585a1180f7'},\n",
       " {'vid_id': '4juoo_jk96U',\n",
       "  'title': 'LLM Zoomcamp 2.8 - Ollama + Elastic in Docker Compose',\n",
       "  'timecode': '8:00',\n",
       "  'text': \"it took more than a minute but actually when I look now at response it's quite good so it did not attempt to answer anything like all the other questions and I think it has to do with the prompt cuz the prompt we used before is different from the prompt we use right now cuz this is the prompt from module one and it actually was able to figure out and and instead of giving like uh instead of answering all the questions in the context it just gave an answer to one question which is amazing and uh yeah so this is how we put everything together and um yeah so that's um that's it and the only thing that is left is just creating a small interface for that so instead of using a jupyter notebook we create a small interface for that and this is what we will do in the next lesson see you soon\",\n",
       "  'timecode_text': 'Response from Ollama',\n",
       "  'description': 'In this video, I demonstrate how to integrate Ollama and Elasticsearch in a docker-compose file. We replace OpenAI with Ollama, set up a docker-compose yaml file, pull the necessary model, and answer user questions.',\n",
       "  'link': 'https://www.youtube.com/watch?v=4juoo_jk96U&t=480s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.8 - Ollama + Elastic in Docker Compose it took more than a minute but actually when I look now at response it's quite good so it did not attempt to answer anything like all the other questions and I think it has to do with the prompt cuz the prompt we used before is different from the prompt we use right now cuz this is the prompt from module one and it actually was able to figure out and and instead of giving like uh instead of answering all the questions in the context it just gave an answer to one question which is amazing and uh yeah so this is how we put everything together and um yeah so that's um that's it and the only thing that is left is just creating a small interface for that so instead of using a jupyter notebook we create a small interface for that and this is what we will do in the next lesson see you soon In this video, I demonstrate how to integrate Ollama and Elasticsearch in a docker-compose file. We replace OpenAI with Ollama, set up a docker-compose yaml file, pull the necessary model, and answer user questions.\",\n",
       "  'id': 'aaf218ac6195791baa5ef90755e1524c'},\n",
       " {'vid_id': 'R6L8PZ-7bGo',\n",
       "  'title': 'LLM Zoomcamp 2.9 - Creating a Streamlit UI',\n",
       "  'timecode': '0:00',\n",
       "  'text': \"hi welcome back in this video we're going to put what we did in the last video in a nice UI so previously we uh put AMA and elastic search in Docker compost and run our simple um rack flow entirely locally so this was the this was The Notebook we used uh this one and that was the result and at the end of that video I thought it would be a nice idea to actually create a UI for that it was not in the plan originally but I thought like you probably want to interact with the thing somewhere outside of jupyter notebook so it's a good idea to cover it now and the funny thing is I have not prepared any materials for that so I'm going to improvise and again here we have our friend chat DPT so what I want to do is\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, I showcase how to transform our previous local RAG flow into a user-friendly Streamlit UI. I guide through the process of setting up Streamlit, creating an input box, and invoking functions.',\n",
       "  'link': 'https://www.youtube.com/watch?v=R6L8PZ-7bGo&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.9 - Creating a Streamlit UI hi welcome back in this video we're going to put what we did in the last video in a nice UI so previously we uh put AMA and elastic search in Docker compost and run our simple um rack flow entirely locally so this was the this was The Notebook we used uh this one and that was the result and at the end of that video I thought it would be a nice idea to actually create a UI for that it was not in the plan originally but I thought like you probably want to interact with the thing somewhere outside of jupyter notebook so it's a good idea to cover it now and the funny thing is I have not prepared any materials for that so I'm going to improvise and again here we have our friend chat DPT so what I want to do is In this video, I showcase how to transform our previous local RAG flow into a user-friendly Streamlit UI. I guide through the process of setting up Streamlit, creating an input box, and invoking functions.\",\n",
       "  'id': 'ba3bdedf3f3d8c6f27440395bd97ff23'},\n",
       " {'vid_id': 'R6L8PZ-7bGo',\n",
       "  'title': 'LLM Zoomcamp 2.9 - Creating a Streamlit UI',\n",
       "  'timecode': '0:58',\n",
       "  'text': \"ask it uh I want you to create a stream lead application with uh input box button ask uh button ask when I click ask I want to put uh so what I want to do is execute take the output I want to take take the out the input from the input box and invoke The Rock function um when it finishes executing I want to display display the output while the function is working I want to show a loading symbol let's see I made a few typos I hope it's uh not a disaster for chpd um well I also need to install stream lead because I don't think I have it so let me just do that while it's\",\n",
       "  'timecode_text': 'Creating Streamlit UI',\n",
       "  'description': 'In this video, I showcase how to transform our previous local RAG flow into a user-friendly Streamlit UI. I guide through the process of setting up Streamlit, creating an input box, and invoking functions.',\n",
       "  'link': 'https://www.youtube.com/watch?v=R6L8PZ-7bGo&t=58s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.9 - Creating a Streamlit UI ask it uh I want you to create a stream lead application with uh input box button ask uh button ask when I click ask I want to put uh so what I want to do is execute take the output I want to take take the out the input from the input box and invoke The Rock function um when it finishes executing I want to display display the output while the function is working I want to show a loading symbol let's see I made a few typos I hope it's uh not a disaster for chpd um well I also need to install stream lead because I don't think I have it so let me just do that while it's In this video, I showcase how to transform our previous local RAG flow into a user-friendly Streamlit UI. I guide through the process of setting up Streamlit, creating an input box, and invoking functions.\",\n",
       "  'id': '69be6239eadca039ff79ea4302ddd8e2'},\n",
       " {'vid_id': 'R6L8PZ-7bGo',\n",
       "  'title': 'LLM Zoomcamp 2.9 - Creating a Streamlit UI',\n",
       "  'timecode': '2:18',\n",
       "  'text': 'creating um the output so I will do peep install stream L stream L is a simp',\n",
       "  'timecode_text': 'Installing Streamlit',\n",
       "  'description': 'In this video, I showcase how to transform our previous local RAG flow into a user-friendly Streamlit UI. I guide through the process of setting up Streamlit, creating an input box, and invoking functions.',\n",
       "  'link': 'https://www.youtube.com/watch?v=R6L8PZ-7bGo&t=138s',\n",
       "  'text_vector': 'LLM Zoomcamp 2.9 - Creating a Streamlit UI creating um the output so I will do peep install stream L stream L is a simp In this video, I showcase how to transform our previous local RAG flow into a user-friendly Streamlit UI. I guide through the process of setting up Streamlit, creating an input box, and invoking functions.',\n",
       "  'id': 'c5954e7a50d4180c7581c37b8c55df95'},\n",
       " {'vid_id': 'R6L8PZ-7bGo',\n",
       "  'title': 'LLM Zoomcamp 2.9 - Creating a Streamlit UI',\n",
       "  'timecode': '2:29',\n",
       "  'text': \"application for creating UI and it doesn't seem like a lot of code so I already have this file QA FAQ and um yeah let's um let's run it how do we run it stream L run app okay okay so it is still executing that I'll uh I'll just navigate to that directory and run the streamly up there Zoom Camp did it install yeah let's see maybe it already yeah it needs a bit of time so I'll put it on pause and wait till it finishes installing it apparently it finished so let's run it of course it's QA fq and yes this is um how do I run a marathon okay\",\n",
       "  'timecode_text': 'Running the App',\n",
       "  'description': 'In this video, I showcase how to transform our previous local RAG flow into a user-friendly Streamlit UI. I guide through the process of setting up Streamlit, creating an input box, and invoking functions.',\n",
       "  'link': 'https://www.youtube.com/watch?v=R6L8PZ-7bGo&t=149s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.9 - Creating a Streamlit UI application for creating UI and it doesn't seem like a lot of code so I already have this file QA FAQ and um yeah let's um let's run it how do we run it stream L run app okay okay so it is still executing that I'll uh I'll just navigate to that directory and run the streamly up there Zoom Camp did it install yeah let's see maybe it already yeah it needs a bit of time so I'll put it on pause and wait till it finishes installing it apparently it finished so let's run it of course it's QA fq and yes this is um how do I run a marathon okay In this video, I showcase how to transform our previous local RAG flow into a user-friendly Streamlit UI. I guide through the process of setting up Streamlit, creating an input box, and invoking functions.\",\n",
       "  'id': '5d32728f3a90927344d4734d98dc7cc2'},\n",
       " {'vid_id': 'R6L8PZ-7bGo',\n",
       "  'title': 'LLM Zoomcamp 2.9 - Creating a Streamlit UI',\n",
       "  'timecode': '4:00',\n",
       "  'text': \"so right now let's take all the code we have here and put it to our stream L application so we will start with uh I need this notbook so I will start with U this command then uh these two connect to elastic search we don't need to create an index we already created it we don't need to index documents we already index them we just need to get oh well index name we need to specify it is uh of course questions I'll put it as a uh parameter here then so have that index built prompt and rock then I remove it and I don't know if I need to rerun it I'll I'll I'll stop it start again maybe stream lead is actually smart enough to recognize changes in the\",\n",
       "  'timecode_text': 'Moving the Code from the Notebook',\n",
       "  'description': 'In this video, I showcase how to transform our previous local RAG flow into a user-friendly Streamlit UI. I guide through the process of setting up Streamlit, creating an input box, and invoking functions.',\n",
       "  'link': 'https://www.youtube.com/watch?v=R6L8PZ-7bGo&t=240s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.9 - Creating a Streamlit UI so right now let's take all the code we have here and put it to our stream L application so we will start with uh I need this notbook so I will start with U this command then uh these two connect to elastic search we don't need to create an index we already created it we don't need to index documents we already index them we just need to get oh well index name we need to specify it is uh of course questions I'll put it as a uh parameter here then so have that index built prompt and rock then I remove it and I don't know if I need to rerun it I'll I'll I'll stop it start again maybe stream lead is actually smart enough to recognize changes in the In this video, I showcase how to transform our previous local RAG flow into a user-friendly Streamlit UI. I guide through the process of setting up Streamlit, creating an input box, and invoking functions.\",\n",
       "  'id': '90cb2a3fa8749e05880b93d0a204d439'},\n",
       " {'vid_id': 'R6L8PZ-7bGo',\n",
       "  'title': 'LLM Zoomcamp 2.9 - Creating a Streamlit UI',\n",
       "  'timecode': '5:25',\n",
       "  'text': \"code and automatically reload for them I don't know but let's see so instead of asking how do I run a marathon let us actually ask this question and it will take some time to let me put it on pause and I'll be back when it finishes well and it finished it took again approximately the same amount of time as last time more than a minute because of course I'm running on a GPU on a CPU not on a GPU that's why it takes a bit of time um but yeah actually it gave like actually I think it gave more answers than I asked but yeah so this is how it works this is how you can create a simple UI for your ra\",\n",
       "  'timecode_text': 'Testing the RAG Flow in Streamlit',\n",
       "  'description': 'In this video, I showcase how to transform our previous local RAG flow into a user-friendly Streamlit UI. I guide through the process of setting up Streamlit, creating an input box, and invoking functions.',\n",
       "  'link': 'https://www.youtube.com/watch?v=R6L8PZ-7bGo&t=325s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.9 - Creating a Streamlit UI code and automatically reload for them I don't know but let's see so instead of asking how do I run a marathon let us actually ask this question and it will take some time to let me put it on pause and I'll be back when it finishes well and it finished it took again approximately the same amount of time as last time more than a minute because of course I'm running on a GPU on a CPU not on a GPU that's why it takes a bit of time um but yeah actually it gave like actually I think it gave more answers than I asked but yeah so this is how it works this is how you can create a simple UI for your ra In this video, I showcase how to transform our previous local RAG flow into a user-friendly Streamlit UI. I guide through the process of setting up Streamlit, creating an input box, and invoking functions.\",\n",
       "  'id': '999e715003a91eec0f3896a95261ec34'},\n",
       " {'vid_id': 'R6L8PZ-7bGo',\n",
       "  'title': 'LLM Zoomcamp 2.9 - Creating a Streamlit UI',\n",
       "  'timecode': '6:30',\n",
       "  'text': \"application you should not be limited you can use whatever you want instead of stream lead you can use uh for example you can create a flk application you can create um I don't know there are many many many options that you can use could be a telegram app could be a slug bot uh telegram bot um I don't know like there are many many things you can do stream lead is the simplest one but in your project you can explore any anything you want and to create a user interface well that's all for this modu I hope you enjoyed it and see you soon\",\n",
       "  'timecode_text': 'Other UI Options',\n",
       "  'description': 'In this video, I showcase how to transform our previous local RAG flow into a user-friendly Streamlit UI. I guide through the process of setting up Streamlit, creating an input box, and invoking functions.',\n",
       "  'link': 'https://www.youtube.com/watch?v=R6L8PZ-7bGo&t=390s',\n",
       "  'text_vector': \"LLM Zoomcamp 2.9 - Creating a Streamlit UI application you should not be limited you can use whatever you want instead of stream lead you can use uh for example you can create a flk application you can create um I don't know there are many many many options that you can use could be a telegram app could be a slug bot uh telegram bot um I don't know like there are many many things you can do stream lead is the simplest one but in your project you can explore any anything you want and to create a user interface well that's all for this modu I hope you enjoyed it and see you soon In this video, I showcase how to transform our previous local RAG flow into a user-friendly Streamlit UI. I guide through the process of setting up Streamlit, creating an input box, and invoking functions.\",\n",
       "  'id': '4f55b8dd184eddc4ed2ef61ddd6ec620'},\n",
       " {'vid_id': 'C5AWdL3kg1Q',\n",
       "  'title': 'LLM Zoomcamp 3.1 - Introduction to Vector Search',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hello everyone welcome to the session in this session let's get into the basics of vector database how they work the intuition behind them before we get to the intuition behind the vector database let's look at the pop let's look at a popular use case semantic search and let's see how someone like Google uses this effectively so if you go to Google and let's say you search how many employees does Apple have the moment you search for this Google understands that you're talking about the company Apple so it ranks its results based on that information that it found about your query similarly if you search for a another term let's say where can I pick apples near me in this context Google understood that the apples that you're referring to in this case are fruits and because of fruits it understood that you need to you need a location where you can go and pick up pick up the fruits how does Google does this it does it by using the semantic search because it's just not purely going by the search to term it's also trying to pick up the context in other words the meaning behind the search and it's able to provide you the results accordingly so to some in summary in the second search understood that the Apple that we referring to here is a fruit and the Apple that we referring to the in first search was the comp perfect so why is it getting popular now like what what is the need for like to now we know that due to the advancement in Internet there is an explosion of data and the explosion of data is in like very Diversified formats it's a text format it's an image format it's an audio format you could you there's multiple different types of formats the data is available today so what you need is a mechanism that can look into this data and it can get meaning out of this data classical databases where good at storing the structured data but they're not good in handling information like this so This presents a really good use case for Vector databases the second reason is large language models lack long-term memory so they're very good for shortterm conversations however anytime you want to store and retrieve uh some of the a long-term um information Vector database is going to be the key because Vector database helps you to store all the uh relevant information in one base and then that information can be available to the large language models during the query time or the inference time so the large language models can then use that information provide you better results so before we talk about vector database let talk about Vector embedding so what is a embedding you will hear this terminology pretty often in the large language model space but again anytime when somebody talks about embedding the only thing you need to remember is it's bunch of numbers it's a bunch of numbers but it means something so that bunch of numbers somehow captures the associations of your text and it stores that information within that bunch of numbers that's all you need to know and we will see how those numbers are are the the vectors are created in the next slides one other thing I want you to remember is in the vector space what happens is the data points that are closer in meaning are placed closer to each other whereas the vector points that are not so similar in meanings are placed far up so that way anytime you're trying to search for certain information a vector database is able to produce results that is closer to each other there are there are different types of vector embeddings a word can be a converted into an embedding a sentence could be converted into an embedding and then think about large documents um like a legal document marketing documents or legal contracts these are like multi-page documents and all of that information can be converted into into a into an embedding as well image can also be converted into embeddings as well so there are variety of techniques that is um that is provided here and um I will leave some documentation in the reference section for you to go and look into it how are vector embeddings created in the first place so Vector embeddings are created through a series of machion learning steps that includes um training the model as you can see here in the visual so the first step is to gather the data set and then you pre-process the data and once you pre-process the data what you do is you will train a machine learning model and the Machine learning model what it will do is it will try to pick up the signals the patterns um or the other representations in the data and then after it understood the representation it will generate embeddings so once those embeddings are generated then you can verify the model quality of what those how those embeddings are uh whether they are providing you good results if the model is not providing you good results then you go back and redo the same step so it's an iterative process you continue doing this over and over and over again until you are confident that the the model that you built is good enough for you to provide proper embeddings then you select the final model so that's the process series of steps that uh that that has been done you can go to hugging face today and just download a pre-train model from there and then you can use that for um creating Vector embeddings for your projects in fact that's what we will do in our lab in the next session so I wanted you to just understand how this whole pre-train models are created so that when we go in the next section and and use a pre-train model you will understand uh how those Vector embeddings are generated perfect now that we we have a good understanding of what an Vector embedding is now think about the scale or the amount of embeddings that you're going to generate in a project it could be millions and millions of data points so what you need is you need the ability to store that Vector information in a much more more optimized way and not only storage but you need to also think about how you can retrieve that information in an optimized way as well so that is where Vector database shines because Vector database indexes the data in a efficient way indexing is an extremely important Concept in Vector database um there are there are multiple ways you can index and Vector databases in the market today use different types of indexing um you can you can read through it I will give you I have provided a link in the reference section you can walk through it uh what are the different types of indexing that's available this is a good visual that was provided by the elastic search uh portal you could see here from left to right what happens is your data type could be an image document audio doesn't matter then what you do is you take this information and then you pass this through a pre-trained model that uh we spoke about in the previous slide and that pre-trained model will provide a vector embedding and now think about the scale or the amount of vector embeddings that you're going to generate because there are like let's assume you have millions of documents so now you need to P you need to Index this information and store it efficiently in a vector database so for that the vector database will store the information in optimized way and now let's say the end there's there's a query that's coming from an end user who wants to search for a for for a term and that term would be converted Again by the by the tree train model as a as a vector and that Vector would be sent into the vector database and the vector database will try and find a similar matching response to the received Vector data and it will provide the results that end results would be semantically closer to meaning because your vector database what it does is it it searches based on the um similari similarity scores so there are multiple similarity scores we will again see that in the uh in the indexing section in the lab so there is cosine similarity there is a DOT product there is ukan distance um so you can you can configure that when you're creating a database but the takeaway here is what you need to remember is a vector data datase offers a way to store vast amount of data in optimized way and it it also provides the ability to search our ability to retrieve the information faster that in terms of applications what it's going to give you is provide the ability to remember your past data and that information can be passed back to the large language models U during the inference time so it helps the mission learning models remember the past data and it also very helpful for semantic search personalized recommendation and text generation as well so in the next session I will walk you through a lab and we will set up a semantic search engine using elastic search end to end and you will see all of the concepts that we discussed here will be put into action so at that point you will understand the end to endend uh workflow of a vector database I hope you found this session helpful thank you so much for joining happy learning\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=C5AWdL3kg1Q',\n",
       "  'text_vector': \"LLM Zoomcamp 3.1 - Introduction to Vector Search hello everyone welcome to the session in this session let's get into the basics of vector database how they work the intuition behind them before we get to the intuition behind the vector database let's look at the pop let's look at a popular use case semantic search and let's see how someone like Google uses this effectively so if you go to Google and let's say you search how many employees does Apple have the moment you search for this Google understands that you're talking about the company Apple so it ranks its results based on that information that it found about your query similarly if you search for a another term let's say where can I pick apples near me in this context Google understood that the apples that you're referring to in this case are fruits and because of fruits it understood that you need to you need a location where you can go and pick up pick up the fruits how does Google does this it does it by using the semantic search because it's just not purely going by the search to term it's also trying to pick up the context in other words the meaning behind the search and it's able to provide you the results accordingly so to some in summary in the second search understood that the Apple that we referring to here is a fruit and the Apple that we referring to the in first search was the comp perfect so why is it getting popular now like what what is the need for like to now we know that due to the advancement in Internet there is an explosion of data and the explosion of data is in like very Diversified formats it's a text format it's an image format it's an audio format you could you there's multiple different types of formats the data is available today so what you need is a mechanism that can look into this data and it can get meaning out of this data classical databases where good at storing the structured data but they're not good in handling information like this so This presents a really good use case for Vector databases the second reason is large language models lack long-term memory so they're very good for shortterm conversations however anytime you want to store and retrieve uh some of the a long-term um information Vector database is going to be the key because Vector database helps you to store all the uh relevant information in one base and then that information can be available to the large language models during the query time or the inference time so the large language models can then use that information provide you better results so before we talk about vector database let talk about Vector embedding so what is a embedding you will hear this terminology pretty often in the large language model space but again anytime when somebody talks about embedding the only thing you need to remember is it's bunch of numbers it's a bunch of numbers but it means something so that bunch of numbers somehow captures the associations of your text and it stores that information within that bunch of numbers that's all you need to know and we will see how those numbers are are the the vectors are created in the next slides one other thing I want you to remember is in the vector space what happens is the data points that are closer in meaning are placed closer to each other whereas the vector points that are not so similar in meanings are placed far up so that way anytime you're trying to search for certain information a vector database is able to produce results that is closer to each other there are there are different types of vector embeddings a word can be a converted into an embedding a sentence could be converted into an embedding and then think about large documents um like a legal document marketing documents or legal contracts these are like multi-page documents and all of that information can be converted into into a into an embedding as well image can also be converted into embeddings as well so there are variety of techniques that is um that is provided here and um I will leave some documentation in the reference section for you to go and look into it how are vector embeddings created in the first place so Vector embeddings are created through a series of machion learning steps that includes um training the model as you can see here in the visual so the first step is to gather the data set and then you pre-process the data and once you pre-process the data what you do is you will train a machine learning model and the Machine learning model what it will do is it will try to pick up the signals the patterns um or the other representations in the data and then after it understood the representation it will generate embeddings so once those embeddings are generated then you can verify the model quality of what those how those embeddings are uh whether they are providing you good results if the model is not providing you good results then you go back and redo the same step so it's an iterative process you continue doing this over and over and over again until you are confident that the the model that you built is good enough for you to provide proper embeddings then you select the final model so that's the process series of steps that uh that that has been done you can go to hugging face today and just download a pre-train model from there and then you can use that for um creating Vector embeddings for your projects in fact that's what we will do in our lab in the next session so I wanted you to just understand how this whole pre-train models are created so that when we go in the next section and and use a pre-train model you will understand uh how those Vector embeddings are generated perfect now that we we have a good understanding of what an Vector embedding is now think about the scale or the amount of embeddings that you're going to generate in a project it could be millions and millions of data points so what you need is you need the ability to store that Vector information in a much more more optimized way and not only storage but you need to also think about how you can retrieve that information in an optimized way as well so that is where Vector database shines because Vector database indexes the data in a efficient way indexing is an extremely important Concept in Vector database um there are there are multiple ways you can index and Vector databases in the market today use different types of indexing um you can you can read through it I will give you I have provided a link in the reference section you can walk through it uh what are the different types of indexing that's available this is a good visual that was provided by the elastic search uh portal you could see here from left to right what happens is your data type could be an image document audio doesn't matter then what you do is you take this information and then you pass this through a pre-trained model that uh we spoke about in the previous slide and that pre-trained model will provide a vector embedding and now think about the scale or the amount of vector embeddings that you're going to generate because there are like let's assume you have millions of documents so now you need to P you need to Index this information and store it efficiently in a vector database so for that the vector database will store the information in optimized way and now let's say the end there's there's a query that's coming from an end user who wants to search for a for for a term and that term would be converted Again by the by the tree train model as a as a vector and that Vector would be sent into the vector database and the vector database will try and find a similar matching response to the received Vector data and it will provide the results that end results would be semantically closer to meaning because your vector database what it does is it it searches based on the um similari similarity scores so there are multiple similarity scores we will again see that in the uh in the indexing section in the lab so there is cosine similarity there is a DOT product there is ukan distance um so you can you can configure that when you're creating a database but the takeaway here is what you need to remember is a vector data datase offers a way to store vast amount of data in optimized way and it it also provides the ability to search our ability to retrieve the information faster that in terms of applications what it's going to give you is provide the ability to remember your past data and that information can be passed back to the large language models U during the inference time so it helps the mission learning models remember the past data and it also very helpful for semantic search personalized recommendation and text generation as well so in the next session I will walk you through a lab and we will set up a semantic search engine using elastic search end to end and you will see all of the concepts that we discussed here will be put into action so at that point you will understand the end to endend uh workflow of a vector database I hope you found this session helpful thank you so much for joining happy learning Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '3cba0dbf73297ec686a5b91511d47dbe'},\n",
       " {'vid_id': 'ptByfB_YcEg',\n",
       "  'title': 'LLM Zoomcamp 3.2 - Semantic Search with Elasticsearc',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"welcome back to the course in this section what we will do is we will build a semantic search engine using elastic search in order to do that we would use the same python environment that you have set up in module one so I would like to use the same environment and also let's use the same Docker container that was used in the previous modules as well so I'm going to go and check if my Docker is currently running so Docker PS I don't have any Docker running right now so what I'm going to do is I'm going to go to our Zoom Camp module one I'm just going to share the screen here you could see it on your screen from unit one me scroll up you could see from unit unit one introduction section there was a Docker command that was provided right here I'm going to copy that command and put it back here and enter let's give it a few seconds and you should see the docker container up and running and what this will help us to do is to connect to the elastic search locally you can see on the screen now the docker container is up and running so when we initiate a connection in the notebook shortly we should be able to you know get the client connected all right so let's go back open up vs code in your local and navigate to the same virtual environment or navigate to the same environment where you used uh which you used for module one so I'm using the same environment here the only difference is I installed one additional package which I will talk about other than that everything else should be the same so what you see here is the overall architecture to build our semantic search in for this module what we're going to do is we're going to use the same documents. Json file which you should be familiar with which was used in the other modules as well we will use the same data set for this class as well so what you need to do is take the documents. Json file create what we call as do documents which you will see shortly what do we mean by that so you need to create something called documents don't confuse with this documents with the documents. Json though the names are same but this terminology is used in elastic search so we need to create from the data set we need to create what we call as documents so once the documents are created then what we will do is we will get a pre-trained model from hugging phas and then we will create create the embeddings for the documents that we creating and once we get the embeddings then we will push that embeddings into an index in the elastic search and once the embeddings are indexed into the elastic search then anytime a new anytime a user enters a new search query then what would that do is that will go into the embeddings again that's query will be converted as embeddings as well and then that would go and query the the elastic search Vector database and the results the semantic results will be displayed so that's the overall architecture so let's go ahead and get started a few Concepts to understand there are two important Concepts in elastic search Vector database one is called documents and other one is index so as you can see here a document is a collection of fields with your Associated values so you will see what do we mean by that so what do we mean by the associated values and then what you will have to do is you need to take those documents right and you need to add all your documents to an index so that's what this architecture exactly talks about you need to take those documents and add it in index however there's one step in between where we'll create embeddings but the idea is you take the document and push it into index and what is an index index is a collection of documents that is stored in a highly optimized format designed to perform efficient searches right think about it why the index is going to be the key index is like exactly like a table of contents in a book so anytime you read a book you go through the table of contents and that will help you to do is it will help you to go to the to the right section pretty fast so you don't have to go through the entire book you just need to go through the table of contents to go to the right portion of right portion of the book so that's exactly how indexing works here so we will see that in action I will I will show you how that that's going to work so let's start preparing the data set first so in order for us to get started what we're going to do is we what we doing is we're opening up document. Json file and reading the file so that after that what you're going to do is if you look at the documents uncore ra what you will notice is you can see here the course and the documents there are two levels of hierarchy right now however what elastic search wants is to have a everything in like a same same level same hierarchy so instead of having course on the top and then documents on the bottom if you want everything to be in like in the same level so in order to do that we have to write a simple for loop again and loop through the docs uncore raw and then basically reassign it so let's look at the values after this step is done all right perfect so now you can see the course and the the questions and all answers in that section everything is in the same level so this is good so this is exactly what we wanted all right so let's move on to step two so in step two what we going to do is you're going to initiate a package called sentence Transformers we're going to import this package recall I have already installed this package but I'm leaving the the package uh information here so just make sure to run this command and uh once you run run this command and then try to import this package I've seen some some issues not in this environment but based on my experience I've noticed some issues happening especially because of the latest numpy edition if that happens just follow these steps if you're not able to import after you install the sentence Transformer all you need to do is UN install napai and then follow these steps you should be able to import your sentence Transformer so what is a sentence Transformer why is it required so let's go back to the documentation so I'm going to bring the sentence Transformer documentation here and then let's go to and you could see here the sentence Transformer sentence transform was created by ukp lab and is currently being maintained by hugging phase so what this will help you is this will help you with a very simple um call from the sentence Transformer package you can call the pre-trained models in just one line of code and you can call the required pre-trained models and then you can also perform inferencing in in another one line of code so it makes the whole process very simple so you you can see what are the pre-trained models that's available within this particular um library and you have to just scroll down here you could see that um there are there are variety of pre-trained models that's available again what is this pre-trained model these are the models that has already been trained on a certain data set and it's it's available for you to start you know creating embeddings so anytime you pass a data into this particular model what you're going to get out is a is a dense numeric Vector so with that Vector we're going to pass that Vector into our Vector database so that's what we going to do so you could see you can see all the performance benchmarks here you can click on any of these um models and you get to see all the required information here so let's go now let's we're going to use all mpet based V2 in our case so let's use the same um let's just copy the code from here and let's go to the cell here then just place it here and then it's executed so this step is now complete now let's go and try to perform a very simple inference and you could see this is a simple sentence and what you would get back is a dense Vector you see here you what you get back is a vector that's it perfect so the idea is anytime you pass a certain text any to this model you're going to get back a vector and you could see the length of the vector this length is important because different models have different lengths all right so now that is done what we will do is we will set up the now before doing that let's go ahead and create the embeddings remember the whole idea why we downloaded this model is because we wanted to create embeddings for our data set all right so before that let's go ahead and take a look at the data set once so in this data set you could see there are four different fields the first field the first and the Third Field are the actual data the section and the course field are like categorical data so there is not a lot of information um or variation that's going to exist in the data set so it makes sense to either create embeddings on the Tex field or on the question field so for our exercise purposes I I chose to do embeddings on the next field in order for you to do that what you need to do is you can um create a simple for Loop and you just Loop through the documents and for every document what you need to do is you need to um put the you need to call the model ncode function and then store the um Vector back into a another another field so we'll see that it's a very simple uh for Loop I'm just going to put it out here so go right here so as I said you just need to Loop through the documents and after you're looping through what you're doing is you are calling the model. encode on the dock of text field and then you assigning it back into the same um list um with a different name called textor Vector so that's pretty much it so let's just start doing this might take a few minutes so let's resume after this um step is complete all right now the step is complete and the embeddings are created so let's go back and verify I think it's in the operations right it's going to operations looks like yes so we have created the embeddings nothing but a dense vector and added this back to this documents perfect looks very good so now what we will do is let's initiate the elastic search connection remember we have already ran the doc container so locally so we should not have any issues perfect so I could see that the connection is established here then what we're going to do is let's as a next step we need to create what we call as mappings so now that the elastic search CLI is created the next step is we need to create an index so in order for us to create an index you need to First create a mapping so think about this from a from a database standpoint whenever you're creating a schema so what you need to provide is for the schema you need to provide the column names the table name and then what the data types are right so you need to provide that metadata in order for you to create a schema very similarly in this case you need to provide what we call as mapping and that mapping will contain all the required metadata information about your uh about the document data so that's exactly what it's mentioned here you can read through it but you could the first line just calls out the exact statement I just said is the process of defining how a document and the fields it contains a stored and index that's it so you should be very familiar with the mapping already because um it's the same almost the same information that I'm copying and pasting it from module one and you could see that uh for the mappings here in the property section you could see there are different fields these fields should be the same fields that we created here fix section question right so these are all the same fields and you could see the data types associated with it the only difference is what we have done today is we have created something called a text Vector which is nothing but a dense vector and uh we need to also specify the dimensions here so remember the dimensions we got it from our inference initially just go back a step here just want to show you real quick so that's the dimensions it is also mentioned on the model uh sentence Transformer documentation for every models as well and then you need to also specify the similarity metric in which in which your the text Vector should be stored um in the in the vector database so there are several metrics that's available so it could be cosine Matrix it could be n to Norm dot product ukian distance um I'm I'm leaving I will leave you some reference uh in the in the references section so you can read through it but by far the most popular type is the is the cosine or a DOT prodct all right so that's pretty much it so let's just run this so now let's go ahead and uh create the index so you could see here the bottom line here so this is how you create the index so you need to specify IND isis. create provide the index name that you have defined here and also pass all the required metadata that we have created here and what I'm doing here on the top is I'm creating something called a delete indexes statement that's because what I've noticed is when you're working through this data set or when you're working on this project you might realize that you you updated the data set or you wanted to change the data type in scenarios like those you go and update the mapping but when you try to create the index with the same name you will get an error so in order for you to avoid that what you can do is as a best practice every time you can just delete the index uh if it exists and if it doesn't exist you can ignore this delete statement so that's all this this line is doing here so now let's go and create the index fantastic that step is done so now the index is created we already have the embeddings created in the document so now what you need to do is you need to populate those embeddings or the whole document into the index so that's what we're going to do here for that we need to just Loop through the operations data set that we have prepared and um what we need to do is for every document you need to push that document into the indexes into the index that's that's what the statement is is doing it might take about a minute or two so let's resume after this statement is complete or this execution is complete this step is now complete so let's go back and take a look at the architecture to see where we are so we have pretty much completed the first portion of our architecture so we took the documents. Json file we created documents out of it and then we took a pre-trained model and in that pre-trained model we passed the documents to create embeddings and once that embeddings are created then we pushed it into the elastic searches index and so with all of the the blue all of the black lines are complete now so now let's go and build the new search query so that's the search term that a end user um wants to query the data on and what we will do is we will pass that to the embeddings as well and then look at the results all right so for that let's go to this section let's create a search term here so the search term the end user wants to search for is Windows or Mac so the user wants to know for the course should they bring a Windows laptop or a Mac laptop or a Windows Os or a Mac OS and remember every term um that the search term that the user is is searching has to be converted it into in into a vector as well so that's what this line is doing and once that step is done let's go and run this now we have to form this query so you could see here so we're building a query and in the query what we what we specifying is in the database which field should the should the database go and search for and what is the the vector that the user is is searching so the user is searching Windows or Mac so and that is converted into a vector and that is that is what this query _ Vector is taking as input and then K5 meaning you want to provide five nearest uh neighbors or five nearest documents uh that matches to this particular search term and number of candidates it's nothing but if you can think about this like a group of documents that um the search is going to look into and you can play around with this number um like 1,000 2,000 and then you can play which whatever works for you and that's pretty much it so now what we are going to do is we are going to pass this to elastic searches search functionality a search method so you could see here there is a search method and it takes the index name and then it takes the query that we have built here in this previous step and then in the results you're specifying what are the fields that you want in the results pack so what I'm going to do is I'm going to just uh in the response that we're going to get I know that the response is uh is mentioned in this particular um field so let's go and run this Fant fantastic so let's open up the results in the text editor so the first thing what I wanted to show you the mention is you could see you remember we mentioned ks5 so you should see one two 3 four five so Five results so you got back Five results and then you could see for every result there is a score so the score the the results are ranked um by the highest score on the top and the and then and then the lowest score on the bottom and the score of anytime a score is close to one it indicates that it's a very good match and anytime a score is close to zero it denotes it's a very bad match so we got a score of 71 which is a pretty decent score so let's go and look at the look at the text on what what did what what did we see here so remember what the inducer search for the inducer was searching for yeah Windows or Mac right and the first result you got back is yes Linux is ideal but technically it should not matter so now think about it right nowhere in the query the end user mentioned whether this is an operating system or a laptop they just mentioned Windows or Mac however this semantic search was smart enough to understand the semantics meaning the the the underlying meanings associated with the end user search so he understood that it's talking about the operating system so it went and searched for the the closest match that that um matches with the operating system and it gave you this this information so you can you can now see the power that uh that that that you can uncover in scenarios like this because this is exactly how your Google search works so recall the in the first introduction section in in this module we talked about how Apple how the Google is ranking based on whether if apple is a flute your search results will vary and if apple is a company your search results will vary so that's exactly how semantic search works behind the scenes so congratulations you built a semantic search in this section um I also wanted to call out that uh this is just a very simple semantic search we built there are much much more complex um semantic searches you can build and I also want to show you one L little bit of um combination of um couple of um queries here so what you can also do is anytime when the IND user is searching you can use um your regular um query search on top of it you can also make sure that the results can be should be filtered from only one course remember there are this data set has information from three courses but what if you want to only look at one course because let's assume that this user is asking this question in the data engineering Zoom Camp course so you want the results only from this course so you can basically chain your Search terms accordingly and then you can filter them you can you can query them um based on um different combinations that is very powerful for for semantic search let's run this results let's look at the result set perfect so you could see now that the result is only coming from the data engineering Zoom Camp so that's good congratulations you built a semantic search engine using elastic search Vector database I hope you enjoyed the session thank you so much for joining happy learning\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=ptByfB_YcEg',\n",
       "  'text_vector': \"LLM Zoomcamp 3.2 - Semantic Search with Elasticsearc welcome back to the course in this section what we will do is we will build a semantic search engine using elastic search in order to do that we would use the same python environment that you have set up in module one so I would like to use the same environment and also let's use the same Docker container that was used in the previous modules as well so I'm going to go and check if my Docker is currently running so Docker PS I don't have any Docker running right now so what I'm going to do is I'm going to go to our Zoom Camp module one I'm just going to share the screen here you could see it on your screen from unit one me scroll up you could see from unit unit one introduction section there was a Docker command that was provided right here I'm going to copy that command and put it back here and enter let's give it a few seconds and you should see the docker container up and running and what this will help us to do is to connect to the elastic search locally you can see on the screen now the docker container is up and running so when we initiate a connection in the notebook shortly we should be able to you know get the client connected all right so let's go back open up vs code in your local and navigate to the same virtual environment or navigate to the same environment where you used uh which you used for module one so I'm using the same environment here the only difference is I installed one additional package which I will talk about other than that everything else should be the same so what you see here is the overall architecture to build our semantic search in for this module what we're going to do is we're going to use the same documents. Json file which you should be familiar with which was used in the other modules as well we will use the same data set for this class as well so what you need to do is take the documents. Json file create what we call as do documents which you will see shortly what do we mean by that so you need to create something called documents don't confuse with this documents with the documents. Json though the names are same but this terminology is used in elastic search so we need to create from the data set we need to create what we call as documents so once the documents are created then what we will do is we will get a pre-trained model from hugging phas and then we will create create the embeddings for the documents that we creating and once we get the embeddings then we will push that embeddings into an index in the elastic search and once the embeddings are indexed into the elastic search then anytime a new anytime a user enters a new search query then what would that do is that will go into the embeddings again that's query will be converted as embeddings as well and then that would go and query the the elastic search Vector database and the results the semantic results will be displayed so that's the overall architecture so let's go ahead and get started a few Concepts to understand there are two important Concepts in elastic search Vector database one is called documents and other one is index so as you can see here a document is a collection of fields with your Associated values so you will see what do we mean by that so what do we mean by the associated values and then what you will have to do is you need to take those documents right and you need to add all your documents to an index so that's what this architecture exactly talks about you need to take those documents and add it in index however there's one step in between where we'll create embeddings but the idea is you take the document and push it into index and what is an index index is a collection of documents that is stored in a highly optimized format designed to perform efficient searches right think about it why the index is going to be the key index is like exactly like a table of contents in a book so anytime you read a book you go through the table of contents and that will help you to do is it will help you to go to the to the right section pretty fast so you don't have to go through the entire book you just need to go through the table of contents to go to the right portion of right portion of the book so that's exactly how indexing works here so we will see that in action I will I will show you how that that's going to work so let's start preparing the data set first so in order for us to get started what we're going to do is we what we doing is we're opening up document. Json file and reading the file so that after that what you're going to do is if you look at the documents uncore ra what you will notice is you can see here the course and the documents there are two levels of hierarchy right now however what elastic search wants is to have a everything in like a same same level same hierarchy so instead of having course on the top and then documents on the bottom if you want everything to be in like in the same level so in order to do that we have to write a simple for loop again and loop through the docs uncore raw and then basically reassign it so let's look at the values after this step is done all right perfect so now you can see the course and the the questions and all answers in that section everything is in the same level so this is good so this is exactly what we wanted all right so let's move on to step two so in step two what we going to do is you're going to initiate a package called sentence Transformers we're going to import this package recall I have already installed this package but I'm leaving the the package uh information here so just make sure to run this command and uh once you run run this command and then try to import this package I've seen some some issues not in this environment but based on my experience I've noticed some issues happening especially because of the latest numpy edition if that happens just follow these steps if you're not able to import after you install the sentence Transformer all you need to do is UN install napai and then follow these steps you should be able to import your sentence Transformer so what is a sentence Transformer why is it required so let's go back to the documentation so I'm going to bring the sentence Transformer documentation here and then let's go to and you could see here the sentence Transformer sentence transform was created by ukp lab and is currently being maintained by hugging phase so what this will help you is this will help you with a very simple um call from the sentence Transformer package you can call the pre-trained models in just one line of code and you can call the required pre-trained models and then you can also perform inferencing in in another one line of code so it makes the whole process very simple so you you can see what are the pre-trained models that's available within this particular um library and you have to just scroll down here you could see that um there are there are variety of pre-trained models that's available again what is this pre-trained model these are the models that has already been trained on a certain data set and it's it's available for you to start you know creating embeddings so anytime you pass a data into this particular model what you're going to get out is a is a dense numeric Vector so with that Vector we're going to pass that Vector into our Vector database so that's what we going to do so you could see you can see all the performance benchmarks here you can click on any of these um models and you get to see all the required information here so let's go now let's we're going to use all mpet based V2 in our case so let's use the same um let's just copy the code from here and let's go to the cell here then just place it here and then it's executed so this step is now complete now let's go and try to perform a very simple inference and you could see this is a simple sentence and what you would get back is a dense Vector you see here you what you get back is a vector that's it perfect so the idea is anytime you pass a certain text any to this model you're going to get back a vector and you could see the length of the vector this length is important because different models have different lengths all right so now that is done what we will do is we will set up the now before doing that let's go ahead and create the embeddings remember the whole idea why we downloaded this model is because we wanted to create embeddings for our data set all right so before that let's go ahead and take a look at the data set once so in this data set you could see there are four different fields the first field the first and the Third Field are the actual data the section and the course field are like categorical data so there is not a lot of information um or variation that's going to exist in the data set so it makes sense to either create embeddings on the Tex field or on the question field so for our exercise purposes I I chose to do embeddings on the next field in order for you to do that what you need to do is you can um create a simple for Loop and you just Loop through the documents and for every document what you need to do is you need to um put the you need to call the model ncode function and then store the um Vector back into a another another field so we'll see that it's a very simple uh for Loop I'm just going to put it out here so go right here so as I said you just need to Loop through the documents and after you're looping through what you're doing is you are calling the model. encode on the dock of text field and then you assigning it back into the same um list um with a different name called textor Vector so that's pretty much it so let's just start doing this might take a few minutes so let's resume after this um step is complete all right now the step is complete and the embeddings are created so let's go back and verify I think it's in the operations right it's going to operations looks like yes so we have created the embeddings nothing but a dense vector and added this back to this documents perfect looks very good so now what we will do is let's initiate the elastic search connection remember we have already ran the doc container so locally so we should not have any issues perfect so I could see that the connection is established here then what we're going to do is let's as a next step we need to create what we call as mappings so now that the elastic search CLI is created the next step is we need to create an index so in order for us to create an index you need to First create a mapping so think about this from a from a database standpoint whenever you're creating a schema so what you need to provide is for the schema you need to provide the column names the table name and then what the data types are right so you need to provide that metadata in order for you to create a schema very similarly in this case you need to provide what we call as mapping and that mapping will contain all the required metadata information about your uh about the document data so that's exactly what it's mentioned here you can read through it but you could the first line just calls out the exact statement I just said is the process of defining how a document and the fields it contains a stored and index that's it so you should be very familiar with the mapping already because um it's the same almost the same information that I'm copying and pasting it from module one and you could see that uh for the mappings here in the property section you could see there are different fields these fields should be the same fields that we created here fix section question right so these are all the same fields and you could see the data types associated with it the only difference is what we have done today is we have created something called a text Vector which is nothing but a dense vector and uh we need to also specify the dimensions here so remember the dimensions we got it from our inference initially just go back a step here just want to show you real quick so that's the dimensions it is also mentioned on the model uh sentence Transformer documentation for every models as well and then you need to also specify the similarity metric in which in which your the text Vector should be stored um in the in the vector database so there are several metrics that's available so it could be cosine Matrix it could be n to Norm dot product ukian distance um I'm I'm leaving I will leave you some reference uh in the in the references section so you can read through it but by far the most popular type is the is the cosine or a DOT prodct all right so that's pretty much it so let's just run this so now let's go ahead and uh create the index so you could see here the bottom line here so this is how you create the index so you need to specify IND isis. create provide the index name that you have defined here and also pass all the required metadata that we have created here and what I'm doing here on the top is I'm creating something called a delete indexes statement that's because what I've noticed is when you're working through this data set or when you're working on this project you might realize that you you updated the data set or you wanted to change the data type in scenarios like those you go and update the mapping but when you try to create the index with the same name you will get an error so in order for you to avoid that what you can do is as a best practice every time you can just delete the index uh if it exists and if it doesn't exist you can ignore this delete statement so that's all this this line is doing here so now let's go and create the index fantastic that step is done so now the index is created we already have the embeddings created in the document so now what you need to do is you need to populate those embeddings or the whole document into the index so that's what we're going to do here for that we need to just Loop through the operations data set that we have prepared and um what we need to do is for every document you need to push that document into the indexes into the index that's that's what the statement is is doing it might take about a minute or two so let's resume after this statement is complete or this execution is complete this step is now complete so let's go back and take a look at the architecture to see where we are so we have pretty much completed the first portion of our architecture so we took the documents. Json file we created documents out of it and then we took a pre-trained model and in that pre-trained model we passed the documents to create embeddings and once that embeddings are created then we pushed it into the elastic searches index and so with all of the the blue all of the black lines are complete now so now let's go and build the new search query so that's the search term that a end user um wants to query the data on and what we will do is we will pass that to the embeddings as well and then look at the results all right so for that let's go to this section let's create a search term here so the search term the end user wants to search for is Windows or Mac so the user wants to know for the course should they bring a Windows laptop or a Mac laptop or a Windows Os or a Mac OS and remember every term um that the search term that the user is is searching has to be converted it into in into a vector as well so that's what this line is doing and once that step is done let's go and run this now we have to form this query so you could see here so we're building a query and in the query what we what we specifying is in the database which field should the should the database go and search for and what is the the vector that the user is is searching so the user is searching Windows or Mac so and that is converted into a vector and that is that is what this query _ Vector is taking as input and then K5 meaning you want to provide five nearest uh neighbors or five nearest documents uh that matches to this particular search term and number of candidates it's nothing but if you can think about this like a group of documents that um the search is going to look into and you can play around with this number um like 1,000 2,000 and then you can play which whatever works for you and that's pretty much it so now what we are going to do is we are going to pass this to elastic searches search functionality a search method so you could see here there is a search method and it takes the index name and then it takes the query that we have built here in this previous step and then in the results you're specifying what are the fields that you want in the results pack so what I'm going to do is I'm going to just uh in the response that we're going to get I know that the response is uh is mentioned in this particular um field so let's go and run this Fant fantastic so let's open up the results in the text editor so the first thing what I wanted to show you the mention is you could see you remember we mentioned ks5 so you should see one two 3 four five so Five results so you got back Five results and then you could see for every result there is a score so the score the the results are ranked um by the highest score on the top and the and then and then the lowest score on the bottom and the score of anytime a score is close to one it indicates that it's a very good match and anytime a score is close to zero it denotes it's a very bad match so we got a score of 71 which is a pretty decent score so let's go and look at the look at the text on what what did what what did we see here so remember what the inducer search for the inducer was searching for yeah Windows or Mac right and the first result you got back is yes Linux is ideal but technically it should not matter so now think about it right nowhere in the query the end user mentioned whether this is an operating system or a laptop they just mentioned Windows or Mac however this semantic search was smart enough to understand the semantics meaning the the the underlying meanings associated with the end user search so he understood that it's talking about the operating system so it went and searched for the the closest match that that um matches with the operating system and it gave you this this information so you can you can now see the power that uh that that that you can uncover in scenarios like this because this is exactly how your Google search works so recall the in the first introduction section in in this module we talked about how Apple how the Google is ranking based on whether if apple is a flute your search results will vary and if apple is a company your search results will vary so that's exactly how semantic search works behind the scenes so congratulations you built a semantic search in this section um I also wanted to call out that uh this is just a very simple semantic search we built there are much much more complex um semantic searches you can build and I also want to show you one L little bit of um combination of um couple of um queries here so what you can also do is anytime when the IND user is searching you can use um your regular um query search on top of it you can also make sure that the results can be should be filtered from only one course remember there are this data set has information from three courses but what if you want to only look at one course because let's assume that this user is asking this question in the data engineering Zoom Camp course so you want the results only from this course so you can basically chain your Search terms accordingly and then you can filter them you can you can query them um based on um different combinations that is very powerful for for semantic search let's run this results let's look at the result set perfect so you could see now that the result is only coming from the data engineering Zoom Camp so that's good congratulations you built a semantic search engine using elastic search Vector database I hope you enjoyed the session thank you so much for joining happy learning Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '956710587f222679f712ceb6efdb4461'},\n",
       " {'vid_id': 'yb3nYGuIL4c',\n",
       "  'title': 'LLM Zoomcamp 3.2.2 - Advanced Semantic Search',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"welcome back in the previous session we saw how to create a vector database using elastic search then we implemented a basic functionality for semantic search and then we also implemented an advanced semantic search functionality however later I realized that in the advanced semantic search that we implemented the search is really not a semantic search rather it is called a keyword search the main reason for that is anytime you are directly using the users input and passing that information into your search function that becomes a keyword search but in order for elastic search to perform the semantic search what we should do is we should pass the information that we receive from the end user and convert that into a vector embedding and that Vector embedding needs to be passed into the search function so that was the key step that we missed last time so what we will do in this session is we will go ahead and get the correct implementation for advanced search for that I'm going to comment this let's go and get the code awesome so here is the correct semantic Advanced semantic search code and the main difference between this one and the previous code as I mentioned earlier is in this case we are passing the vector embedding if you recall from your previous session this Vector search term was created for basic semantic search right so you go to the top here we created the vector search term for our basic semantic search and you could see for the search term that we received from the end user what we did is we created a vector embed so now we are passing that information into the advanced semantic search functionality and the other value should be very familiar to you exactly the same as the uh basic semantic search and what we are doing in the advanced search is we are restricting the results come from only one course which is data engineering Zoom Camp let's go and run this code so you could see the results perfect let's open this in the text editor so we can see the results better I am going to hi the embeddings so you could see the results much easier pleas perfect if you look at the results now the results should look very familiar to you based on your basic semantic search what we've also done is we have restricted or filtered the results to only come from one course which is data engineering Zoom Camp so you could see the data engineering Zoom Camp is the only course that the results are coming from perfect one other thing I wanted to call out is if you look at the score here recall when we performed a basic semantic search the scores were between 0 and one where zero indicated not a good match whereas one indicated a very good match if you notice here in the advanc semantic search you could see the scores have a value of greater than one so one thing I wanted you to remember is the core functionality is handled by elastic search and when you're performing basic semantic search you could see the scores values between 0o and one but anytime you're doing any sort of advanced functionalities the values may be in varying scales so if that happens and if you're not sure why those varying scale scores are showing up the first thing is for you the first recommendation is you go and look at the results and validate the results to see if the results make sense and if you think the results are good it's just the scales are coming from various um ranges then what you can do with this you can go and add another keyword called explain is equal to true and what this keyword will show you is how the scores are calculated so that could give you a lot more information on how the scores are calculated and using that information what you can potentially do is so you can create your own custom scoring function that could make sense for your end user or the business so if you look at here the explanation for the scores is provided here and it also details of how those values were computed this could help you to understand the reason behind the scoring ranges and as I mentioned before you can always override the scores elastic search provides the capability to do that as well I hope you enjoyed the session thanks for joining have a great day\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=yb3nYGuIL4c',\n",
       "  'text_vector': \"LLM Zoomcamp 3.2.2 - Advanced Semantic Search welcome back in the previous session we saw how to create a vector database using elastic search then we implemented a basic functionality for semantic search and then we also implemented an advanced semantic search functionality however later I realized that in the advanced semantic search that we implemented the search is really not a semantic search rather it is called a keyword search the main reason for that is anytime you are directly using the users input and passing that information into your search function that becomes a keyword search but in order for elastic search to perform the semantic search what we should do is we should pass the information that we receive from the end user and convert that into a vector embedding and that Vector embedding needs to be passed into the search function so that was the key step that we missed last time so what we will do in this session is we will go ahead and get the correct implementation for advanced search for that I'm going to comment this let's go and get the code awesome so here is the correct semantic Advanced semantic search code and the main difference between this one and the previous code as I mentioned earlier is in this case we are passing the vector embedding if you recall from your previous session this Vector search term was created for basic semantic search right so you go to the top here we created the vector search term for our basic semantic search and you could see for the search term that we received from the end user what we did is we created a vector embed so now we are passing that information into the advanced semantic search functionality and the other value should be very familiar to you exactly the same as the uh basic semantic search and what we are doing in the advanced search is we are restricting the results come from only one course which is data engineering Zoom Camp let's go and run this code so you could see the results perfect let's open this in the text editor so we can see the results better I am going to hi the embeddings so you could see the results much easier pleas perfect if you look at the results now the results should look very familiar to you based on your basic semantic search what we've also done is we have restricted or filtered the results to only come from one course which is data engineering Zoom Camp so you could see the data engineering Zoom Camp is the only course that the results are coming from perfect one other thing I wanted to call out is if you look at the score here recall when we performed a basic semantic search the scores were between 0 and one where zero indicated not a good match whereas one indicated a very good match if you notice here in the advanc semantic search you could see the scores have a value of greater than one so one thing I wanted you to remember is the core functionality is handled by elastic search and when you're performing basic semantic search you could see the scores values between 0o and one but anytime you're doing any sort of advanced functionalities the values may be in varying scales so if that happens and if you're not sure why those varying scale scores are showing up the first thing is for you the first recommendation is you go and look at the results and validate the results to see if the results make sense and if you think the results are good it's just the scales are coming from various um ranges then what you can do with this you can go and add another keyword called explain is equal to true and what this keyword will show you is how the scores are calculated so that could give you a lot more information on how the scores are calculated and using that information what you can potentially do is so you can create your own custom scoring function that could make sense for your end user or the business so if you look at here the explanation for the scores is provided here and it also details of how those values were computed this could help you to understand the reason behind the scoring ranges and as I mentioned before you can always override the scores elastic search provides the capability to do that as well I hope you enjoyed the session thanks for joining have a great day Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'aef0b065ccbf47d546f71e1eb7f8651d'},\n",
       " {'vid_id': 'APMrUnC_dy0',\n",
       "  'title': 'LLM Zoomcamp 3.3.1 - Evaluation Metrics for Retrieval',\n",
       "  'timecode': '0:00',\n",
       "  'text': 'hi everyone in this bunch of videos in this series of videos we want to talk about evolation Evola in our search results this is the r part in our uh rck where we retrieve data from our knowledge base and there are many ways uh of storing and getting the data for the r part so we have seen Min search we have seen elastic search with text only retrieval we have seen also Vector search also in our Cas it was elastic search and you might be wondering what is actually the best way of doing this and the answer is typically it depends it really depends on your data and your requirements and you need to have a reliable way of saying okay for this data set for this problem I have this is the best way and for that we use evaluation techniques so in this series we will talk about different evaluation metrics and we will also talk about um getting the data we need for this',\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video series, I talk about evaluation metrics for ranking. I discuss the importance of choosing the right method based on data and requirements, emphasizing the need for reliable evaluation techniques to assess system performance. Viewers will learn about generating ground truth data sets and evaluating search results using various metrics.',\n",
       "  'link': 'https://www.youtube.com/watch?v=APMrUnC_dy0&t=0s',\n",
       "  'text_vector': 'LLM Zoomcamp 3.3.1 - Evaluation Metrics for Retrieval hi everyone in this bunch of videos in this series of videos we want to talk about evolation Evola in our search results this is the r part in our uh rck where we retrieve data from our knowledge base and there are many ways uh of storing and getting the data for the r part so we have seen Min search we have seen elastic search with text only retrieval we have seen also Vector search also in our Cas it was elastic search and you might be wondering what is actually the best way of doing this and the answer is typically it depends it really depends on your data and your requirements and you need to have a reliable way of saying okay for this data set for this problem I have this is the best way and for that we use evaluation techniques so in this series we will talk about different evaluation metrics and we will also talk about um getting the data we need for this In this video series, I talk about evaluation metrics for ranking. I discuss the importance of choosing the right method based on data and requirements, emphasizing the need for reliable evaluation techniques to assess system performance. Viewers will learn about generating ground truth data sets and evaluating search results using various metrics.',\n",
       "  'id': '6aceb53945b0442b86c9a085966864c8'},\n",
       " {'vid_id': 'APMrUnC_dy0',\n",
       "  'title': 'LLM Zoomcamp 3.3.1 - Evaluation Metrics for Retrieval',\n",
       "  'timecode': '1:00',\n",
       "  'text': \"evolation metrix so typically you need to have some sort of gold standard data set or ground prooof data set uh in this data set for each query so for example you have a query uh um like I just discover that the course uh can I still join right so you have a query and then you know for this query which are the relevant documents so relevant documents and could be like Doc 1 doc 10 uh doc 11 right and then you have I don't know th000 2,000 3,000 whatever many queries and for each of these queries you know which items are relevant and you can use different evaluation metrics in order to understand uh if your system is actually performing well or not because you know uh that you expect to see this this and this document in the search results and if you see them it's good if you don't see them or if you don't see all of them it's not good right and then you can apply different relation metrics uh different queries to different retal techniques different retal tools methods and see which one has the best uh is able to retrieve the the the relevant documents best right so this is the idea behind evaluation and in this series of videos we will talk about getting this uh ground roof uh uh or gold uh standard data and uh we will see how to generate this gold standard data with an llm typically in production systems you have some feedback maybe from users you also have ulators who look at the results so for example a user performance query then uh you can see at the results and see if these results are good uh or not and then sometimes it's people who look at this sometimes maybe often times these days it could be an llm but then you have a way typically of sampling data from your production uh deployment in our case we do not have yet a production deployment so what we will do is we will generate the ground to data with an llm uh and we we will use this data to aate the search results and we will see which of the methods of the retrial methods work best so this is the plan and before I go to the next video I want\",\n",
       "  'timecode_text': 'Ground truth data',\n",
       "  'description': 'In this video series, I talk about evaluation metrics for ranking. I discuss the importance of choosing the right method based on data and requirements, emphasizing the need for reliable evaluation techniques to assess system performance. Viewers will learn about generating ground truth data sets and evaluating search results using various metrics.',\n",
       "  'link': 'https://www.youtube.com/watch?v=APMrUnC_dy0&t=60s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.1 - Evaluation Metrics for Retrieval evolation metrix so typically you need to have some sort of gold standard data set or ground prooof data set uh in this data set for each query so for example you have a query uh um like I just discover that the course uh can I still join right so you have a query and then you know for this query which are the relevant documents so relevant documents and could be like Doc 1 doc 10 uh doc 11 right and then you have I don't know th000 2,000 3,000 whatever many queries and for each of these queries you know which items are relevant and you can use different evaluation metrics in order to understand uh if your system is actually performing well or not because you know uh that you expect to see this this and this document in the search results and if you see them it's good if you don't see them or if you don't see all of them it's not good right and then you can apply different relation metrics uh different queries to different retal techniques different retal tools methods and see which one has the best uh is able to retrieve the the the relevant documents best right so this is the idea behind evaluation and in this series of videos we will talk about getting this uh ground roof uh uh or gold uh standard data and uh we will see how to generate this gold standard data with an llm typically in production systems you have some feedback maybe from users you also have ulators who look at the results so for example a user performance query then uh you can see at the results and see if these results are good uh or not and then sometimes it's people who look at this sometimes maybe often times these days it could be an llm but then you have a way typically of sampling data from your production uh deployment in our case we do not have yet a production deployment so what we will do is we will generate the ground to data with an llm uh and we we will use this data to aate the search results and we will see which of the methods of the retrial methods work best so this is the plan and before I go to the next video I want In this video series, I talk about evaluation metrics for ranking. I discuss the importance of choosing the right method based on data and requirements, emphasizing the need for reliable evaluation techniques to assess system performance. Viewers will learn about generating ground truth data sets and evaluating search results using various metrics.\",\n",
       "  'id': '56088a6f5c22c247443d3ffca3fdaa46'},\n",
       " {'vid_id': 'APMrUnC_dy0',\n",
       "  'title': 'LLM Zoomcamp 3.3.1 - Evaluation Metrics for Retrieval',\n",
       "  'timecode': '3:40',\n",
       "  'text': \"to spend spend a little bit of time talking about different evolation Matrix so these are um so these are the ways of computing uh a particular number to a a retrieval method so for example uh we used previously uh this way of getting data from elastic search right and here we can choose quite a few things in this search query right so we can choose a different type then we can choose a different boosting coefficient we can choose which fields we include so for example um is it better if we include section or if we drop the section is it better that we give question boost of three or five right there are millions of um things we can try and for us to uh to say okay this is the best way we need to have this gold standard data set uh evaluation data set right and also like there are could be some other tools there are so many uh different Vector databases and you want to see which one is the best or there are so many ways of embedding your uh text Data turning it into a vector right which one was the best one and the answer is always it depends and it depends on the qualities of your data uh the qualities I mean characteristics because it's Unique for each data set and you need to use again the evolation metrics what are those evalation metrics so I ask chpt what are the common ranking evolation metrics and here I use term ranking because typically we in Search and also in Rec Commander systems we have a um ranked list so we have a query we have a lot of documents and we for each document we have in our knowledge base we want to see how relevant this document to this particular query and then we sort all the documents by the relevance uh Factor but the relevance score and we return the top ranking ones so our problem is ranking here and for ranking the common um evaluation metrics you can see them here I I will include them in um in the list uh in the GitHub repo and uh I'll probably record the explanation of some of them in a separate video and we will all of them are good they all serve different purposes we will use um so we will use heat rate and we will use uh this one mrr but like it doesn't have to be this ones I just think that they fit uh the problem we have but of course like sometimes uh it mean makes more sense to choose something else so in a separate video I'll record the explanation about these two uh but what we are going to do next is uh we will spend some time talking about uh gold standard data set and how we can actually get it uh using nlm so see you soon\",\n",
       "  'timecode_text': 'Evaluation metrics',\n",
       "  'description': 'In this video series, I talk about evaluation metrics for ranking. I discuss the importance of choosing the right method based on data and requirements, emphasizing the need for reliable evaluation techniques to assess system performance. Viewers will learn about generating ground truth data sets and evaluating search results using various metrics.',\n",
       "  'link': 'https://www.youtube.com/watch?v=APMrUnC_dy0&t=220s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.1 - Evaluation Metrics for Retrieval to spend spend a little bit of time talking about different evolation Matrix so these are um so these are the ways of computing uh a particular number to a a retrieval method so for example uh we used previously uh this way of getting data from elastic search right and here we can choose quite a few things in this search query right so we can choose a different type then we can choose a different boosting coefficient we can choose which fields we include so for example um is it better if we include section or if we drop the section is it better that we give question boost of three or five right there are millions of um things we can try and for us to uh to say okay this is the best way we need to have this gold standard data set uh evaluation data set right and also like there are could be some other tools there are so many uh different Vector databases and you want to see which one is the best or there are so many ways of embedding your uh text Data turning it into a vector right which one was the best one and the answer is always it depends and it depends on the qualities of your data uh the qualities I mean characteristics because it's Unique for each data set and you need to use again the evolation metrics what are those evalation metrics so I ask chpt what are the common ranking evolation metrics and here I use term ranking because typically we in Search and also in Rec Commander systems we have a um ranked list so we have a query we have a lot of documents and we for each document we have in our knowledge base we want to see how relevant this document to this particular query and then we sort all the documents by the relevance uh Factor but the relevance score and we return the top ranking ones so our problem is ranking here and for ranking the common um evaluation metrics you can see them here I I will include them in um in the list uh in the GitHub repo and uh I'll probably record the explanation of some of them in a separate video and we will all of them are good they all serve different purposes we will use um so we will use heat rate and we will use uh this one mrr but like it doesn't have to be this ones I just think that they fit uh the problem we have but of course like sometimes uh it mean makes more sense to choose something else so in a separate video I'll record the explanation about these two uh but what we are going to do next is uh we will spend some time talking about uh gold standard data set and how we can actually get it uh using nlm so see you soon In this video series, I talk about evaluation metrics for ranking. I discuss the importance of choosing the right method based on data and requirements, emphasizing the need for reliable evaluation techniques to assess system performance. Viewers will learn about generating ground truth data sets and evaluating search results using various metrics.\",\n",
       "  'id': '81ef4abaac5f74ca24d4275cc62d4207'},\n",
       " {'vid_id': 'bpxi6fKcyLw',\n",
       "  'title': 'LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation',\n",
       "  'timecode': '00:00',\n",
       "  'text': 'hi everyone we have talked about the importance of evaluating our retrial results and uh we talked also about the importance of having a ground roof gold standard data set this is a data set that looks like that so we have thousands uh of queries could be 1,000 2,000 10,000 many as many as possible right and then we have a bunch of queries and for each query we know what are the documents in our knowledge base in our data set that are relevant for this query right and then you have many such queries uh query document pairs right and uh typically uh for one query',\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.',\n",
       "  'link': 'https://www.youtube.com/watch?v=bpxi6fKcyLw&t=0s',\n",
       "  'text_vector': 'LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation hi everyone we have talked about the importance of evaluating our retrial results and uh we talked also about the importance of having a ground roof gold standard data set this is a data set that looks like that so we have thousands uh of queries could be 1,000 2,000 10,000 many as many as possible right and then we have a bunch of queries and for each query we know what are the documents in our knowledge base in our data set that are relevant for this query right and then you have many such queries uh query document pairs right and uh typically uh for one query In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.',\n",
       "  'id': '8da8ea0d5c0e9f0695199551f500029d'},\n",
       " {'vid_id': 'bpxi6fKcyLw',\n",
       "  'title': 'LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation',\n",
       "  'timecode': '00:40',\n",
       "  'text': \"you have multiple relevant documents but we will simplify our problem a little bit and we will generate in this video we will generate a data set where for each query you know one um what is the relevant document that actually talks about this uh this is a simplification because in practice uh in our data set and Q&A data set for one question from a student could be multiple answers but this is already good enough so if we manage to retrieve at least one relevant answer then there could be others but if at least one this is good to go so what we're going to do is we will generate uh for for each question in our FAQ uh we will uh generate five um questions uh for each Al call record in fq will generate five questions and then uh for each of these questions we know that uh this record in fq is the relevant one for this question because we used it for generating so what we will have is we'll have um so if our data set is like I don't remember how 10 oh like 1,000 uh fq Records so we will have uh roughly 5,000 uh query queries and for each of these query we will know what is the relevant document so we are going to use nlm for that in practice there are many different ways of actually collecting or generating this data one of them is actually using humans uh human anator uh also domain experts what they do is they look at the documents you have in the knowledge base and they look at the queries that the user generate and they say okay like for this query This Record contains the answer for this query This Record contains an answer this is of course the most time consuming way but usually the result is quite uh that's why it's called gold standard because it's uh difficult to get collect but usually it's good quality then another way of doing of collecting such data set of creating such data set is just observing what kind of um queries user generate user ask the system then seeing what the system returns and then evaluating these results right so usually it's also quite often is done by human annotators it could also be done and maybe these days um it's done by llms so then you can ask okay these are the results this is the query which of these documents are relevant right um we will do the simplest thing we will do this one because we don't have a production system yet but we already want to experiment and find um a good approach um like the best uh retrieval technique among those do we TR we want to select the one that works best on our particular data set and for these purposes what we are going to do is sufficient okay so um I have\",\n",
       "  'timecode_text': 'Problem simplification and algorithm for data generation',\n",
       "  'description': 'In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.',\n",
       "  'link': 'https://www.youtube.com/watch?v=bpxi6fKcyLw&t=40s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation you have multiple relevant documents but we will simplify our problem a little bit and we will generate in this video we will generate a data set where for each query you know one um what is the relevant document that actually talks about this uh this is a simplification because in practice uh in our data set and Q&A data set for one question from a student could be multiple answers but this is already good enough so if we manage to retrieve at least one relevant answer then there could be others but if at least one this is good to go so what we're going to do is we will generate uh for for each question in our FAQ uh we will uh generate five um questions uh for each Al call record in fq will generate five questions and then uh for each of these questions we know that uh this record in fq is the relevant one for this question because we used it for generating so what we will have is we'll have um so if our data set is like I don't remember how 10 oh like 1,000 uh fq Records so we will have uh roughly 5,000 uh query queries and for each of these query we will know what is the relevant document so we are going to use nlm for that in practice there are many different ways of actually collecting or generating this data one of them is actually using humans uh human anator uh also domain experts what they do is they look at the documents you have in the knowledge base and they look at the queries that the user generate and they say okay like for this query This Record contains the answer for this query This Record contains an answer this is of course the most time consuming way but usually the result is quite uh that's why it's called gold standard because it's uh difficult to get collect but usually it's good quality then another way of doing of collecting such data set of creating such data set is just observing what kind of um queries user generate user ask the system then seeing what the system returns and then evaluating these results right so usually it's also quite often is done by human annotators it could also be done and maybe these days um it's done by llms so then you can ask okay these are the results this is the query which of these documents are relevant right um we will do the simplest thing we will do this one because we don't have a production system yet but we already want to experiment and find um a good approach um like the best uh retrieval technique among those do we TR we want to select the one that works best on our particular data set and for these purposes what we are going to do is sufficient okay so um I have In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.\",\n",
       "  'id': '70f96d00a2654e9ada5920145b58597b'},\n",
       " {'vid_id': 'bpxi6fKcyLw',\n",
       "  'title': 'LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation',\n",
       "  'timecode': '03:55',\n",
       "  'text': \"already downloaded the the documents so this is um what we used before so this is the same the same documents uh here one of the problems is that we don't have um an ID for each of these documents right because we we actually need to know uh for this record what is the um for this record uh what's the idea of this record so later when we retrieve it we want to to understand um that this is this record and this is relevant for this particular query right so we need to have a way to assign a unique number to each of the documents um one of the possible things we can do is just give a number so for example we can do for I in range so let let's call and the number of documents right and then for range and uh what we can do is um uh documents I ID equals I right so then now our records will have an ID one three right so the problem with that is um when we update our um RF FAQs uh the number will change right so then uh if we generated some data already ground through data uh but then somebody changed our fq then and we index it then our like our IDs kind of become broken right so this is not always the best way uh the actual best way for our particular case could be uh using the IDS from sections um so here we have this um table of content now it will load and if we click on something we can see that we have heading and this is the ID for the heading right and uh because we use uh because the way we generated our data set the way we passed this document was first by downloading a doix file and then using python doix to read the content of this file unfortunately this is lost so ideally we should have used some Google API for accessing the document parsing it because uh because it preserves it keeps the the ID in our case we don't have this ID it would be ideal cuz uh when we change the Capt when we change the title The Heading here the ID still stays the same so this is great right um so in our case what we can do is the second best thing is we can generate ID based on the content so we can look at combination of the course question um to generate an ID and I have already written a function here so what this function is doing uh it um takes a combination of course question and text then creates an md5 hash of that and and then takes eight First characters and the reason this one is better than this approach is because um here we do not depend on the order we only depend on the content but the moment somebody changes the the question the title of the question then the hash becomes different so we have a problem again so this approach of trying to keep the ad is better but we will just do what we can in these conditions right so of course we can think of other ways of doing that but for now I'll just go with this approach um which will be I'll go over uh our documents and then for each document I'll calculate document ID using this function so now we will have this are right um so here I this is how I\",\n",
       "  'timecode_text': 'Generating IDs for each record',\n",
       "  'description': 'In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.',\n",
       "  'link': 'https://www.youtube.com/watch?v=bpxi6fKcyLw&t=235s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation already downloaded the the documents so this is um what we used before so this is the same the same documents uh here one of the problems is that we don't have um an ID for each of these documents right because we we actually need to know uh for this record what is the um for this record uh what's the idea of this record so later when we retrieve it we want to to understand um that this is this record and this is relevant for this particular query right so we need to have a way to assign a unique number to each of the documents um one of the possible things we can do is just give a number so for example we can do for I in range so let let's call and the number of documents right and then for range and uh what we can do is um uh documents I ID equals I right so then now our records will have an ID one three right so the problem with that is um when we update our um RF FAQs uh the number will change right so then uh if we generated some data already ground through data uh but then somebody changed our fq then and we index it then our like our IDs kind of become broken right so this is not always the best way uh the actual best way for our particular case could be uh using the IDS from sections um so here we have this um table of content now it will load and if we click on something we can see that we have heading and this is the ID for the heading right and uh because we use uh because the way we generated our data set the way we passed this document was first by downloading a doix file and then using python doix to read the content of this file unfortunately this is lost so ideally we should have used some Google API for accessing the document parsing it because uh because it preserves it keeps the the ID in our case we don't have this ID it would be ideal cuz uh when we change the Capt when we change the title The Heading here the ID still stays the same so this is great right um so in our case what we can do is the second best thing is we can generate ID based on the content so we can look at combination of the course question um to generate an ID and I have already written a function here so what this function is doing uh it um takes a combination of course question and text then creates an md5 hash of that and and then takes eight First characters and the reason this one is better than this approach is because um here we do not depend on the order we only depend on the content but the moment somebody changes the the question the title of the question then the hash becomes different so we have a problem again so this approach of trying to keep the ad is better but we will just do what we can in these conditions right so of course we can think of other ways of doing that but for now I'll just go with this approach um which will be I'll go over uh our documents and then for each document I'll calculate document ID using this function so now we will have this are right um so here I this is how I In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.\",\n",
       "  'id': '03e639d69d74b2df90e7da77b141bf0c'},\n",
       " {'vid_id': 'bpxi6fKcyLw',\n",
       "  'title': 'LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation',\n",
       "  'timecode': '08:30',\n",
       "  'text': \"combined it this is how I created it and I wanted to show why I chose this particular way so for example we could have uh instead of uh using this combination we could have used something like this right uh we without including the text right then this would be our ID and we can see how many uh how unique these IDs are by creating I'll use default dictionary um from collections import default dictionary uh so default dictionary is when we don't have a record it uses some function that we provide to initialize an item in this dictionary which is which is quite convenient when we want to do some sort of group by uh logic in our case we will create a dictionary hashes with list inside we will go over all the documents and we will get the dock ID and we will put this um uh documents by doc ID in our um PES up and Doc right and then we can look at the length of this think and we can look at the length of our documents ideally they should be the same and this will mean that um our IDs are unique across the data set I can see that they are not so we have actually fewer hashes than uh documents and we can see what these hashes are by uh looping uh uh over the hashes items um if Len value use more than one print key right so these are the hashes that are not unique let me also print to see how many um like this is uh hash Collision or ID Collision when multiple items have the same ID but in our case um actually what happens we can see that um these questions are the Collision happens because these questions are duplicates right so we can see that um this and this is the same this is why they have the same IDE right it's the same title and we apply the hash function so we get the same um uh the same ID so what I did uh so ideally maybe we should work on our original data set and try to um somehow uh fix this problem by merging these two questions into one because this is clearly a duplicate but the way I address it is I also included the first 10 characters of the question of the answer because here and here they are different right so now if I um re execute that we will have um different hash we still have some collisions in particular it's one Collision for this document uh yeah for this document cuz uh they start with the same text but I think like for this one I just ignore I say whatever okay we have a duplicate I don't care we just kind leave with that because D like real life data is dirty data and yeah sometimes there are duplicates okay so now uh what I'm going to do is I'm going to save this uh to Json so then you can also use it you don't have to do this uh you can just download data and that we created documents withis Json uh so it will be right text as F out and then I do Json I need to import Json okay one uh Json D so first is the object the documents and second will be uh the file ID and if we look now at this this is what we can see that everything is in one line not very convenient so we can add a bit of identation so then it's more readable right so this is our uh documents with ads and now the fun part now we are\",\n",
       "  'timecode_text': 'ID collisions',\n",
       "  'description': 'In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.',\n",
       "  'link': 'https://www.youtube.com/watch?v=bpxi6fKcyLw&t=510s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation combined it this is how I created it and I wanted to show why I chose this particular way so for example we could have uh instead of uh using this combination we could have used something like this right uh we without including the text right then this would be our ID and we can see how many uh how unique these IDs are by creating I'll use default dictionary um from collections import default dictionary uh so default dictionary is when we don't have a record it uses some function that we provide to initialize an item in this dictionary which is which is quite convenient when we want to do some sort of group by uh logic in our case we will create a dictionary hashes with list inside we will go over all the documents and we will get the dock ID and we will put this um uh documents by doc ID in our um PES up and Doc right and then we can look at the length of this think and we can look at the length of our documents ideally they should be the same and this will mean that um our IDs are unique across the data set I can see that they are not so we have actually fewer hashes than uh documents and we can see what these hashes are by uh looping uh uh over the hashes items um if Len value use more than one print key right so these are the hashes that are not unique let me also print to see how many um like this is uh hash Collision or ID Collision when multiple items have the same ID but in our case um actually what happens we can see that um these questions are the Collision happens because these questions are duplicates right so we can see that um this and this is the same this is why they have the same IDE right it's the same title and we apply the hash function so we get the same um uh the same ID so what I did uh so ideally maybe we should work on our original data set and try to um somehow uh fix this problem by merging these two questions into one because this is clearly a duplicate but the way I address it is I also included the first 10 characters of the question of the answer because here and here they are different right so now if I um re execute that we will have um different hash we still have some collisions in particular it's one Collision for this document uh yeah for this document cuz uh they start with the same text but I think like for this one I just ignore I say whatever okay we have a duplicate I don't care we just kind leave with that because D like real life data is dirty data and yeah sometimes there are duplicates okay so now uh what I'm going to do is I'm going to save this uh to Json so then you can also use it you don't have to do this uh you can just download data and that we created documents withis Json uh so it will be right text as F out and then I do Json I need to import Json okay one uh Json D so first is the object the documents and second will be uh the file ID and if we look now at this this is what we can see that everything is in one line not very convenient so we can add a bit of identation so then it's more readable right so this is our uh documents with ads and now the fun part now we are In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.\",\n",
       "  'id': '4ca7156800bc72f5e93f77b1d120935e'},\n",
       " {'vid_id': 'bpxi6fKcyLw',\n",
       "  'title': 'LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation',\n",
       "  'timecode': '13:25',\n",
       "  'text': \"going to use an llm and and for each record here uh in uh this set we are going to generate user questions so let's start with uh writing a\",\n",
       "  'timecode_text': 'Using LLMs for generating user questions',\n",
       "  'description': 'In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.',\n",
       "  'link': 'https://www.youtube.com/watch?v=bpxi6fKcyLw&t=805s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation going to use an llm and and for each record here uh in uh this set we are going to generate user questions so let's start with uh writing a In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.\",\n",
       "  'id': '876f639a9ed271d6d877f9fc0fa01b2b'},\n",
       " {'vid_id': 'bpxi6fKcyLw',\n",
       "  'title': 'LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation',\n",
       "  'timecode': '13:40',\n",
       "  'text': \"prompt prompt template um so here we can say something like you um am late a student uh taking our course and generate uh five user queries five user questions based on the provided FAQ record um the questions should uh let me actually see what what was the prompt I used before I think it's better if I just copy and then we spend a little bit less time so this is the prompt I brought ulate a student who is taken our course formulate five questions the student might ask by based on FAQ record uh the record should contain the answer to the question and the question should be complete and not too short so I added this while experimenting cuz that it was like creating super short questions that Wen even um like they didn't make much sense so I included complete and not to short and also um this part if possible use as fewer words as possible from the record because we want to formulate it not in the same way right so then we can test how well our retrieval system performs when uh there are fewer words uh from this records um um in the query right so then it's especially important for testing our text based searches and then um we provide the record and we say provide the output in part ible Json without using Code blocks um so without using Code blocks I also edit this cuz if I did not do this if I don't do this then it adds um this thing Json and then response so this would be the answer right because this is what it uses in Char BT um so then the code is formatted so that's why I use without at without using Code blocks so this prompt and now let's get our charb client where it is um I probably don't have the key yeah so\",\n",
       "  'timecode_text': 'Prompt',\n",
       "  'description': 'In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.',\n",
       "  'link': 'https://www.youtube.com/watch?v=bpxi6fKcyLw&t=820s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation prompt prompt template um so here we can say something like you um am late a student uh taking our course and generate uh five user queries five user questions based on the provided FAQ record um the questions should uh let me actually see what what was the prompt I used before I think it's better if I just copy and then we spend a little bit less time so this is the prompt I brought ulate a student who is taken our course formulate five questions the student might ask by based on FAQ record uh the record should contain the answer to the question and the question should be complete and not too short so I added this while experimenting cuz that it was like creating super short questions that Wen even um like they didn't make much sense so I included complete and not to short and also um this part if possible use as fewer words as possible from the record because we want to formulate it not in the same way right so then we can test how well our retrieval system performs when uh there are fewer words uh from this records um um in the query right so then it's especially important for testing our text based searches and then um we provide the record and we say provide the output in part ible Json without using Code blocks um so without using Code blocks I also edit this cuz if I did not do this if I don't do this then it adds um this thing Json and then response so this would be the answer right because this is what it uses in Char BT um so then the code is formatted so that's why I use without at without using Code blocks so this prompt and now let's get our charb client where it is um I probably don't have the key yeah so In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.\",\n",
       "  'id': 'a903130522762af88fa8b065d5a1ee02'},\n",
       " {'vid_id': 'bpxi6fKcyLw',\n",
       "  'title': 'LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation',\n",
       "  'timecode': '16:08',\n",
       "  'text': \"I need to configure it I'll put it on pause and I'll be right back okay so now the key I put the key and we can continue with um so actually this is not where I wanted to put the key of course okay hopefully now this is the right one yeah so I put the key I am able to import it and what I'm going to do now is use this prompt template let me create a prompt now prom template format and I'll just use um a document from one of the documents from our knowledge base let's use this one and we can see the prompt prompt um so this is the prompt and now we are going to send this to open AI of course and this is the code we use so this is the same code we used before so response um we use this CH completion thing um so let me just EX execute this part um and see what it returns okay so this is uh the result and now we can pass uh uh loads and pass the Json response and this is what we get right so now we just need to do this for all the documents we have and uh uh yeah let me actually check what we have can I enroll the course after it has already started if I'm if I join later can I still submit assignments am I allowed to register and submit homework after yeah so like you see there are different variations variations of questions um which is good this is what we want right and now we just need to do it for\",\n",
       "  'timecode_text': 'OpenAI for generating the questions',\n",
       "  'description': 'In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.',\n",
       "  'link': 'https://www.youtube.com/watch?v=bpxi6fKcyLw&t=968s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation I need to configure it I'll put it on pause and I'll be right back okay so now the key I put the key and we can continue with um so actually this is not where I wanted to put the key of course okay hopefully now this is the right one yeah so I put the key I am able to import it and what I'm going to do now is use this prompt template let me create a prompt now prom template format and I'll just use um a document from one of the documents from our knowledge base let's use this one and we can see the prompt prompt um so this is the prompt and now we are going to send this to open AI of course and this is the code we use so this is the same code we used before so response um we use this CH completion thing um so let me just EX execute this part um and see what it returns okay so this is uh the result and now we can pass uh uh loads and pass the Json response and this is what we get right so now we just need to do this for all the documents we have and uh uh yeah let me actually check what we have can I enroll the course after it has already started if I'm if I join later can I still submit assignments am I allowed to register and submit homework after yeah so like you see there are different variations variations of questions um which is good this is what we want right and now we just need to do it for In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.\",\n",
       "  'id': '16ff7a602494e861e51477b721651770'},\n",
       " {'vid_id': 'bpxi6fKcyLw',\n",
       "  'title': 'LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation',\n",
       "  'timecode': '18:20',\n",
       "  'text': \"[Music] all for all the documents so will uh we already have a function which is copied from the other um notebook so we just need to do a for loop I think we have a for Loop somewhere here um yeah so I'll just use the same for Loop uh and then for in this for Loop I'll also use tqdm uh and wrap um and then um so what I'll have is uh results so I'll have a dictionary with results and uh so here doc ID is the doc ID now we generate um questions questions and these questions are in j right and then I want to put results uh put this in results questions right and then um when I Was preparing this what happened to me was that uh there was an error in Midway like maybe 30% of documents I uh processed and then I hit a um rate limit and then it stopped right and then I kind of needed to redo too this this is why I want to add this statement so if if doc ID is in results then we just continue so this acts as a cache in some way so then if we already computed the the results the the question the potential questions for for this we just Skip and in my case when it stopped Midway I added this statement and just continued and the other upside of this is when we have duplicates we don't need to do it multiple times and then I start uh if talk in result now I start results of course and it takes a while and actually I have done this already so let me stop it and um so I spent around um $4 on that with experiments so maybe if you do this you'll spend less actually let me quickly see what do we have in the results right so I did it for two documents and this is the results right um I already have the results and I want to open them I saved them using pickle with pickle with open um I think they are in results don't B uh we read open it as read binary as F in uh and then results is pickle what F in so this is I did before that's why like there is no not much sense for me to to redo the thing and pay $4 again and this is how the results look like so let me take it for this particular document um and this is not passed yet so now actually we need to pass it so\",\n",
       "  'timecode_text': 'Generating questions for all the documents',\n",
       "  'description': 'In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.',\n",
       "  'link': 'https://www.youtube.com/watch?v=bpxi6fKcyLw&t=1100s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation [Music] all for all the documents so will uh we already have a function which is copied from the other um notebook so we just need to do a for loop I think we have a for Loop somewhere here um yeah so I'll just use the same for Loop uh and then for in this for Loop I'll also use tqdm uh and wrap um and then um so what I'll have is uh results so I'll have a dictionary with results and uh so here doc ID is the doc ID now we generate um questions questions and these questions are in j right and then I want to put results uh put this in results questions right and then um when I Was preparing this what happened to me was that uh there was an error in Midway like maybe 30% of documents I uh processed and then I hit a um rate limit and then it stopped right and then I kind of needed to redo too this this is why I want to add this statement so if if doc ID is in results then we just continue so this acts as a cache in some way so then if we already computed the the results the the question the potential questions for for this we just Skip and in my case when it stopped Midway I added this statement and just continued and the other upside of this is when we have duplicates we don't need to do it multiple times and then I start uh if talk in result now I start results of course and it takes a while and actually I have done this already so let me stop it and um so I spent around um $4 on that with experiments so maybe if you do this you'll spend less actually let me quickly see what do we have in the results right so I did it for two documents and this is the results right um I already have the results and I want to open them I saved them using pickle with pickle with open um I think they are in results don't B uh we read open it as read binary as F in uh and then results is pickle what F in so this is I did before that's why like there is no not much sense for me to to redo the thing and pay $4 again and this is how the results look like so let me take it for this particular document um and this is not passed yet so now actually we need to pass it so In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.\",\n",
       "  'id': 'da2f570d2decde32eb904cb80730bc67'},\n",
       " {'vid_id': 'bpxi6fKcyLw',\n",
       "  'title': 'LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation',\n",
       "  'timecode': '22:12',\n",
       "  'text': \"I'll call it paret results will be a dictionary again and here for dock ID Jason um is questions in there is items so what I will do is I'll pass results uh Json what um s and Jason questions and um there is a problem sometimes it happens that even though you ask it to return adjon it does not so we can see what is the problem here um so the problem uh let me try to print it so because probably of these things uh it cannot really parse it so what I can do is just fix it manually uh to make sure it's parsible so then again like this uh uh think because of this I need to do something like that um yeah okay now it pares what I'll do is just Jon uh dumps this one and I have doc ID for this one so I'll go results dock ID this yeah I think now it should let's see I don't want to see this Tech Trace so I'll just do this uh okay now it passes everything so I can remove that I did I needed to do a little bit uh of um manual cleaning but now we have this uh\",\n",
       "  'timecode_text': 'Parsing the results',\n",
       "  'description': 'In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.',\n",
       "  'link': 'https://www.youtube.com/watch?v=bpxi6fKcyLw&t=1332s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation I'll call it paret results will be a dictionary again and here for dock ID Jason um is questions in there is items so what I will do is I'll pass results uh Json what um s and Jason questions and um there is a problem sometimes it happens that even though you ask it to return adjon it does not so we can see what is the problem here um so the problem uh let me try to print it so because probably of these things uh it cannot really parse it so what I can do is just fix it manually uh to make sure it's parsible so then again like this uh uh think because of this I need to do something like that um yeah okay now it pares what I'll do is just Jon uh dumps this one and I have doc ID for this one so I'll go results dock ID this yeah I think now it should let's see I don't want to see this Tech Trace so I'll just do this uh okay now it passes everything so I can remove that I did I needed to do a little bit uh of um manual cleaning but now we have this uh In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.\",\n",
       "  'id': '15e4586d2f239b0d3535976456159559'},\n",
       " {'vid_id': 'bpxi6fKcyLw',\n",
       "  'title': 'LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation',\n",
       "  'timecode': '24:30',\n",
       "  'text': \"questions and now what I want to do is save these questions as a CSV file so I will um so what I want to have in the CSV is uh question then which course this question uh belongs to and then ID of the relevant document so I already have two things out of three that I need in order to get the fourth I'll need to create an uh doc index where for each so I basically want to quickly look up based on the document ID the actual document so I'll do the ID d4d in documents so this document index looks like that now uh for each document ID we have the think right which will allow me to quickly uh pass it in the form I want and then uh I'll call it final results uh now I will iterate over this paret results uh doc ID uh questions in par results I think I have an item a typo there but whatever items and then um uh now I want to iterate for Q question for Q in questions and I want to append here um I want to append doc ID so let me first openend the question then course and then doc ID so the only thing I need now is course and course is I can use the index doc idop right so now this is how our final results look like like we just need to turn it into a cc file I'm using I'll use pandas for that SPD then PD data frame so this our data frame we just need to give uh columns um names quy or question question course and then uh document and of course I need a break it here so this is our data set this our ground proof data set we know question for which course and what is the relevant document and now we can use that to evaluate uh the quality of our um search systems this is what we are going to do next so we are going to compute uh some of these metrics here uh on uh elastic search on Min search and also on elastic search with vectors uh right now I'll save it to CSV call it um ground truth data CSV false so now we have um we have the data will push this data to our course repo and then later we can use this uh toate search results so see you soon\",\n",
       "  'timecode_text': 'Saving the results to CSV',\n",
       "  'description': 'In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.',\n",
       "  'link': 'https://www.youtube.com/watch?v=bpxi6fKcyLw&t=1470s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.2 - Ground Truth Dataset Generation for Retrieval Evaluation questions and now what I want to do is save these questions as a CSV file so I will um so what I want to have in the CSV is uh question then which course this question uh belongs to and then ID of the relevant document so I already have two things out of three that I need in order to get the fourth I'll need to create an uh doc index where for each so I basically want to quickly look up based on the document ID the actual document so I'll do the ID d4d in documents so this document index looks like that now uh for each document ID we have the think right which will allow me to quickly uh pass it in the form I want and then uh I'll call it final results uh now I will iterate over this paret results uh doc ID uh questions in par results I think I have an item a typo there but whatever items and then um uh now I want to iterate for Q question for Q in questions and I want to append here um I want to append doc ID so let me first openend the question then course and then doc ID so the only thing I need now is course and course is I can use the index doc idop right so now this is how our final results look like like we just need to turn it into a cc file I'm using I'll use pandas for that SPD then PD data frame so this our data frame we just need to give uh columns um names quy or question question course and then uh document and of course I need a break it here so this is our data set this our ground proof data set we know question for which course and what is the relevant document and now we can use that to evaluate uh the quality of our um search systems this is what we are going to do next so we are going to compute uh some of these metrics here uh on uh elastic search on Min search and also on elastic search with vectors uh right now I'll save it to CSV call it um ground truth data CSV false so now we have um we have the data will push this data to our course repo and then later we can use this uh toate search results so see you soon In this video, I explain the process of generating a dataset for evaluating our search system. We simplify the problem by linking each query to a relevant document, discussing the use of human annotators, and the challenges of data uniqueness.\",\n",
       "  'id': 'a66d1d46e92b22905932d6cdc26f5228'},\n",
       " {'vid_id': 'fdIV4xCsp0c',\n",
       "  'title': 'LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out',\n",
       "  'timecode': '00:40',\n",
       "  'text': \"in our data set in ground uh truth data set uh we are going to execute this query and check if uh check if this document um where it is if this document is actually returned if is in the results right and then based on that we can compute different pics in particular we will use two metrics today we will use the heat rate which tells us in general if we are able to retrieve the relevant document or not if it's among the top five in our case if it's among the the top five uh documents that are returned and also we will look at uh this mrr metric which not only tells us if uh uh we were able to retrieve in general this uh document but also how good uh the ranking is meaning that we want relevant documents to be at the top to have as high as trun as possible this metric will tell us if they're at the top or maybe they don't have the first rank so the higher the metric value is the better uh rank the better position the relevant document is in our results so we're going to evaluate these two metrics and I'll show you how and talk more about this metrix in details and right now I\",\n",
       "  'timecode_text': 'Plan',\n",
       "  'description': 'In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.',\n",
       "  'link': 'https://www.youtube.com/watch?v=fdIV4xCsp0c&t=40s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out in our data set in ground uh truth data set uh we are going to execute this query and check if uh check if this document um where it is if this document is actually returned if is in the results right and then based on that we can compute different pics in particular we will use two metrics today we will use the heat rate which tells us in general if we are able to retrieve the relevant document or not if it's among the top five in our case if it's among the the top five uh documents that are returned and also we will look at uh this mrr metric which not only tells us if uh uh we were able to retrieve in general this uh document but also how good uh the ranking is meaning that we want relevant documents to be at the top to have as high as trun as possible this metric will tell us if they're at the top or maybe they don't have the first rank so the higher the metric value is the better uh rank the better position the relevant document is in our results so we're going to evaluate these two metrics and I'll show you how and talk more about this metrix in details and right now I In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.\",\n",
       "  'id': '5b9c260761add5f39749609f7d3bc96e'},\n",
       " {'vid_id': 'fdIV4xCsp0c',\n",
       "  'title': 'LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out',\n",
       "  'timecode': '02:21',\n",
       "  'text': \"already uh loaded the document we created this document with uh Idis uh now for each document we also have this ID field um otherwise this document is the the same as what we used before and then I indexed these documents with elastic search and for that I also added an extra um fi here extra property ID uh I could have used uh the built-in ID mechanism from elastic search but this way is just simpler because like I don't need to redo much of the previous code we created uh and it's simpler right and uh I have indexed all these documents and we have the search query it's almost the same as we used before the only difference uh is that now we have the course field which is the the filter because we in addition to data engineering course we can also have mlops course or ml engineering course right and this is how we use it so it's the same as we did before right so right now what we're going to do is we're\",\n",
       "  'timecode_text': 'Elasticsearch: document loading and indexing',\n",
       "  'description': 'In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.',\n",
       "  'link': 'https://www.youtube.com/watch?v=fdIV4xCsp0c&t=141s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out already uh loaded the document we created this document with uh Idis uh now for each document we also have this ID field um otherwise this document is the the same as what we used before and then I indexed these documents with elastic search and for that I also added an extra um fi here extra property ID uh I could have used uh the built-in ID mechanism from elastic search but this way is just simpler because like I don't need to redo much of the previous code we created uh and it's simpler right and uh I have indexed all these documents and we have the search query it's almost the same as we used before the only difference uh is that now we have the course field which is the the filter because we in addition to data engineering course we can also have mlops course or ml engineering course right and this is how we use it so it's the same as we did before right so right now what we're going to do is we're In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.\",\n",
       "  'id': '810666fbbff9ceaa42710b047051c919'},\n",
       " {'vid_id': 'fdIV4xCsp0c',\n",
       "  'title': 'LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out',\n",
       "  'timecode': '03:30',\n",
       "  'text': \"going to iterate over all the queries in our ground through data and um execute uh invoke this function right so I'll start by uh actually reading the the data set that we have I think we called it ground through data set uh call DF ground truth right and this is how it looks like I'll just turn it to dictionary uh Records so let's call it uh ground right now we are going to iterate over this um queries truth and in this function search um query will be Q is it qu yeah um question and course would be Q course all right and then it will return some results and what we want to do is check if our document ID which is Q ID it's just ID is actually in the results so what we will do is we will iterate over the results over each of the records that is returned by elastic search and you will check if this is the document we're looking for uh results so um I'll call it relevance for the lack of uh better name and here uh D ID equals doc ID for D in results so D is document um so I'll just do it for one I'll add this uh break statement and we will see it's called we will see how this relevance look like so this this is what we get so this is the results so we get some results uh back from elastic search and then it turns out that the first result is the relevant one so this is the record we were uh hoping to to retrieve and it was the first one so this is great right so now we just want to do it for all the documents and let me call it [Music] um um I don't know relevance um I don't like this name um query performance uh query results okay let me call relevance and relevance tot cuz now what I want to do is uh put like all these things together uhm just to track the progress and now we need to execute it you can see that elastic search is quite fast so we can retrieve uh like do like 40 iterations per second I'll put it on pause and come back when it finishes okay yeah it finished so it was doing uh almost one 100 iterations per second actually like when I was doing experiments before it was faster but okay um it's still quite quite good results right so we can see that it took a minute and this relevance total this is how it looks like right so for each document in our gold standard data set we see if the relevant document was retrieved and not only that we can also see at which position it was retrieved right so here it was retrieved here it was was not was not was not um now we can talk about metrics and actually I'll just take this thing as\",\n",
       "  'timecode_text': 'Executing all the queries with Elastic',\n",
       "  'description': 'In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.',\n",
       "  'link': 'https://www.youtube.com/watch?v=fdIV4xCsp0c&t=210s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out going to iterate over all the queries in our ground through data and um execute uh invoke this function right so I'll start by uh actually reading the the data set that we have I think we called it ground through data set uh call DF ground truth right and this is how it looks like I'll just turn it to dictionary uh Records so let's call it uh ground right now we are going to iterate over this um queries truth and in this function search um query will be Q is it qu yeah um question and course would be Q course all right and then it will return some results and what we want to do is check if our document ID which is Q ID it's just ID is actually in the results so what we will do is we will iterate over the results over each of the records that is returned by elastic search and you will check if this is the document we're looking for uh results so um I'll call it relevance for the lack of uh better name and here uh D ID equals doc ID for D in results so D is document um so I'll just do it for one I'll add this uh break statement and we will see it's called we will see how this relevance look like so this this is what we get so this is the results so we get some results uh back from elastic search and then it turns out that the first result is the relevant one so this is the record we were uh hoping to to retrieve and it was the first one so this is great right so now we just want to do it for all the documents and let me call it [Music] um um I don't know relevance um I don't like this name um query performance uh query results okay let me call relevance and relevance tot cuz now what I want to do is uh put like all these things together uhm just to track the progress and now we need to execute it you can see that elastic search is quite fast so we can retrieve uh like do like 40 iterations per second I'll put it on pause and come back when it finishes okay yeah it finished so it was doing uh almost one 100 iterations per second actually like when I was doing experiments before it was faster but okay um it's still quite quite good results right so we can see that it took a minute and this relevance total this is how it looks like right so for each document in our gold standard data set we see if the relevant document was retrieved and not only that we can also see at which position it was retrieved right so here it was retrieved here it was was not was not was not um now we can talk about metrics and actually I'll just take this thing as In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.\",\n",
       "  'id': 'd3bd433df2e6d96c478177483e1ad908'},\n",
       " {'vid_id': 'fdIV4xCsp0c',\n",
       "  'title': 'LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out',\n",
       "  'timecode': '08:10',\n",
       "  'text': \"example example um so we want to talk about two metrics right so we want to let format a little bit we want to talk about um two metrics first is a heat aray also called recall and the second one will be mrr which is mean reciprocal rank I always always forget Mr um so the reason heat rate and recall are the same if you know what recall is is because we always have only one document per uh result set like there's only one relevant document in our in our case right so heat rate looks at uh we\",\n",
       "  'timecode_text': 'Evaluation metrics',\n",
       "  'description': 'In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.',\n",
       "  'link': 'https://www.youtube.com/watch?v=fdIV4xCsp0c&t=490s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out example example um so we want to talk about two metrics right so we want to let format a little bit we want to talk about um two metrics first is a heat aray also called recall and the second one will be mrr which is mean reciprocal rank I always always forget Mr um so the reason heat rate and recall are the same if you know what recall is is because we always have only one document per uh result set like there's only one relevant document in our in our case right so heat rate looks at uh we In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.\",\n",
       "  'id': '92e46681300c699e2396d3203591cb6b'},\n",
       " {'vid_id': 'fdIV4xCsp0c',\n",
       "  'title': 'LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out',\n",
       "  'timecode': '09:03',\n",
       "  'text': \"look at each uh each row and if there is at least one true then heat rate is one and recall would be like calculating how many true uh results are in general there so this one is one this one is zero this one is zero uh this one is zero this one is zero this one is one one one one one one and zero right so um let me look at the length of um example it's 12 and we have 1 2 3 5 6 7 8 um so8 out 12 in our case heat rate is 50% so in % of cases we were able to retrieve to find the relevant document in 50% we were not able to so our heat rate is 50% so this would be our um let's call it heat rate heat rate um so let's call it a I don't like this result whatever um so now we just iterate over line in relance total uh then we have counter and for uh let's call let's do it if true in uh line counter is incremented and then at the end we return counter divided by the length of the I think so this should give us the heat rate so we check if three is there like we did here and if it's there we increment the counter and then at the end we just divide which should give us the the percentage of cases where uh it's correct so let's check it with the example um so maybe one 2 3 four five six seven yeah that's why uh it's not the same okay now it's correct so this is the heat rate the other thing we are interested in is mrr so mrr\",\n",
       "  'timecode_text': 'Hit-rate',\n",
       "  'description': 'In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.',\n",
       "  'link': 'https://www.youtube.com/watch?v=fdIV4xCsp0c&t=543s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out look at each uh each row and if there is at least one true then heat rate is one and recall would be like calculating how many true uh results are in general there so this one is one this one is zero this one is zero uh this one is zero this one is zero this one is one one one one one one and zero right so um let me look at the length of um example it's 12 and we have 1 2 3 5 6 7 8 um so8 out 12 in our case heat rate is 50% so in % of cases we were able to retrieve to find the relevant document in 50% we were not able to so our heat rate is 50% so this would be our um let's call it heat rate heat rate um so let's call it a I don't like this result whatever um so now we just iterate over line in relance total uh then we have counter and for uh let's call let's do it if true in uh line counter is incremented and then at the end we return counter divided by the length of the I think so this should give us the heat rate so we check if three is there like we did here and if it's there we increment the counter and then at the end we just divide which should give us the the percentage of cases where uh it's correct so let's check it with the example um so maybe one 2 3 four five six seven yeah that's why uh it's not the same okay now it's correct so this is the heat rate the other thing we are interested in is mrr so mrr In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.\",\n",
       "  'id': 'c082e3f56bc2d5927d904e1c422a4fc2'},\n",
       " {'vid_id': 'fdIV4xCsp0c',\n",
       "  'title': 'LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out',\n",
       "  'timecode': '11:33',\n",
       "  'text': \"doesn't just look at whether the result is there or not it also looks at the position so if this is at the first position then we give uh just add a comma here then we instead of adding one so okay it's similar to heat rate but instead of adding always one we add one or less depending on at which point uh true is right so in this case when it's the first position first position we add one when it's second position we add uh 1 / 2 which is 0.5 when it's the third position we add 1 / 3 so we divide by the position number by rank which is like 1/3 uh and so on so rank it's one ided by rank right so if it's four um then yeah it would be 0.25 and five would be 0.2 right so in our case we can only have five uh and of course if none if uh we cannot retrieve uh then it's zero right so in our case uh it's one one one one one one and here it's uh 1/3 right so it doesn't in this case change it significantly uh except instead of a in one in one of the cases we would add 13 right so here we are interested in the position so now we would need to do another loop for rank in um Range uh length line and um let's see if no it's not wrong yeah if line I is true then counter we increment counter not by one but by uh rank CT plus uh rank + one and we divide to something like this because it starts with zero right and so if the first one uh is true then it's one divided by one if second one is true it's one divided by two and so on so this is what we do here uh so maybe the correct way would be not calling it um run uh counter but uh score so total score and then mean score total score and then at the end it would be mean score right okay so in our case m r r should be similar slightly less than uh I it's wrong yeah just slightly less than uh heat rate because only in one case it's instead of one we add 1/3 okay so these are the two matrics and now we calculate these two matrics for our Elance\",\n",
       "  'timecode_text': 'MRR',\n",
       "  'description': 'In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.',\n",
       "  'link': 'https://www.youtube.com/watch?v=fdIV4xCsp0c&t=693s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out doesn't just look at whether the result is there or not it also looks at the position so if this is at the first position then we give uh just add a comma here then we instead of adding one so okay it's similar to heat rate but instead of adding always one we add one or less depending on at which point uh true is right so in this case when it's the first position first position we add one when it's second position we add uh 1 / 2 which is 0.5 when it's the third position we add 1 / 3 so we divide by the position number by rank which is like 1/3 uh and so on so rank it's one ided by rank right so if it's four um then yeah it would be 0.25 and five would be 0.2 right so in our case we can only have five uh and of course if none if uh we cannot retrieve uh then it's zero right so in our case uh it's one one one one one one and here it's uh 1/3 right so it doesn't in this case change it significantly uh except instead of a in one in one of the cases we would add 13 right so here we are interested in the position so now we would need to do another loop for rank in um Range uh length line and um let's see if no it's not wrong yeah if line I is true then counter we increment counter not by one but by uh rank CT plus uh rank + one and we divide to something like this because it starts with zero right and so if the first one uh is true then it's one divided by one if second one is true it's one divided by two and so on so this is what we do here uh so maybe the correct way would be not calling it um run uh counter but uh score so total score and then mean score total score and then at the end it would be mean score right okay so in our case m r r should be similar slightly less than uh I it's wrong yeah just slightly less than uh heat rate because only in one case it's instead of one we add 1/3 okay so these are the two matrics and now we calculate these two matrics for our Elance In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.\",\n",
       "  'id': '83354fca9cf2d99e5f8624263b4c9829'},\n",
       " {'vid_id': 'fdIV4xCsp0c',\n",
       "  'title': 'LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out',\n",
       "  'timecode': '15:30',\n",
       "  'text': \"total uh let's see what they are heat rate and Mr and these are the results so in 73 74% we are actually able to retrieve the relevant um record and then 25% 26 we do not and it's not always the first position so we hope that uh the higher the position the better and and this mrr tells us um like how good our ranking is and now we can do the same thing but\",\n",
       "  'timecode_text': 'Hit-rate and MRR for elastic',\n",
       "  'description': 'In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.',\n",
       "  'link': 'https://www.youtube.com/watch?v=fdIV4xCsp0c&t=930s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out total uh let's see what they are heat rate and Mr and these are the results so in 73 74% we are actually able to retrieve the relevant um record and then 25% 26 we do not and it's not always the first position so we hope that uh the higher the position the better and and this mrr tells us um like how good our ranking is and now we can do the same thing but In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.\",\n",
       "  'id': 'ff02c2c1a4f7959ea5f52cdd9c5f5d45'},\n",
       " {'vid_id': 'fdIV4xCsp0c',\n",
       "  'title': 'LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out',\n",
       "  'timecode': '16:07',\n",
       "  'text': \"for um our mean search and here I will just download it this way um and also um so this is the code I prepared before and it's yeah it's the same Library we used in module one except here we add one keyword field like we did with elastic uh it's uh ID and we do search um I think we use um I'll just copy the code from here um right so this is the same search uh method we used before um so I think here uh I have question question and course quy would be question and course would be course right so here I will do the same thing but now instead of using uh elastic search I use me search search uh let's see um question query yeah let me call it query too so query is a search query we send to search engine and question is a question from the student um somehow I tend to use these terms here interchangeably um but yeah so the reason I call the question not query is because this is a question from a student so uh yeah it's almost as fast as elastic search I think it's slightly faster uh in my previous experiments uh I think it was a bit slower I'm kind of giving away already the um the metrics but let's see so remember that for elastic search it was U I'll take this line uh this is was the data for elastic search and this is the data for we will see now for in Search and let me just copy it uh yeah it seems like me search is slightly better compare with dat or is results right so uh in our case heat rate is higher and then relevance score is also higher which is um I guess it's cool but like it's not that better right so it's just uh I don't know 4% better or what like it's not like a super significant Improvement actually um like 3% yeah so it's like I wouldn't say it's it's a lot I think many queries are actually tough ones there and um right now I want to do a\",\n",
       "  'timecode_text': 'Evaluating minsearch',\n",
       "  'description': 'In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.',\n",
       "  'link': 'https://www.youtube.com/watch?v=fdIV4xCsp0c&t=967s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out for um our mean search and here I will just download it this way um and also um so this is the code I prepared before and it's yeah it's the same Library we used in module one except here we add one keyword field like we did with elastic uh it's uh ID and we do search um I think we use um I'll just copy the code from here um right so this is the same search uh method we used before um so I think here uh I have question question and course quy would be question and course would be course right so here I will do the same thing but now instead of using uh elastic search I use me search search uh let's see um question query yeah let me call it query too so query is a search query we send to search engine and question is a question from the student um somehow I tend to use these terms here interchangeably um but yeah so the reason I call the question not query is because this is a question from a student so uh yeah it's almost as fast as elastic search I think it's slightly faster uh in my previous experiments uh I think it was a bit slower I'm kind of giving away already the um the metrics but let's see so remember that for elastic search it was U I'll take this line uh this is was the data for elastic search and this is the data for we will see now for in Search and let me just copy it uh yeah it seems like me search is slightly better compare with dat or is results right so uh in our case heat rate is higher and then relevance score is also higher which is um I guess it's cool but like it's not that better right so it's just uh I don't know 4% better or what like it's not like a super significant Improvement actually um like 3% yeah so it's like I wouldn't say it's it's a lot I think many queries are actually tough ones there and um right now I want to do a In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.\",\n",
       "  'id': '6412d6e2d532fce4ec11d144d4779354'},\n",
       " {'vid_id': 'fdIV4xCsp0c',\n",
       "  'title': 'LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out',\n",
       "  'timecode': '19:33',\n",
       "  'text': \"little bit of cleaning and make it more generic so we have code that is almost uh is that is super similar the only difference is that uh we have uh this function right uh here this function and where [Music] yeah here right so this is the only difference and I want to uh make a function we can call it evalate uh and then uh what do we have here uh ground truth so this will be one of parameters and search function right and then we would execute the search function here um we can maybe make it even more generic um we can say uh that search function gets the Q the record from here um and then at the end it returns uh heat rate which would be be H rate I'll just copy it and then mrr which is the Mr and now let us aate uh with ground truth um our search function would be um first elastic search Q [Music] uh question right or I don't remember question and then Q course I not callose something yeah so this is elastic search and then uh it's funny okay and now mean search so now it finishes and then we'll see me search I'll put it on pause we know there like there is no secret what will happen so I'll just put it on pause and then we'll see when it finishes okay it finished so now we can see the results and now we can also tweak uh different parameters so for example we can uh give different boosts so let's see what happens if instead of uh three we give uh a boost of let's say five and we don't include the setion so we can do that and we would just need to uh execute it one more time um of course like it's not the the best uh way that I just did ideally we would pass a parameter to a function something we want to experiment with so then we have clear history of what was changed and even better we would use something like um ml flow for tracking these experiments um because with ML flow it's pretty easy to track and this is something we talk in uh our mlops course uh in this experiment tracking model so if you're interested in learning about these tools like mfo you can check it here we use simpler approach just jupter notbook and we can see that uh excluding section and giving a higher boost to question does not work um so it does not improve the situation so we actually now have a very good way of measuring this and instead of blindly trying different uh combinations and then seeing what works or trying to have the gut feeling of okay work does not we actually can rely on a number that tells us if it's becoming better or not um this ground roof data set maybe is not ideal of course we can take a look at this see which things does not make sense maybe we can see like what are this 25% that our search engine missed like do this query make sense do they not um so our ground prooof data set might require further cleaning because it was generated by an llm so maybe it makes sense to involve a human data to actually take a look at this but it's a very good start and it's a very good way of understanding if what we do by tweaking our uh retrieval part is actually improving the situation or making Force so what I want to do next is in the next video I want to repeat the same thing but for Vector search and that would be it for the evaluation part at least for now okay so see you around\",\n",
       "  'timecode_text': 'Cleaning',\n",
       "  'description': 'In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.',\n",
       "  'link': 'https://www.youtube.com/watch?v=fdIV4xCsp0c&t=1173s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.3 - Evaluation of Text Retrieval Techniques for RAG out little bit of cleaning and make it more generic so we have code that is almost uh is that is super similar the only difference is that uh we have uh this function right uh here this function and where [Music] yeah here right so this is the only difference and I want to uh make a function we can call it evalate uh and then uh what do we have here uh ground truth so this will be one of parameters and search function right and then we would execute the search function here um we can maybe make it even more generic um we can say uh that search function gets the Q the record from here um and then at the end it returns uh heat rate which would be be H rate I'll just copy it and then mrr which is the Mr and now let us aate uh with ground truth um our search function would be um first elastic search Q [Music] uh question right or I don't remember question and then Q course I not callose something yeah so this is elastic search and then uh it's funny okay and now mean search so now it finishes and then we'll see me search I'll put it on pause we know there like there is no secret what will happen so I'll just put it on pause and then we'll see when it finishes okay it finished so now we can see the results and now we can also tweak uh different parameters so for example we can uh give different boosts so let's see what happens if instead of uh three we give uh a boost of let's say five and we don't include the setion so we can do that and we would just need to uh execute it one more time um of course like it's not the the best uh way that I just did ideally we would pass a parameter to a function something we want to experiment with so then we have clear history of what was changed and even better we would use something like um ml flow for tracking these experiments um because with ML flow it's pretty easy to track and this is something we talk in uh our mlops course uh in this experiment tracking model so if you're interested in learning about these tools like mfo you can check it here we use simpler approach just jupter notbook and we can see that uh excluding section and giving a higher boost to question does not work um so it does not improve the situation so we actually now have a very good way of measuring this and instead of blindly trying different uh combinations and then seeing what works or trying to have the gut feeling of okay work does not we actually can rely on a number that tells us if it's becoming better or not um this ground roof data set maybe is not ideal of course we can take a look at this see which things does not make sense maybe we can see like what are this 25% that our search engine missed like do this query make sense do they not um so our ground prooof data set might require further cleaning because it was generated by an llm so maybe it makes sense to involve a human data to actually take a look at this but it's a very good start and it's a very good way of understanding if what we do by tweaking our uh retrieval part is actually improving the situation or making Force so what I want to do next is in the next video I want to repeat the same thing but for Vector search and that would be it for the evaluation part at least for now okay so see you around In this video, we utilize data from previous sessions to evaluate document relevance using metrics like hit rate and Mean Reciprocal Rank. We use generate questions to assess document retrieval, and compare different text retrieval methods.\",\n",
       "  'id': 'b641a4ac197e70d91cbd34c7207c6890'},\n",
       " {'vid_id': 'VRprIm9-VV8',\n",
       "  'title': 'LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hi everyone in this series of videos we are discussing evaluating our retrieval methods we have already evaluated uh teex search and there we took a look at evaluating the results from elastic search and Min search in both cases we used only Tex search and we use uh two uh metrics we used heat rate and we use mrr now what we are going to do is too the same thing but uh with Vector search we'll use elastic search like previously and we will try to uh three different approaches to ranking um we will use embeddings for questions only we will use embeddings for answers only and we will also combine question and answer and create embeddings for that so we'll evaluate three different methods to see which one works best for for us and which should we use so that's the plan I have already what I did is I copied the document we created\",\n",
       "  'timecode_text': 'Plan',\n",
       "  'description': 'In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.',\n",
       "  'link': 'https://www.youtube.com/watch?v=VRprIm9-VV8&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval hi everyone in this series of videos we are discussing evaluating our retrieval methods we have already evaluated uh teex search and there we took a look at evaluating the results from elastic search and Min search in both cases we used only Tex search and we use uh two uh metrics we used heat rate and we use mrr now what we are going to do is too the same thing but uh with Vector search we'll use elastic search like previously and we will try to uh three different approaches to ranking um we will use embeddings for questions only we will use embeddings for answers only and we will also combine question and answer and create embeddings for that so we'll evaluate three different methods to see which one works best for for us and which should we use so that's the plan I have already what I did is I copied the document we created In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.\",\n",
       "  'id': '1ad766ce3761a1090652841642e9734a'},\n",
       " {'vid_id': 'VRprIm9-VV8',\n",
       "  'title': 'LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval',\n",
       "  'timecode': '01:02',\n",
       "  'text': 'previously in the previous video when we evaluated text uh results so this is what we have created previously and um yeah so what we will do now um is uh adjust this slightly to also include things we talked about in the vector search module in the vector search videos so here I have the notebook we used and what we need to do is use this sentence Transformer I already um installed library and uh here this is the um page where we can see what kind of um',\n",
       "  'timecode_text': 'Adding vector search',\n",
       "  'description': 'In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.',\n",
       "  'link': 'https://www.youtube.com/watch?v=VRprIm9-VV8&t=62s',\n",
       "  'text_vector': 'LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval previously in the previous video when we evaluated text uh results so this is what we have created previously and um yeah so what we will do now um is uh adjust this slightly to also include things we talked about in the vector search module in the vector search videos so here I have the notebook we used and what we need to do is use this sentence Transformer I already um installed library and uh here this is the um page where we can see what kind of um In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.',\n",
       "  'id': 'f65bf0589502bb8ecdbead1446b526d9'},\n",
       " {'vid_id': 'VRprIm9-VV8',\n",
       "  'title': 'LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval',\n",
       "  'timecode': '01:45',\n",
       "  'text': \"pre-rain models are available and we have this um list with models here where we have performance uh SE performance on sentence betting performance on semantic search average performance speed and model so I want the model with the best performance search uh but also I want to have a rather small model I don't want to wait forever uh which is why I will use this one multi Q&A mini LM L 6 cos V1 data set and here we can actually see that it has this cost uh in the name I think you scroll down uh they say that the difference between cos and Dot models is here we use cosine they are already the embeddings that it uh returns are already normalized um um which is like maybe a bit technical but if you know what is the difference between cosine similarity and Dot product so here this one uses cosine similarity this one uses dot product if you don't know what this is um use chpd it will tell you um that okay but what we are going to do is use this one right um this will be the model name and here this is the code we use for loading the model so let me just put them in one uh okay so we have loaded the model we can quickly test it um model and code and we can encode something like I just discovered discovered the course can I still join right so this will be our vector and the dimensionality of this Vector is uh 3 84 right so it's two times smaller I think than the one uh the embedding model we used in other videos for semic search uh but this is also just a bunch of uh numbers so this is our vector and by the way here we can compute the similarity which is Vector product uh on the dot product in this case CU it's already normalized and yeah so when we compute the similarity between the vector and Vector we get the highest possible similarity which is one okay and and by the way this is what plastic search will actually compute to and right now we will we want to create three embeddings right so we want\",\n",
       "  'timecode_text': 'Selecting the embedding model',\n",
       "  'description': 'In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.',\n",
       "  'link': 'https://www.youtube.com/watch?v=VRprIm9-VV8&t=105s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval pre-rain models are available and we have this um list with models here where we have performance uh SE performance on sentence betting performance on semantic search average performance speed and model so I want the model with the best performance search uh but also I want to have a rather small model I don't want to wait forever uh which is why I will use this one multi Q&A mini LM L 6 cos V1 data set and here we can actually see that it has this cost uh in the name I think you scroll down uh they say that the difference between cos and Dot models is here we use cosine they are already the embeddings that it uh returns are already normalized um um which is like maybe a bit technical but if you know what is the difference between cosine similarity and Dot product so here this one uses cosine similarity this one uses dot product if you don't know what this is um use chpd it will tell you um that okay but what we are going to do is use this one right um this will be the model name and here this is the code we use for loading the model so let me just put them in one uh okay so we have loaded the model we can quickly test it um model and code and we can encode something like I just discovered discovered the course can I still join right so this will be our vector and the dimensionality of this Vector is uh 3 84 right so it's two times smaller I think than the one uh the embedding model we used in other videos for semic search uh but this is also just a bunch of uh numbers so this is our vector and by the way here we can compute the similarity which is Vector product uh on the dot product in this case CU it's already normalized and yeah so when we compute the similarity between the vector and Vector we get the highest possible similarity which is one okay and and by the way this is what plastic search will actually compute to and right now we will we want to create three embeddings right so we want In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.\",\n",
       "  'id': '477582a357a3cf2401560f11abff2991'},\n",
       " {'vid_id': 'VRprIm9-VV8',\n",
       "  'title': 'LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval',\n",
       "  'timecode': '04:46',\n",
       "  'text': \"to create an embedding for each question want to create embeddings for answers and want to create embeddings for the combination of question and answer and this is the syntax we use when we create uh mappings so I'll have uh question Vector uh type so this will be the same right except we need to change the dimensionality cuz um yeah it's two times smaller how much was it how many uh yeah this one question Vector then we will also have uh answer vector or text Vector as we call text text and let's call it question text Vector right so we'll have three uh three vectors and we create an index and now we will need to Loop over our documents and for each document um compute these things right um so let's do that so here I will use tqdm already here or document and documents so here we want to encode uh so first is question right and this is this will be Doc question then we have text this will be Doc text and then uh question text right I'll just call it QT which is question plus text right and now for all of the them we will need to create embeddings v i I'll just short Vector question Vector text Vector Q text right and then uh we use uh this thing model in code question uh model import uh text and mod called qtt and now we need to put it back to the document right uh maybe I can just instead of using this uh the cryptic names I'll just use uh question Vector here um then and we have text Vector right text vector and the last one is question text vector and so Le let's see how long so we will not index it immediately I just want to First compute the the embeddings and then index uh but actually we might as well just index it um because imagine there are scenarios where um we have a lot of documents and we don't want to actually keep all these vectors in memory we want to uh put them to the database as fast as possible so then um we don't have like large memory footprint uh for now I want to uh split this into two parts to Simply see how long each part takes so that's why we'll first do the embedding we'll create the embeddings and then in the second Loop which I'll write right now uh we will index these documents okay so let me execute that yeah it's somewhat slow of course like these are neural networks and I don't have a GPU uh well I have a small GPU on my laptop but it's not configured so it uses a CPU that's why I'm putting this on pause and come back when it finishes okay finished I am wondering how long it would take for this model uh CU this one is the the best performing one in terms of uh semantic search um but also yeah it's how many times like four five times um slower but yeah anyways now we can index it so yeah indexing is faster I'll still put on on pause uh to save some time okay so as we saw and actually the reason I wanted to split it into two parts to see which part is lower so in our case Computing the embeding is the slowest slowest part um compared to index in the documents okay we don't need this anymore and now we need to uh construct\",\n",
       "  'timecode_text': 'Indexing the embeddings',\n",
       "  'description': 'In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.',\n",
       "  'link': 'https://www.youtube.com/watch?v=VRprIm9-VV8&t=286s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval to create an embedding for each question want to create embeddings for answers and want to create embeddings for the combination of question and answer and this is the syntax we use when we create uh mappings so I'll have uh question Vector uh type so this will be the same right except we need to change the dimensionality cuz um yeah it's two times smaller how much was it how many uh yeah this one question Vector then we will also have uh answer vector or text Vector as we call text text and let's call it question text Vector right so we'll have three uh three vectors and we create an index and now we will need to Loop over our documents and for each document um compute these things right um so let's do that so here I will use tqdm already here or document and documents so here we want to encode uh so first is question right and this is this will be Doc question then we have text this will be Doc text and then uh question text right I'll just call it QT which is question plus text right and now for all of the them we will need to create embeddings v i I'll just short Vector question Vector text Vector Q text right and then uh we use uh this thing model in code question uh model import uh text and mod called qtt and now we need to put it back to the document right uh maybe I can just instead of using this uh the cryptic names I'll just use uh question Vector here um then and we have text Vector right text vector and the last one is question text vector and so Le let's see how long so we will not index it immediately I just want to First compute the the embeddings and then index uh but actually we might as well just index it um because imagine there are scenarios where um we have a lot of documents and we don't want to actually keep all these vectors in memory we want to uh put them to the database as fast as possible so then um we don't have like large memory footprint uh for now I want to uh split this into two parts to Simply see how long each part takes so that's why we'll first do the embedding we'll create the embeddings and then in the second Loop which I'll write right now uh we will index these documents okay so let me execute that yeah it's somewhat slow of course like these are neural networks and I don't have a GPU uh well I have a small GPU on my laptop but it's not configured so it uses a CPU that's why I'm putting this on pause and come back when it finishes okay finished I am wondering how long it would take for this model uh CU this one is the the best performing one in terms of uh semantic search um but also yeah it's how many times like four five times um slower but yeah anyways now we can index it so yeah indexing is faster I'll still put on on pause uh to save some time okay so as we saw and actually the reason I wanted to split it into two parts to see which part is lower so in our case Computing the embeding is the slowest slowest part um compared to index in the documents okay we don't need this anymore and now we need to uh construct In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.\",\n",
       "  'id': '9bc9513b9324aa2d85fae389597127e2'},\n",
       " {'vid_id': 'VRprIm9-VV8',\n",
       "  'title': 'LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval',\n",
       "  'timecode': '09:50',\n",
       "  'text': \"a search query I'll again just use this thing um yeah that's our alol search query and um yeah so this is what we this is what we use or actually I wanted to call it search query not elastic search um yeah so field is uh yeah let's use uh question Vector qu Vector is um so first of all our query will be query will be um I just uh discovered the course can I still join it right and um we of course need to compute um the embeddings uh quy embedding or I'll call it uh VQ short model and code uh query right and this is what we U what we use here uh and I'll call it elastic search results not to confuse it with anything else um and yeah I'll leave this one and also add ID because we want to know what is the ID uh for for for for each document so let's see so this is the result wait where it is by the request okay what's happening I know Q or Val string okay I'll put it on post to figure this out of course like query search query let me try one more time good this this time it works uh so let's see what we have and yeah I think here we have this um um where it is this is where we pass right so this is the the results um so these are all the documents and we see that uh yeah we have questions from machine learning Zoom Camp from data engineering Zoom Camp uh yeah we want to actually\",\n",
       "  'timecode_text': 'Creating the search query',\n",
       "  'description': 'In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.',\n",
       "  'link': 'https://www.youtube.com/watch?v=VRprIm9-VV8&t=590s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval a search query I'll again just use this thing um yeah that's our alol search query and um yeah so this is what we this is what we use or actually I wanted to call it search query not elastic search um yeah so field is uh yeah let's use uh question Vector qu Vector is um so first of all our query will be query will be um I just uh discovered the course can I still join it right and um we of course need to compute um the embeddings uh quy embedding or I'll call it uh VQ short model and code uh query right and this is what we U what we use here uh and I'll call it elastic search results not to confuse it with anything else um and yeah I'll leave this one and also add ID because we want to know what is the ID uh for for for for each document so let's see so this is the result wait where it is by the request okay what's happening I know Q or Val string okay I'll put it on post to figure this out of course like query search query let me try one more time good this this time it works uh so let's see what we have and yeah I think here we have this um um where it is this is where we pass right so this is the the results um so these are all the documents and we see that uh yeah we have questions from machine learning Zoom Camp from data engineering Zoom Camp uh yeah we want to actually In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.\",\n",
       "  'id': '9dba8b507b5b58a7adf9a5af2e5e5d19'},\n",
       " {'vid_id': 'VRprIm9-VV8',\n",
       "  'title': 'LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval',\n",
       "  'timecode': '12:40',\n",
       "  'text': \"restrict ourselves to just one uh course like previously and for that we will need to modify our search query I to be honest don't remember the synx so what I'll do is I'll just go to chbt and then say I want to add filter here like here um um this is something we did in first model right so I think uh what we will have at the end is a search query very similar to this one so I want to add a filter statement and then instead of bull I think we'll have k Ann there so I want to add filter here in my uh plastic search query like here yeah let's see what it response with so this is the search query okay that's good so I think it made it a little bit more complicated than uh when I want it uh then I want it but let's see uh so our search query will be query like that yeah or complicated uh yeah so by default elastic search queries to me are complicated already and then yeah we have this Source I think which is outside of K KNN I think right so query then inside query we have Cann statement this is our uh query Vector uh number of candidates filter source and this our body uh let's see if this thing works I hope it does um did I make uh something so let me see if I somehow screwed it up search qu Canon filter I'll just put it to elastic search and uh I'll just put it to char PT and let it figure out so let's see what it responds with so I'll just simply copy this I don't know where the problem could be let's see no yeah it worked and now I'll remove this one and see now the filtering is applied cool so we have this thing now\",\n",
       "  'timecode_text': 'Filtering the results',\n",
       "  'description': 'In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.',\n",
       "  'link': 'https://www.youtube.com/watch?v=VRprIm9-VV8&t=760s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval restrict ourselves to just one uh course like previously and for that we will need to modify our search query I to be honest don't remember the synx so what I'll do is I'll just go to chbt and then say I want to add filter here like here um um this is something we did in first model right so I think uh what we will have at the end is a search query very similar to this one so I want to add a filter statement and then instead of bull I think we'll have k Ann there so I want to add filter here in my uh plastic search query like here yeah let's see what it response with so this is the search query okay that's good so I think it made it a little bit more complicated than uh when I want it uh then I want it but let's see uh so our search query will be query like that yeah or complicated uh yeah so by default elastic search queries to me are complicated already and then yeah we have this Source I think which is outside of K KNN I think right so query then inside query we have Cann statement this is our uh query Vector uh number of candidates filter source and this our body uh let's see if this thing works I hope it does um did I make uh something so let me see if I somehow screwed it up search qu Canon filter I'll just put it to elastic search and uh I'll just put it to char PT and let it figure out so let's see what it responds with so I'll just simply copy this I don't know where the problem could be let's see no yeah it worked and now I'll remove this one and see now the filtering is applied cool so we have this thing now In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.\",\n",
       "  'id': 'd1f3659ee273a909fbb03c9ddad6d93a'},\n",
       " {'vid_id': 'VRprIm9-VV8',\n",
       "  'title': 'LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval',\n",
       "  'timecode': '16:05',\n",
       "  'text': \"what we will do is we will put it again to our um an elastic search um and we probably want to make it um generic such that we can specify uh what kind kind of um what kind of embeddings we want to use right so what we can do is I'll call elastic search K NN so then query could be um query could be um a vector so let's have uh our KN andn uh here okay and and here so we don't specify query Vector we go for uh Q uh Q will be query V vector or value in query items so this will be a dictionary with vectors we will simply add this to our Canon and then um so what I want to do here is um actually know like what I wanted to do here is uh we want to know which Vector to choose actually I don't think I do it the right way so I think we need to have here field and Vector right oh yeah this is better uh field will be this one and V will be this one right okay uh search query Sol document return uh result documents so let us test it so I will test it again with the disector that we already have elastic search Canon uh field is um which one question Vector right question Vector then um this isq and course is data engineer zom Camp of course I need to put it put the course here okay so this one should return some results and it does so that's pretty\",\n",
       "  'timecode_text': 'Creating the function for search',\n",
       "  'description': 'In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.',\n",
       "  'link': 'https://www.youtube.com/watch?v=VRprIm9-VV8&t=965s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval what we will do is we will put it again to our um an elastic search um and we probably want to make it um generic such that we can specify uh what kind kind of um what kind of embeddings we want to use right so what we can do is I'll call elastic search K NN so then query could be um query could be um a vector so let's have uh our KN andn uh here okay and and here so we don't specify query Vector we go for uh Q uh Q will be query V vector or value in query items so this will be a dictionary with vectors we will simply add this to our Canon and then um so what I want to do here is um actually know like what I wanted to do here is uh we want to know which Vector to choose actually I don't think I do it the right way so I think we need to have here field and Vector right oh yeah this is better uh field will be this one and V will be this one right okay uh search query Sol document return uh result documents so let us test it so I will test it again with the disector that we already have elastic search Canon uh field is um which one question Vector right question Vector then um this isq and course is data engineer zom Camp of course I need to put it put the course here okay so this one should return some results and it does so that's pretty In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.\",\n",
       "  'id': 'ae89b69da3de0741c9e895a3a2ada802'},\n",
       " {'vid_id': 'VRprIm9-VV8',\n",
       "  'title': 'LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval',\n",
       "  'timecode': '19:00',\n",
       "  'text': \"cool so now we can use this function to evaluate our um um the this uh this way of retrieving this so I'll put it in a function um that we will use in our so remember so I'll delete this one this one um so here this stuff we don't need so this was an example and here we used um this evalate right so in evate we have this ground roof and search function so I'll remove that so now I want to specify the search function right and this search function is something I will put there and I'll call it um so question Vector Canon um will be question Vector q&n uh and then\",\n",
       "  'timecode_text': 'Wrapper function for evaluation',\n",
       "  'description': 'In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.',\n",
       "  'link': 'https://www.youtube.com/watch?v=VRprIm9-VV8&t=1140s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval cool so now we can use this function to evaluate our um um the this uh this way of retrieving this so I'll put it in a function um that we will use in our so remember so I'll delete this one this one um so here this stuff we don't need so this was an example and here we used um this evalate right so in evate we have this ground roof and search function so I'll remove that so now I want to specify the search function right and this search function is something I will put there and I'll call it um so question Vector Canon um will be question Vector q&n uh and then In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.\",\n",
       "  'id': 'e1dd7aa5035769f727dfa46dd6c385a9'},\n",
       " {'vid_id': 'VRprIm9-VV8',\n",
       "  'title': 'LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval',\n",
       "  'timecode': '20:01',\n",
       "  'text': \"uh so what we will have is um from query we need to have um question right so let me quickly take a look at our ground through data uh ground through data zero is question course document right so Q question so this will be our question um then course Q course and then doc ID is our Doc it's document right right so now we need to create this VQ VQ which we use model for this one model en code right uh and question and now we return elastic show search Canon uh Vector dat engine just course we probably don't need do ID right just course and question um let us see if this is\",\n",
       "  'timecode_text': 'Function Implementation and Testing',\n",
       "  'description': 'In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.',\n",
       "  'link': 'https://www.youtube.com/watch?v=VRprIm9-VV8&t=1201s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval uh so what we will have is um from query we need to have um question right so let me quickly take a look at our ground through data uh ground through data zero is question course document right so Q question so this will be our question um then course Q course and then doc ID is our Doc it's document right right so now we need to create this VQ VQ which we use model for this one model en code right uh and question and now we return elastic show search Canon uh Vector dat engine just course we probably don't need do ID right just course and question um let us see if this is In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.\",\n",
       "  'id': '61f89477676414c52317c6a66b9aca9a'},\n",
       " {'vid_id': 'VRprIm9-VV8',\n",
       "  'title': 'LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval',\n",
       "  'timecode': '21:20',\n",
       "  'text': \"sufficient iate this one evate uh ground truth and search function is uh what did we call it question Vector KNN um yeah so it works dock ID is something we use here inside the aate function not outside so I don't need it inside the search function and we can see that it is uh I would say significantly slower and the reason it's slower is because computing the embeddings is slower right so the search itself is rather fast but we need to compute the embeddings and this is where it takes time so I'll put it on pause and come back when it finishes ating uh this way of retrieving um data okay it finished I do not see the answers myself so let's see uh so it's 77 and 66 I think something very similar we had with plain elastic search I think let me check quickly so I'll probably move these documents in evaluation folder but for now they are here in um um and all the way down uh this is not the right one of course um did I not commit it okay I will check it here I thought I committed it okay so this is our result for plane elastic search with text um so elastic search text only right so we see that it's actually better right especially m r r is better uh heat rate is slightly better um okay so maybe 6% of improvement so here we need to think about two things okay number wise it's definitely better right but speed wise it was um do we have data on how um yeah I think we don't have any viets here um yeah sadly but elastic search was uh with just X was significantly faster so this is also what we need to take into account like do we want to sacrifice speed in terms of uh like do we want to make a sacrifice in terms of speed in order to get uh this uh extra percent 6% of uh Improvement in Mr could be actually a good thing right so 6% is uh looks significant but um let's take a look\",\n",
       "  'timecode_text': 'Evaluating question embeddings',\n",
       "  'description': 'In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.',\n",
       "  'link': 'https://www.youtube.com/watch?v=VRprIm9-VV8&t=1280s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval sufficient iate this one evate uh ground truth and search function is uh what did we call it question Vector KNN um yeah so it works dock ID is something we use here inside the aate function not outside so I don't need it inside the search function and we can see that it is uh I would say significantly slower and the reason it's slower is because computing the embeddings is slower right so the search itself is rather fast but we need to compute the embeddings and this is where it takes time so I'll put it on pause and come back when it finishes ating uh this way of retrieving um data okay it finished I do not see the answers myself so let's see uh so it's 77 and 66 I think something very similar we had with plain elastic search I think let me check quickly so I'll probably move these documents in evaluation folder but for now they are here in um um and all the way down uh this is not the right one of course um did I not commit it okay I will check it here I thought I committed it okay so this is our result for plane elastic search with text um so elastic search text only right so we see that it's actually better right especially m r r is better uh heat rate is slightly better um okay so maybe 6% of improvement so here we need to think about two things okay number wise it's definitely better right but speed wise it was um do we have data on how um yeah I think we don't have any viets here um yeah sadly but elastic search was uh with just X was significantly faster so this is also what we need to take into account like do we want to sacrifice speed in terms of uh like do we want to make a sacrifice in terms of speed in order to get uh this uh extra percent 6% of uh Improvement in Mr could be actually a good thing right so 6% is uh looks significant but um let's take a look In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.\",\n",
       "  'id': '1564081d87a59a79fe34e1e0f798c411'},\n",
       " {'vid_id': 'VRprIm9-VV8',\n",
       "  'title': 'LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval',\n",
       "  'timecode': '24:28',\n",
       "  'text': \"actually at other two uh options right so in addition to uh question Vector we can also have text Vector right so text text and it would be text Vector uh yeah let me we can quickly check if it actually works ground truth zero so something is wrong okay so uh yeah of course this is our query Yeah question so this still stays question and our uh Vector is still question because this is the question from the student this is not the question from our database and I got a bit confused because of that but here we use text Vector did we use text Vector previously so previously it was question Vector here we use text Vector right so actually the rest stays the same because the question from the student is still the question from the student we just use a different um field inside elastic too much uh to against uh to compute the similarity against this uh Vector right uh okay so I think uh this works now and I will also execute this and I think the speed is um well it shouldn't be different because like the dimensionality of factors uh is the same okay so while it's evaluating I will also do the other thing so we have text Vector we have question vector and then we have question text Vector right uh which would be question text Vector if I remember correctly um where do we have it question text Vector yes so this one and this is something I will execute to right now okay so again putting this on pause and then coming back when when both of them finish and I hope this one does not have any errors any mistakes so it will just run okay so in the first part computed it took quite some time but as we see uh if we just use the text Vector the uh it improved even more than uh just using question and now we see combined and in the meantime what I wanted to check is uh I think there is a way in elastic search to combine like we did with text so with text we computed uh both um um so we have our query and then we executed it against both um do I have it here an example so when we did it in the first module our yeah so our query was multimatch and it was multimatch with text uh I think there is a way to do the same with uh KNN and this is something uh we can ask chat GPD uh hey I have um I'll just put it on pause finish typing and then you'll see the my prompt so this is what I typed I have an elastics search index with three dense vectors uh I want to quy all three of them with one vector all of them were created using the same embedding model so we can compare them so I want to have something similar to this and but with Canon and this is my current Canon quer so let's see uh what it wrote okay interesting so actually what we can do is uh use a script here to compute cign similarity between all these three I had no idea um but let us check it uh while this is uh this is being computed um so I hope it works CU I have never used it uh so is it um elastic search can is it similar to this one yes so have field Vector but we will just have one vector right uh combined and uh yeah I want to see this for qn and what we have of course let's say data engineering Zoom Camp so it still needs to finish uh um evaluating the results no it's not so long yeah I'll just put it on B yeah so the result is even better when we combine them it's 91 so I imagine if we used a better model from um where is it if we used this one it would be even better right which is quite good where is it it's quite good so heat rate and Mr are good and let us see if this one actually works Q is not defined and how did we call it I don't remember inq yeah okay so it doesn't seem to work well I will not attempt to spend uh I will not spend time right now cuz uh this is something new sometimes things coming from jgpt they don't necessarily work um you can experiment with that I'll also experiment with that uh after this video and maybe I'll share the results with you I will commit this notebook as this with a note that note this doesn't work and then feel free to play with this maybe you know elastic search better than me uh and uh if you know elastic search better than me uh maybe you know how to actually do this in the right way and if that's if this is the case please let us know share this in chat and anyways we can see that uh this is clearly better even though it is uh slower but sometimes being slower is okay and we can do 17 iterations per second so per one student they will probably not even notice that there is uh uh it's slower than what we had before right okay so this is how we evaluate different methods and here we compare text uh evaluation of text results with evaluation of vector uh search we can see that in this data set Vector search worked better one of the reasons for that could be in the way we constructed the data we asked GPT to specifically include as fewer words as possible from the original FAQ record so it kind of makes sense and we see that in these situations Vector search makes sense okay so that's all for this section I hope you enjoyed it and see you around so five or six hours later I actually read what charp road saying make sure to adjust Vector one vector 2 and Vector 3 to the actual field names because obviously for some reasons it cannot read my mind so what I will do is I will actually replace it so we have text vector and then question text Vector right so I replace that now execute and then it complains that for some document it returned an invalid score which is weird like it shouldn't happen so I don't know how it's possible but whatever what we can do is just add one to make sure it always stays positive and then we get the results right and we can now evaluate this result too so I'll do it um yeah so what we need is um um I guess these two I'll combine them into one one cell and now copy it so this uh I'll call it vectors combined K andn and we here use this elastic search K and then combined K and then combined and we only need to pass the vector oops too much the vector and course and yeah let's remove that and execute that I think it's is it slightly slower yeah a little bit right so now H I guess for elastic it's more difficult to compute this similarity I actually have no idea how this works so I have some ideas how it works when it comes to individual uh one so for this one yeah I don't know uh but yeah we can see that it's only slightly slower well actually not slightly but 30 maybe even 40% slower or if not if not 50 but anyways actually like it's still 15 iterations per second so for a user uh probably they will not notice anything uh but of course when we evaluate across the entire data set it's not as well so I'll put it on pause again and come back when it finishes and I'm really curious what kind of score we will have does it make sense to actually do that cuz it makes it more complex so now we will see okay so it finished heat rate 90% Mr 80% let's compare it with the previous one yeah it's actually worse so like making it more complicated does not necessarily mean it's becoming better so as we see the best option and it's also like a bit faster to just use use the question text and Vector text combined so I guess this is what we should use and now thanks to this ground proof data set that we have we can answer questions like that so that's all for this series and see you soon\",\n",
       "  'timecode_text': 'Evaluating answer embeddings',\n",
       "  'description': 'In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.',\n",
       "  'link': 'https://www.youtube.com/watch?v=VRprIm9-VV8&t=1468s',\n",
       "  'text_vector': \"LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval actually at other two uh options right so in addition to uh question Vector we can also have text Vector right so text text and it would be text Vector uh yeah let me we can quickly check if it actually works ground truth zero so something is wrong okay so uh yeah of course this is our query Yeah question so this still stays question and our uh Vector is still question because this is the question from the student this is not the question from our database and I got a bit confused because of that but here we use text Vector did we use text Vector previously so previously it was question Vector here we use text Vector right so actually the rest stays the same because the question from the student is still the question from the student we just use a different um field inside elastic too much uh to against uh to compute the similarity against this uh Vector right uh okay so I think uh this works now and I will also execute this and I think the speed is um well it shouldn't be different because like the dimensionality of factors uh is the same okay so while it's evaluating I will also do the other thing so we have text Vector we have question vector and then we have question text Vector right uh which would be question text Vector if I remember correctly um where do we have it question text Vector yes so this one and this is something I will execute to right now okay so again putting this on pause and then coming back when when both of them finish and I hope this one does not have any errors any mistakes so it will just run okay so in the first part computed it took quite some time but as we see uh if we just use the text Vector the uh it improved even more than uh just using question and now we see combined and in the meantime what I wanted to check is uh I think there is a way in elastic search to combine like we did with text so with text we computed uh both um um so we have our query and then we executed it against both um do I have it here an example so when we did it in the first module our yeah so our query was multimatch and it was multimatch with text uh I think there is a way to do the same with uh KNN and this is something uh we can ask chat GPD uh hey I have um I'll just put it on pause finish typing and then you'll see the my prompt so this is what I typed I have an elastics search index with three dense vectors uh I want to quy all three of them with one vector all of them were created using the same embedding model so we can compare them so I want to have something similar to this and but with Canon and this is my current Canon quer so let's see uh what it wrote okay interesting so actually what we can do is uh use a script here to compute cign similarity between all these three I had no idea um but let us check it uh while this is uh this is being computed um so I hope it works CU I have never used it uh so is it um elastic search can is it similar to this one yes so have field Vector but we will just have one vector right uh combined and uh yeah I want to see this for qn and what we have of course let's say data engineering Zoom Camp so it still needs to finish uh um evaluating the results no it's not so long yeah I'll just put it on B yeah so the result is even better when we combine them it's 91 so I imagine if we used a better model from um where is it if we used this one it would be even better right which is quite good where is it it's quite good so heat rate and Mr are good and let us see if this one actually works Q is not defined and how did we call it I don't remember inq yeah okay so it doesn't seem to work well I will not attempt to spend uh I will not spend time right now cuz uh this is something new sometimes things coming from jgpt they don't necessarily work um you can experiment with that I'll also experiment with that uh after this video and maybe I'll share the results with you I will commit this notebook as this with a note that note this doesn't work and then feel free to play with this maybe you know elastic search better than me uh and uh if you know elastic search better than me uh maybe you know how to actually do this in the right way and if that's if this is the case please let us know share this in chat and anyways we can see that uh this is clearly better even though it is uh slower but sometimes being slower is okay and we can do 17 iterations per second so per one student they will probably not even notice that there is uh uh it's slower than what we had before right okay so this is how we evaluate different methods and here we compare text uh evaluation of text results with evaluation of vector uh search we can see that in this data set Vector search worked better one of the reasons for that could be in the way we constructed the data we asked GPT to specifically include as fewer words as possible from the original FAQ record so it kind of makes sense and we see that in these situations Vector search makes sense okay so that's all for this section I hope you enjoyed it and see you around so five or six hours later I actually read what charp road saying make sure to adjust Vector one vector 2 and Vector 3 to the actual field names because obviously for some reasons it cannot read my mind so what I will do is I will actually replace it so we have text vector and then question text Vector right so I replace that now execute and then it complains that for some document it returned an invalid score which is weird like it shouldn't happen so I don't know how it's possible but whatever what we can do is just add one to make sure it always stays positive and then we get the results right and we can now evaluate this result too so I'll do it um yeah so what we need is um um I guess these two I'll combine them into one one cell and now copy it so this uh I'll call it vectors combined K andn and we here use this elastic search K and then combined K and then combined and we only need to pass the vector oops too much the vector and course and yeah let's remove that and execute that I think it's is it slightly slower yeah a little bit right so now H I guess for elastic it's more difficult to compute this similarity I actually have no idea how this works so I have some ideas how it works when it comes to individual uh one so for this one yeah I don't know uh but yeah we can see that it's only slightly slower well actually not slightly but 30 maybe even 40% slower or if not if not 50 but anyways actually like it's still 15 iterations per second so for a user uh probably they will not notice anything uh but of course when we evaluate across the entire data set it's not as well so I'll put it on pause again and come back when it finishes and I'm really curious what kind of score we will have does it make sense to actually do that cuz it makes it more complex so now we will see okay so it finished heat rate 90% Mr 80% let's compare it with the previous one yeah it's actually worse so like making it more complicated does not necessarily mean it's becoming better so as we see the best option and it's also like a bit faster to just use use the question text and Vector text combined so I guess this is what we should use and now thanks to this ground proof data set that we have we can answer questions like that so that's all for this series and see you soon In this video, I discuss evaluating retrieval methods, comparing text results and vector search. We use sentence embeddings of different fields and see which one performs better.\",\n",
       "  'id': 'a81608cc2024126780a1afcc7632ddc8'},\n",
       " {'vid_id': 'OWqinqemCmk',\n",
       "  'title': 'LLM Zoomcamp 4.1 - Introduction to monitoring answer quality',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hi everyone and welcome back to the L&M Zoom Camp so this week it will be all about monitoring so we're now in the fourth module and by now youve learned about the okay the overall concepts of rack systems like how you can retrieve augment and generate an llm answer so you've not only learned to Ping open AI endpoint to to to for instance a GPT model but also we've covered um how to self-host a model with ol Lama for instance and then okay how do we actually add domain data or context data to this llm system by using Vector databases so it means you're storing data into a vector store you retrieving and then Alexa also has um covered how do you actually compute or or assess if the vector retrieval was good enough and then for today we want to observe and monitor if the answer of the llm and the quality is good enough and there are different ways to do this so this is what we will cover in today's um video so today's video will just be like 10 to 15 minutes theoretical and then in the upcoming videos we will really dive into the code but for today we're just going to do an extended introduction like what are the topics that we are going to cover and the concepts behind monitoring llm systems first of all why monitoring L llm systems it might be that we don't need to convince anyone but um there are I think also like maybe maybe people in your company that they are saying like hey you know I'm just trusting the system I'm deploying it and then it's I'm done but actually it's not enough to just build a system and deploy the llm system but in the end you also have to monitor it over time when you operate system so that you know how is my llm doing so that's what we're going to discuss then also okay how do I now actually monitor the quality of my llms so how do I know if the answer that my llm is giving is is good enough towards the user so we can compute different types of quality metrix store the computed metrix in a relational database and then use grafana to visualize the metrics over time so that we can really keep track of the information of the important information and metrics over time then the other main part of the monitoring module will be okay how can I also use user feedback so how can I store the chat sessions and then also collect user feedback in a database and then I can connect this database again to crana and visualize the user feedback and the corresponding chat sessions over time this module is meant to be be extending over the course of one week so we won't be able to cover everything and also what I'll be sharing here is definitely it's not everything for the perfect monitoring systems it's just some starting starting points that you can take to your company or to your private project and start monitoring your system but there will also be things that we are not going to be able to cover as part of monitoring module so let's dive into the slide deag why do we need to monitor llm systems I think um it's it's pretty obvious like everyone thinks that llm systems are super super smart and intelligent and we don't need to do anything about it but then I think people also realize that okay actually we need to watch over these systems and it's super hard to watch over these systems because it's not just like an if else rule it's not just a hardcoded rule engine and and Rule based system but instead it's an llm it's an it's an llm system that is intelligent that generates answers that are different from each other the system can be sort of creative and come up with their own answer and and most importantly it really just answers in a way based on the data that is that it has been feed to so here is one example I think it's super famous it's even from 2016 like we don't even need to to call out any like recent misbehavior for any chatbot system but instead we can just remember the classical things where the where one AI chatbot was racist after I think a couple of days I think it didn't take long after this chatbot went bad and as a company you just want to Simply avoid to to yeah to disappoint your customers you don't want to be in the press and be famous for for your chatbot not behaving so that's why you want to monitor it and know what's going on over time as we said it's not enough to just deploy it and then hope for the best but better have an eye on the system one way of monitoring is to really monitor and observe the answer quality of the llm so how do I actually know that my llm system gave a good answer so there are different types of metrics that you can compute there's not this one single metric that you can compute and then you know everything about your your rag or llm system but instead you will have to come up with a set of feature H with a set of metrics that is going to tell you if your model is doing good or if you want to readjust and refine it so I I brought these three example metrics and these three are also the ones that we will then actually compute in Python code and and work with so I've chosen a very classical Vector similarity metric so what you're doing is you're just you're collector collecting a data set with crown roof so this is like the expected answer so you have a data set where you have your question the expected answer and then you add the llm generated answer to it and both of these texts you're going to just St as a vector embedding and then once you have it as a vector embedding you can just simply compute the vector similarity so that you know the mathematical distance between those two things so you know okay how how far is the generated answer from the expected answer so that also already gives you a clue of if it might make sense or if your llm system is completely off with with the generating the answer so that's a yeah a pure mathematical approach so there's no much room for for creativity it's just yeah Computing the vector similarity but then there are also um some more advanced ways of of estimating if if the answer is good enough of your system so you can actually use llms as a judge so finnally you could just use an llm that is like focused on for instance detecting toxicity within a text and then you can just ask the llm like hey do you think that this llm answer that my other llm generated is toxic or not and you can get a score for it so there are are for instance open source models on hugging phas available that that just focus on okay assessing if a text is toxic or not so this is what we're also going to do and compute we will be downloading a model from hugging phase and then prepare a prompt to ask the llm to judge about the toxicity of our other lm's answer and then as a third metric what we're going to compute is to really ask the llm like we're going to write a prompt where we ask the llm like look this is the llm answer that I received from another llm system and this is the expected answer and then we just asked LM do you think this makes sense or not so this is definitely a more creative way of of judging if the answer is good enough so I would say it can detect more complex patterns but it might also go um be like too creative and then that's where we that's why we also want to have the combination with the mathematical approach and then then those three metrics as I set are already a good starting point and then you as a company or with your pet project at home can just assess which metrics do you need and do you want to extend it with further metrics then we're going to store the computed metrics in a relational database we're going to use postr SQL for it we will boot it up inside of a Docker container and then also have a Docker compos file so that we can combine all the services and easily connect the database to crana and then on crana we going to to add some graphics so that we can simply visualize the metrix over time next one we also want to give our beloved users the chance to give feedback so you might know it from tat front and there you're also able to give a thumbs up or a coms down and that's what we also want to this concept is what we also want to do with our Rack or llm system so we want to collect the user feedback thumbs up and thumbs down in a database and alongside this we also want to start the chat sessions because obviously then we want to know okay if there's like a lot of negative feedback we want to know okay about which chat session was it and what did the what did the customer or user actually ask and what does the system answer so that's why we also need to store the whole chat history so that we can trace it back and here again we will also use the open source visualization tool grafana put it up in a Docker container connected to the database where we have stored the user feedback in the chat sessions and then visualize the user feedback over time as I said in this one week we don't want to overwhelm you and we're also not experts in everything so that's why I've listed some further the things you might want to monitor that are not covered by this module obviously you can track and record further quality metrics and user feedback I have only proposed three different kind of metrics and storing the thumbs up and thumbs down button but you should do more you can and should do more so for instance you might want to know about buyers and fairness you want to know how how fair is your model towards different genders towards different nationalities and regions of the world then you might also want to know what is your customers talking about you might think that every customer just ask like hey what's the quickest way to buy your product but in fact they're just asking like how can I how can I get a refund how can I get a discount code right and then this is things that you could cover and observe with topic clustering that you just have like yeah clusters of topics where you know oh this is actually what my people are talking about with the llm system this is how they are really using it and you might not only want to have structured feedback with thumbs up or thumbs down but you could also have textual user feedback that they can really explain like hey this answer was not correct because of of course it's then more effort to analyze this feedback because it's unstructured text but for some situations if you for instance detect some chat sessions and messages where the feedback was very negative then you might want to dive into the corresponding textual user feedback and then for instance on cat GPT front L you also have the possibility to copy copy an answer or or the like the code of an answer and then that's also that could also be an indirect sign of the user likes what I'm proposing and now the user is going to to try it out that's why the user is copy pasting it so that's also a way of indirect feedback it might be worth monitoring then for all the devops engineers here you might have already heard about four golden signals so you not only want to know about the like quality of your llm model answer but you also want to know other so so-called system metrics of your system they are called latency traffic errors and saturation those four golden signals and latency is obviously you want to know if your customer is waiting like 10 seconds for an answer or or if the system is quick enough in answering you also want to know okay how many customers are actually using my chatbot how much traffic am I receiving and does my llm system respond with error codes or is everything fine because I mean I think everyone of you has seen this like 404 or or with ch GPT or other providers there's there are error messages like hey something went wrong and that's what the user is seeing on the UI and is is like sad or desperate about it so we as developers of an llm system also want to know if our customers are seeing a lot of errors or or everything is going smooth and it's also no surprise that these systems are very resource hungry so we want to know about the saturation how much memory and GPU is my system using so this is referred to as the four golden signal so hey if you have a daop engineer in your company just just grabbed them and ask him to to add those also um to your llm system so that you not only have the quality metrics and the user feedback but that you also compl it with system metrics another nons surprising fact is that llms are not that cheap so if you have the time you should also invest in really monitoring and tracking the cost of your used infrastructure so typically you would have a vector store and an llm IPI that you are calling the vector store itself like I found it's not that expensive but what's really expensive if you either use a managed API for instance with open AI or other model providers you have to buy tokens and you have to pay for it per use so that might get expensive pretty quick or even if you're self host and you're running the model on gpus 24/7 then this can also increase the costs of your of your cloud provider or wherever you're using the gpus from but yeah here we also we won't have the time in this week to cover everything but you can read up on this LinkedIn post about it so thanks for listening in the next couple of videos we will really dive into the code so here for monitoring the answer quality of llms with metrics and um with those three we will uh write the python code together and equip our system store the metrics in a pogress database and use grafana to visualize it so we will build the code together and I will share it with you then in the GitHub repository and same same for monitoring the user feedback we will prepare all the python code to to store the chat sessions and collect the user feedback in in also post CR and then create a cunner dashboard to visualize it over time so see you in the next video\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=OWqinqemCmk',\n",
       "  'text_vector': \"LLM Zoomcamp 4.1 - Introduction to monitoring answer quality hi everyone and welcome back to the L&M Zoom Camp so this week it will be all about monitoring so we're now in the fourth module and by now youve learned about the okay the overall concepts of rack systems like how you can retrieve augment and generate an llm answer so you've not only learned to Ping open AI endpoint to to to for instance a GPT model but also we've covered um how to self-host a model with ol Lama for instance and then okay how do we actually add domain data or context data to this llm system by using Vector databases so it means you're storing data into a vector store you retrieving and then Alexa also has um covered how do you actually compute or or assess if the vector retrieval was good enough and then for today we want to observe and monitor if the answer of the llm and the quality is good enough and there are different ways to do this so this is what we will cover in today's um video so today's video will just be like 10 to 15 minutes theoretical and then in the upcoming videos we will really dive into the code but for today we're just going to do an extended introduction like what are the topics that we are going to cover and the concepts behind monitoring llm systems first of all why monitoring L llm systems it might be that we don't need to convince anyone but um there are I think also like maybe maybe people in your company that they are saying like hey you know I'm just trusting the system I'm deploying it and then it's I'm done but actually it's not enough to just build a system and deploy the llm system but in the end you also have to monitor it over time when you operate system so that you know how is my llm doing so that's what we're going to discuss then also okay how do I now actually monitor the quality of my llms so how do I know if the answer that my llm is giving is is good enough towards the user so we can compute different types of quality metrix store the computed metrix in a relational database and then use grafana to visualize the metrics over time so that we can really keep track of the information of the important information and metrics over time then the other main part of the monitoring module will be okay how can I also use user feedback so how can I store the chat sessions and then also collect user feedback in a database and then I can connect this database again to crana and visualize the user feedback and the corresponding chat sessions over time this module is meant to be be extending over the course of one week so we won't be able to cover everything and also what I'll be sharing here is definitely it's not everything for the perfect monitoring systems it's just some starting starting points that you can take to your company or to your private project and start monitoring your system but there will also be things that we are not going to be able to cover as part of monitoring module so let's dive into the slide deag why do we need to monitor llm systems I think um it's it's pretty obvious like everyone thinks that llm systems are super super smart and intelligent and we don't need to do anything about it but then I think people also realize that okay actually we need to watch over these systems and it's super hard to watch over these systems because it's not just like an if else rule it's not just a hardcoded rule engine and and Rule based system but instead it's an llm it's an it's an llm system that is intelligent that generates answers that are different from each other the system can be sort of creative and come up with their own answer and and most importantly it really just answers in a way based on the data that is that it has been feed to so here is one example I think it's super famous it's even from 2016 like we don't even need to to call out any like recent misbehavior for any chatbot system but instead we can just remember the classical things where the where one AI chatbot was racist after I think a couple of days I think it didn't take long after this chatbot went bad and as a company you just want to Simply avoid to to yeah to disappoint your customers you don't want to be in the press and be famous for for your chatbot not behaving so that's why you want to monitor it and know what's going on over time as we said it's not enough to just deploy it and then hope for the best but better have an eye on the system one way of monitoring is to really monitor and observe the answer quality of the llm so how do I actually know that my llm system gave a good answer so there are different types of metrics that you can compute there's not this one single metric that you can compute and then you know everything about your your rag or llm system but instead you will have to come up with a set of feature H with a set of metrics that is going to tell you if your model is doing good or if you want to readjust and refine it so I I brought these three example metrics and these three are also the ones that we will then actually compute in Python code and and work with so I've chosen a very classical Vector similarity metric so what you're doing is you're just you're collector collecting a data set with crown roof so this is like the expected answer so you have a data set where you have your question the expected answer and then you add the llm generated answer to it and both of these texts you're going to just St as a vector embedding and then once you have it as a vector embedding you can just simply compute the vector similarity so that you know the mathematical distance between those two things so you know okay how how far is the generated answer from the expected answer so that also already gives you a clue of if it might make sense or if your llm system is completely off with with the generating the answer so that's a yeah a pure mathematical approach so there's no much room for for creativity it's just yeah Computing the vector similarity but then there are also um some more advanced ways of of estimating if if the answer is good enough of your system so you can actually use llms as a judge so finnally you could just use an llm that is like focused on for instance detecting toxicity within a text and then you can just ask the llm like hey do you think that this llm answer that my other llm generated is toxic or not and you can get a score for it so there are are for instance open source models on hugging phas available that that just focus on okay assessing if a text is toxic or not so this is what we're also going to do and compute we will be downloading a model from hugging phase and then prepare a prompt to ask the llm to judge about the toxicity of our other lm's answer and then as a third metric what we're going to compute is to really ask the llm like we're going to write a prompt where we ask the llm like look this is the llm answer that I received from another llm system and this is the expected answer and then we just asked LM do you think this makes sense or not so this is definitely a more creative way of of judging if the answer is good enough so I would say it can detect more complex patterns but it might also go um be like too creative and then that's where we that's why we also want to have the combination with the mathematical approach and then then those three metrics as I set are already a good starting point and then you as a company or with your pet project at home can just assess which metrics do you need and do you want to extend it with further metrics then we're going to store the computed metrics in a relational database we're going to use postr SQL for it we will boot it up inside of a Docker container and then also have a Docker compos file so that we can combine all the services and easily connect the database to crana and then on crana we going to to add some graphics so that we can simply visualize the metrix over time next one we also want to give our beloved users the chance to give feedback so you might know it from tat front and there you're also able to give a thumbs up or a coms down and that's what we also want to this concept is what we also want to do with our Rack or llm system so we want to collect the user feedback thumbs up and thumbs down in a database and alongside this we also want to start the chat sessions because obviously then we want to know okay if there's like a lot of negative feedback we want to know okay about which chat session was it and what did the what did the customer or user actually ask and what does the system answer so that's why we also need to store the whole chat history so that we can trace it back and here again we will also use the open source visualization tool grafana put it up in a Docker container connected to the database where we have stored the user feedback in the chat sessions and then visualize the user feedback over time as I said in this one week we don't want to overwhelm you and we're also not experts in everything so that's why I've listed some further the things you might want to monitor that are not covered by this module obviously you can track and record further quality metrics and user feedback I have only proposed three different kind of metrics and storing the thumbs up and thumbs down button but you should do more you can and should do more so for instance you might want to know about buyers and fairness you want to know how how fair is your model towards different genders towards different nationalities and regions of the world then you might also want to know what is your customers talking about you might think that every customer just ask like hey what's the quickest way to buy your product but in fact they're just asking like how can I how can I get a refund how can I get a discount code right and then this is things that you could cover and observe with topic clustering that you just have like yeah clusters of topics where you know oh this is actually what my people are talking about with the llm system this is how they are really using it and you might not only want to have structured feedback with thumbs up or thumbs down but you could also have textual user feedback that they can really explain like hey this answer was not correct because of of course it's then more effort to analyze this feedback because it's unstructured text but for some situations if you for instance detect some chat sessions and messages where the feedback was very negative then you might want to dive into the corresponding textual user feedback and then for instance on cat GPT front L you also have the possibility to copy copy an answer or or the like the code of an answer and then that's also that could also be an indirect sign of the user likes what I'm proposing and now the user is going to to try it out that's why the user is copy pasting it so that's also a way of indirect feedback it might be worth monitoring then for all the devops engineers here you might have already heard about four golden signals so you not only want to know about the like quality of your llm model answer but you also want to know other so so-called system metrics of your system they are called latency traffic errors and saturation those four golden signals and latency is obviously you want to know if your customer is waiting like 10 seconds for an answer or or if the system is quick enough in answering you also want to know okay how many customers are actually using my chatbot how much traffic am I receiving and does my llm system respond with error codes or is everything fine because I mean I think everyone of you has seen this like 404 or or with ch GPT or other providers there's there are error messages like hey something went wrong and that's what the user is seeing on the UI and is is like sad or desperate about it so we as developers of an llm system also want to know if our customers are seeing a lot of errors or or everything is going smooth and it's also no surprise that these systems are very resource hungry so we want to know about the saturation how much memory and GPU is my system using so this is referred to as the four golden signal so hey if you have a daop engineer in your company just just grabbed them and ask him to to add those also um to your llm system so that you not only have the quality metrics and the user feedback but that you also compl it with system metrics another nons surprising fact is that llms are not that cheap so if you have the time you should also invest in really monitoring and tracking the cost of your used infrastructure so typically you would have a vector store and an llm IPI that you are calling the vector store itself like I found it's not that expensive but what's really expensive if you either use a managed API for instance with open AI or other model providers you have to buy tokens and you have to pay for it per use so that might get expensive pretty quick or even if you're self host and you're running the model on gpus 24/7 then this can also increase the costs of your of your cloud provider or wherever you're using the gpus from but yeah here we also we won't have the time in this week to cover everything but you can read up on this LinkedIn post about it so thanks for listening in the next couple of videos we will really dive into the code so here for monitoring the answer quality of llms with metrics and um with those three we will uh write the python code together and equip our system store the metrics in a pogress database and use grafana to visualize it so we will build the code together and I will share it with you then in the GitHub repository and same same for monitoring the user feedback we will prepare all the python code to to store the chat sessions and collect the user feedback in in also post CR and then create a cunner dashboard to visualize it over time so see you in the next video Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '5bcd67efc6833389d0f49254eae120a3'},\n",
       " {'vid_id': 'yTKGSqkhgI4',\n",
       "  'title': 'LLM Zoomcamp 4.2 - Evaluation and Monitoring in LLMs',\n",
       "  'timecode': '00:00',\n",
       "  'text': 'hi everyone welcome to another video of llm Zoom camp and in this series of videos we continue talking about evaluation and we will also talk about monitoring evaluation and monitoring come hand in hand um they are quite closely related so in this video we will start with offline evaluation and specifically we will talk about evaluating rack but also the things we talk about here apply in general to llms and um things we talk about here here uh will work for evaluating quality of other LM applications too not on the Rock and um I want to do first a quick recap of what we have done also in the previous um uh modules uh because we are going to put together now everything we have learned so far so in the first model we talked about the rck flow so remember we defined this R fun function which takes in a query uh then the first thing we did was uh performance search so we would get search results uh results search for this que right then we build prompt prompt um build prompt based on the query and the search results and then finally the last step was asking uh an llm and then giving returning the answer right so this is something we covered in the first module um so we talked about uh text search here uh and then we built a prompt and then we used open AI um to give us answer right then in the second module we talked about replacing open AI with other llms right so we took care of changing this thing here and in third',\n",
       "  'timecode_text': 'Introduction to evaluation and monitoring',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=yTKGSqkhgI4&t=0s',\n",
       "  'text_vector': 'LLM Zoomcamp 4.2 - Evaluation and Monitoring in LLMs hi everyone welcome to another video of llm Zoom camp and in this series of videos we continue talking about evaluation and we will also talk about monitoring evaluation and monitoring come hand in hand um they are quite closely related so in this video we will start with offline evaluation and specifically we will talk about evaluating rack but also the things we talk about here apply in general to llms and um things we talk about here here uh will work for evaluating quality of other LM applications too not on the Rock and um I want to do first a quick recap of what we have done also in the previous um uh modules uh because we are going to put together now everything we have learned so far so in the first model we talked about the rck flow so remember we defined this R fun function which takes in a query uh then the first thing we did was uh performance search so we would get search results uh results search for this que right then we build prompt prompt um build prompt based on the query and the search results and then finally the last step was asking uh an llm and then giving returning the answer right so this is something we covered in the first module um so we talked about uh text search here uh and then we built a prompt and then we used open AI um to give us answer right then in the second module we talked about replacing open AI with other llms right so we took care of changing this thing here and in third Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'id': 'a44949529795c9cb8a0ef294de5b6207'},\n",
       " {'vid_id': 'yTKGSqkhgI4',\n",
       "  'title': 'LLM Zoomcamp 4.2 - Evaluation and Monitoring in LLMs',\n",
       "  'timecode': '02:05',\n",
       "  'text': \"model in Vector search we talked more about this and not only we talked about um using Vector search uh replacing text search with Vector search but also very important we talked about evaluating this part so here we talked about uh evaluating retrial uh so we talked about metrics like heat rate we talked about Mr uh yeah mean reciprocal rank right uh was it yeah I think that was uh I remember it's mrr right so we here we spent uh in module 3 talking about this part and also evaluating this part because there are so many ways you can Implement uh the search function right and how do you know which one is best and what is left to actually evaluating the whole thing right okay now we know how to evaluate retrieval so we know heat rate we know Mr there are other metrics that we can use too but how good is our prompt or how good is the llm we use right so how do we evoluate this whole thing in general and there are two approaches to evoluation and it's not only true for llms but also for other things so we have offline evaluation and online evaluation uh so when it comes to offline evaluation so these are the things uh um to heat rate yes thank you so I I use um co-pilot from GitHub that's what it suggested so heat rate was actually an offline evaluation metric for evaluating search results right so offline meaning here we develop a system and we try to evalate the system to get a metric and this is what we do before we deploy this metric right so we see okay this method for search is better than this method for search because heat rate or Mr or some other metric is better so now we can deploy this and once we deploy there are other things that we can do to overate it offline so usually uh we do things like AB tests this is not something that we will cover here or some other experiments right so here now the idea is that um there are a lot of users coming to our website we split the traffic into certain proportions and a part of users go to our new method and we see how the new method uh is better or worse with respect to some business metrics um then yeah and this is something actually we will also talk a little bit about we will not specifically talk about AB test but in context of llms quite often what people use is user feedback like this thumbs up thumbs down this is something we will Implement too and also monitoring and online evaluation often comes together come together and monitoring is just observing the overall health uh Health uh the system right so this could be some performance metrics like it suggests like CPU uh and so on um and um but when it comes to uh llms here we care about things like um yeah user feedback and like how good the answer is uh and so on so let me talk a bit first like I'll make it just text text so let me first talk about um offline evolation because this is something we will cover\",\n",
       "  'timecode_text': 'Replacing OpenAI with Other LLMs',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=yTKGSqkhgI4&t=125s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.2 - Evaluation and Monitoring in LLMs model in Vector search we talked more about this and not only we talked about um using Vector search uh replacing text search with Vector search but also very important we talked about evaluating this part so here we talked about uh evaluating retrial uh so we talked about metrics like heat rate we talked about Mr uh yeah mean reciprocal rank right uh was it yeah I think that was uh I remember it's mrr right so we here we spent uh in module 3 talking about this part and also evaluating this part because there are so many ways you can Implement uh the search function right and how do you know which one is best and what is left to actually evaluating the whole thing right okay now we know how to evaluate retrieval so we know heat rate we know Mr there are other metrics that we can use too but how good is our prompt or how good is the llm we use right so how do we evoluate this whole thing in general and there are two approaches to evoluation and it's not only true for llms but also for other things so we have offline evaluation and online evaluation uh so when it comes to offline evaluation so these are the things uh um to heat rate yes thank you so I I use um co-pilot from GitHub that's what it suggested so heat rate was actually an offline evaluation metric for evaluating search results right so offline meaning here we develop a system and we try to evalate the system to get a metric and this is what we do before we deploy this metric right so we see okay this method for search is better than this method for search because heat rate or Mr or some other metric is better so now we can deploy this and once we deploy there are other things that we can do to overate it offline so usually uh we do things like AB tests this is not something that we will cover here or some other experiments right so here now the idea is that um there are a lot of users coming to our website we split the traffic into certain proportions and a part of users go to our new method and we see how the new method uh is better or worse with respect to some business metrics um then yeah and this is something actually we will also talk a little bit about we will not specifically talk about AB test but in context of llms quite often what people use is user feedback like this thumbs up thumbs down this is something we will Implement too and also monitoring and online evaluation often comes together come together and monitoring is just observing the overall health uh Health uh the system right so this could be some performance metrics like it suggests like CPU uh and so on um and um but when it comes to uh llms here we care about things like um yeah user feedback and like how good the answer is uh and so on so let me talk a bit first like I'll make it just text text so let me first talk about um offline evolation because this is something we will cover Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'daeac2f999b54646a9971040e05fcfe9'},\n",
       " {'vid_id': 'yTKGSqkhgI4',\n",
       "  'title': 'LLM Zoomcamp 4.2 - Evaluation and Monitoring in LLMs',\n",
       "  'timecode': '06:08',\n",
       "  'text': \"first and this will be things uh like um uh cign similarity and I'll quickly explain how exactly we calculate it and then we can also use llm as a judge right so cosine similarity uh is uh seeing how close the answer that llm produces to the answer we expect and this is what we did in module 3 so remember in module 3 we created um test data set uh ground through data set where for each um Q&A uh for each so in our data said we have a lot of answers right so we have answers and answer right and then based on the answer we created a question right uh so we used anlm to create a question to this answer and now we can use this question to generate the answer to this question using anlm right so the first answer is original and then the second answer is an answer from nlm and then now we have this uh original answer and we have the answer from coming from llm and now we can compute cosign similarity between them right and then this will give us give us some understanding of how good the generated answer uh is cuz we know the original answer and now we can compare so what we can do is we can just take our ground prooof data set that we created previously and apply our rock this thing to the entire uh to each record in this data set and then compute the cosign and this will give us cign similarity or how like a good evaluation metric how good our rock is our Rock system is um using this metric the cosign similarity then another metric is uh that we will compute to this llm as a judge is we can ask an llm okay we have the original answer and we have the answer um coming from nlm we don't need to specify that it's nlm but here we can say this is the uh the original answer uh this is the answer that is coming Fromm how good uh like how similar they are LM as a uh judge right um and of course uh sometimes we don't have uh the original answer in some situations we only have a question so then we can also compute ask an LM okay this is a question this is an answer how good this answer answers this question right so this is what we are going to do in the next video so I did not expect this video to take um that long to introduce um to recap everything so I'm going to stop right now and then in the next one in the next video we are going to take care uh take a look at offline evaluation of rack systems and we will compute this things so see you soon\",\n",
       "  'timecode_text': 'Offline evaluation methods',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=yTKGSqkhgI4&t=368s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.2 - Evaluation and Monitoring in LLMs first and this will be things uh like um uh cign similarity and I'll quickly explain how exactly we calculate it and then we can also use llm as a judge right so cosine similarity uh is uh seeing how close the answer that llm produces to the answer we expect and this is what we did in module 3 so remember in module 3 we created um test data set uh ground through data set where for each um Q&A uh for each so in our data said we have a lot of answers right so we have answers and answer right and then based on the answer we created a question right uh so we used anlm to create a question to this answer and now we can use this question to generate the answer to this question using anlm right so the first answer is original and then the second answer is an answer from nlm and then now we have this uh original answer and we have the answer from coming from llm and now we can compute cosign similarity between them right and then this will give us give us some understanding of how good the generated answer uh is cuz we know the original answer and now we can compare so what we can do is we can just take our ground prooof data set that we created previously and apply our rock this thing to the entire uh to each record in this data set and then compute the cosign and this will give us cign similarity or how like a good evaluation metric how good our rock is our Rock system is um using this metric the cosign similarity then another metric is uh that we will compute to this llm as a judge is we can ask an llm okay we have the original answer and we have the answer um coming from nlm we don't need to specify that it's nlm but here we can say this is the uh the original answer uh this is the answer that is coming Fromm how good uh like how similar they are LM as a uh judge right um and of course uh sometimes we don't have uh the original answer in some situations we only have a question so then we can also compute ask an LM okay this is a question this is an answer how good this answer answers this question right so this is what we are going to do in the next video so I did not expect this video to take um that long to introduce um to recap everything so I'm going to stop right now and then in the next one in the next video we are going to take care uh take a look at offline evaluation of rack systems and we will compute this things so see you soon Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '475c0be7170527daa8e865694bc180b5'},\n",
       " {'vid_id': 'yTO5sRw6x78',\n",
       "  'title': 'LLM Zoomcamp 4.3 - Offline RAG Evaluation',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hi everyone welcome back this is llm zoom camp and we here talk about offline Rock evaluation so in the previous video I made a recap of everything we learned so far in this course and uh we also talked about evaluation and what uh we want to do now is we have this function Rock uh that consists of three components and we want to have a way of evaluating this entire think how good it is overall right so previously we used only evaluated only this part but now we want to evaluate the entire thing and for for doing that we can use the same data set we used previously and the synthetically generated data set we we created in the previous module for Ev relating the search function because uh here we can also evate the we for record for or an answer we generated a question and from the what that question now we can generate an answer and compute uh the similarity between original answer and uh answer that is provided by ANM this is what we're going to do and before I continue I want to give credits to Magdalena and I actually use your um materials for that it's simply I'm recording but uh the credit should go to here so let me start um so what I have done here in this notebook is uh uh I\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.',\n",
       "  'link': 'https://www.youtube.com/watch?v=yTO5sRw6x78&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.3 - Offline RAG Evaluation hi everyone welcome back this is llm zoom camp and we here talk about offline Rock evaluation so in the previous video I made a recap of everything we learned so far in this course and uh we also talked about evaluation and what uh we want to do now is we have this function Rock uh that consists of three components and we want to have a way of evaluating this entire think how good it is overall right so previously we used only evaluated only this part but now we want to evaluate the entire thing and for for doing that we can use the same data set we used previously and the synthetically generated data set we we created in the previous module for Ev relating the search function because uh here we can also evate the we for record for or an answer we generated a question and from the what that question now we can generate an answer and compute uh the similarity between original answer and uh answer that is provided by ANM this is what we're going to do and before I continue I want to give credits to Magdalena and I actually use your um materials for that it's simply I'm recording but uh the credit should go to here so let me start um so what I have done here in this notebook is uh uh I In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.\",\n",
       "  'id': '7efcb62560d74d66b3639208a62470ce'},\n",
       " {'vid_id': 'yTO5sRw6x78',\n",
       "  'title': 'LLM Zoomcamp 4.3 - Offline RAG Evaluation',\n",
       "  'timecode': '01:27',\n",
       "  'text': \"just did a little bit of preparation work so here uh in the previous module we created documents with IDs so here uh we have uh question answer and also ID for each of the records here right and the next thing I did is I um loaded the ground roof data set this is also something we uh generated in the previous modu when we talked about the evaluation of uh retrieval so here um the data set that we have is we have a question we have a course for which this question is and then we have the document that contains the answer right and then we I also created an index here for documents so I can quickly retrieve uh using the ID um I can quickly retrieve the actual document right and uh this is the the actual answer right so this is the question that we generated for this answer and this is the answer uh the next thing I did is I simply took the notebook we created in the vector search model and um according to our valuations the when we canate question and text and create an AIC for that that gave the best performance so this is what we're going to use here uh in this module uh for evate in the the quality um so the index I create contains this question text vector um and it contains this uh it uses this function right um so here I already indexed that so iated question and answer I created an eding for that and I put this anistic search and this is the function we use for retrieving um the results from elastic search it's the same function we used in uh the previous when we eval Vector search I simply copied it from there and this is how it works right so we have a question we have a course for which this question is and it gives back Five results right and then in module one we had this rack flow here it is I modified it slightly so now instead of a string as a query we have a dictionary because we have the question and the course so now it's a dictionary um so then I use this function for retrieving the results then I buil prompt and then I sent the prompt to nlm and this is the prompt it's exactly the same prompt as we used before right and um so here we get some answer for from an LM from theti rack function and this is the expected answer so this is an answer for which we generated the question and this is an actual answer from nlm so now we can compare the two and see how similar they are right um so for that we need to\",\n",
       "  'timecode_text': 'Starter code explanation',\n",
       "  'description': 'In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.',\n",
       "  'link': 'https://www.youtube.com/watch?v=yTO5sRw6x78&t=87s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.3 - Offline RAG Evaluation just did a little bit of preparation work so here uh in the previous module we created documents with IDs so here uh we have uh question answer and also ID for each of the records here right and the next thing I did is I um loaded the ground roof data set this is also something we uh generated in the previous modu when we talked about the evaluation of uh retrieval so here um the data set that we have is we have a question we have a course for which this question is and then we have the document that contains the answer right and then we I also created an index here for documents so I can quickly retrieve uh using the ID um I can quickly retrieve the actual document right and uh this is the the actual answer right so this is the question that we generated for this answer and this is the answer uh the next thing I did is I simply took the notebook we created in the vector search model and um according to our valuations the when we canate question and text and create an AIC for that that gave the best performance so this is what we're going to use here uh in this module uh for evate in the the quality um so the index I create contains this question text vector um and it contains this uh it uses this function right um so here I already indexed that so iated question and answer I created an eding for that and I put this anistic search and this is the function we use for retrieving um the results from elastic search it's the same function we used in uh the previous when we eval Vector search I simply copied it from there and this is how it works right so we have a question we have a course for which this question is and it gives back Five results right and then in module one we had this rack flow here it is I modified it slightly so now instead of a string as a query we have a dictionary because we have the question and the course so now it's a dictionary um so then I use this function for retrieving the results then I buil prompt and then I sent the prompt to nlm and this is the prompt it's exactly the same prompt as we used before right and um so here we get some answer for from an LM from theti rack function and this is the expected answer so this is an answer for which we generated the question and this is an actual answer from nlm so now we can compare the two and see how similar they are right um so for that we need to In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.\",\n",
       "  'id': 'b0eac2d939f2b3e6c1ee2b6cfbec1920'},\n",
       " {'vid_id': 'yTO5sRw6x78',\n",
       "  'title': 'LLM Zoomcamp 4.3 - Offline RAG Evaluation',\n",
       "  'timecode': '04:30',\n",
       "  'text': \"create um anting so I'll just copy this uh I'll create I'll call answer original right so this is original answer uh right and then [Music] answer then what we do is we generate vectors uh Vector LM so model and code answer LM and then uh we have a vector for the original answer again model and code and now we can compute cosign similarity which simply is a DOT product between these two right and and this is the cosine similarity between this remember one is the maximal possible cosine similarity zero is minimal possible so similar uh zero means completely uh not similar one means identical right so here they are somewhat similar I'd say it's a rather High uh degree of similarity and we can see from the actual questions uh here yes sessions recorded if you miss fond um yeah so it's quite close right there they're quite close so what we want to do now is we want to uh generate an answer to each record to all the records in our ground to data set uh and then uh safety results and then compute the embeddings between the answer produced by n llm and an answer produced by uh the answer we expect and now we can see the mean metric and we can also look at the dis ution and other things just to understand how good our system performs in general and this is again offline evaluation this is something we do before we decide to roll this out of production uh or at least we this way we can uh like when we have a question is this prompt better than this prompt right or is this uh llm better than this one we at least have a way uh for us to tell okay yeah this actually looks better right so this is what we're going to Implement right now um and for that\",\n",
       "  'timecode_text': 'Computing the cosine similarity metric',\n",
       "  'description': 'In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.',\n",
       "  'link': 'https://www.youtube.com/watch?v=yTO5sRw6x78&t=270s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.3 - Offline RAG Evaluation create um anting so I'll just copy this uh I'll create I'll call answer original right so this is original answer uh right and then [Music] answer then what we do is we generate vectors uh Vector LM so model and code answer LM and then uh we have a vector for the original answer again model and code and now we can compute cosign similarity which simply is a DOT product between these two right and and this is the cosine similarity between this remember one is the maximal possible cosine similarity zero is minimal possible so similar uh zero means completely uh not similar one means identical right so here they are somewhat similar I'd say it's a rather High uh degree of similarity and we can see from the actual questions uh here yes sessions recorded if you miss fond um yeah so it's quite close right there they're quite close so what we want to do now is we want to uh generate an answer to each record to all the records in our ground to data set uh and then uh safety results and then compute the embeddings between the answer produced by n llm and an answer produced by uh the answer we expect and now we can see the mean metric and we can also look at the dis ution and other things just to understand how good our system performs in general and this is again offline evaluation this is something we do before we decide to roll this out of production uh or at least we this way we can uh like when we have a question is this prompt better than this prompt right or is this uh llm better than this one we at least have a way uh for us to tell okay yeah this actually looks better right so this is what we're going to Implement right now um and for that In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.\",\n",
       "  'id': 'b5cbe8f7078f2771f586a56ae1db8608'},\n",
       " {'vid_id': 'yTO5sRw6x78',\n",
       "  'title': 'LLM Zoomcamp 4.3 - Offline RAG Evaluation',\n",
       "  'timecode': '07:00',\n",
       "  'text': \"uh what we need is uh I'll call answers and um so it will actually be um uh a list and I will go over so we have this ground roof uh uh data set so I'll just go over um this and for each question I'll generate an answer and I'll save it um in the list right okay so this will be uh for record in ground truth um so first uh let me actually do it uh add dictionary in case something happens and I don't want to recalculate the entire thing again so I will just stop um I'll just continue from where it broke um so for that um I'll just use in EN numerate uh so here when I do enumerate I get the index for each of the records and then here if I is in uh in answers continue so we don't redo the calculation and here I um we'll do first answer this will be LM um record so record is uh this one right this is the answer then I'll call it answer llm then answer original will be um so first let me have document ID doc ID will be record document and um so we have um original document I'll use document index to retrieve it right so then original answer would be original document um how was it text right uh and then then uh I think this is what we need so um have answer LM which is this thing then we have answer uh original which is this thing and we have uh document which is this thing and I will now run it and note I I use GPD 4 all and it can be quite expensive because we uh remember it's not a a small data set so you need to make sure you either have some money I don't know how much it will cost I'll tell you at the end and of course you do not need to do it yourself uh I'll share the results um but yeah you can also use GPD 3.5 um it should be cheaper uh here I'll just use GPT for all um okay so let me actually check if it works uh okay so it does not uh probably something is uh broken um I also want to put it in a tqdm thing just to see um the progress um let's see what we have in Rec I did not plan to do live buing but um yeah so it should actually be Rock right not all that's why it um it doesn't work see why it looks so weird um okay so now it's Computing um yeah so like because in number eight let me actually stop it I don't like it did I stop it like why it looks so strange yeah so I think maybe if I do it this way it will work let's see so actually now answer should contain already some questions right if we look at uh answer values this is more interesting right so we already have uh uh the answer from llm the answer from uh original and now this is something we can compare but let's finish um executing it okay I don't know why it looks okay so different anyways so now I think it will take some time W that's long um I also don't want to do it in parallel so of course course I can speed this uh things up by uh using multi-threading and then sending multiple requests at the same time but what can happen is and what happened to me before is I quickly reach the rate limit and they start blocking me so uh in this particular situation it's better to be uh slow right or they block me so let me wait till it finishes uh yeah I hope uh I have enough money in my uh um open a platform but we'll see so I'm going to stop recording and then I'll record another video and then just put them together later so see you soon so it finished even though it says\",\n",
       "  'timecode_text': 'Getting answers for the ground truth dataset with gpt-4o',\n",
       "  'description': 'In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.',\n",
       "  'link': 'https://www.youtube.com/watch?v=yTO5sRw6x78&t=420s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.3 - Offline RAG Evaluation uh what we need is uh I'll call answers and um so it will actually be um uh a list and I will go over so we have this ground roof uh uh data set so I'll just go over um this and for each question I'll generate an answer and I'll save it um in the list right okay so this will be uh for record in ground truth um so first uh let me actually do it uh add dictionary in case something happens and I don't want to recalculate the entire thing again so I will just stop um I'll just continue from where it broke um so for that um I'll just use in EN numerate uh so here when I do enumerate I get the index for each of the records and then here if I is in uh in answers continue so we don't redo the calculation and here I um we'll do first answer this will be LM um record so record is uh this one right this is the answer then I'll call it answer llm then answer original will be um so first let me have document ID doc ID will be record document and um so we have um original document I'll use document index to retrieve it right so then original answer would be original document um how was it text right uh and then then uh I think this is what we need so um have answer LM which is this thing then we have answer uh original which is this thing and we have uh document which is this thing and I will now run it and note I I use GPD 4 all and it can be quite expensive because we uh remember it's not a a small data set so you need to make sure you either have some money I don't know how much it will cost I'll tell you at the end and of course you do not need to do it yourself uh I'll share the results um but yeah you can also use GPD 3.5 um it should be cheaper uh here I'll just use GPT for all um okay so let me actually check if it works uh okay so it does not uh probably something is uh broken um I also want to put it in a tqdm thing just to see um the progress um let's see what we have in Rec I did not plan to do live buing but um yeah so it should actually be Rock right not all that's why it um it doesn't work see why it looks so weird um okay so now it's Computing um yeah so like because in number eight let me actually stop it I don't like it did I stop it like why it looks so strange yeah so I think maybe if I do it this way it will work let's see so actually now answer should contain already some questions right if we look at uh answer values this is more interesting right so we already have uh uh the answer from llm the answer from uh original and now this is something we can compare but let's finish um executing it okay I don't know why it looks okay so different anyways so now I think it will take some time W that's long um I also don't want to do it in parallel so of course course I can speed this uh things up by uh using multi-threading and then sending multiple requests at the same time but what can happen is and what happened to me before is I quickly reach the rate limit and they start blocking me so uh in this particular situation it's better to be uh slow right or they block me so let me wait till it finishes uh yeah I hope uh I have enough money in my uh um open a platform but we'll see so I'm going to stop recording and then I'll record another video and then just put them together later so see you soon so it finished even though it says In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.\",\n",
       "  'id': 'bc7d980b442134f84047f8ce38977eaf'},\n",
       " {'vid_id': 'yTO5sRw6x78',\n",
       "  'title': 'LLM Zoomcamp 4.3 - Offline RAG Evaluation',\n",
       "  'timecode': '13:17',\n",
       "  'text': \"1 hour it took two or even almost three hours um and one of the things I did I put answers in a separate cell when we Define the variable and the reason for that is um it broke a few times when I was executing it so I needed to re execute and I didn't want to erase all the answers and right now uh if we take a look at the answers uh yeah so for each of the records we have an answer and this is how it looks like maybe I should not do this yeah so basically for each of the indices from the ground Ruth we have an answer and what we're interested in is values from here and I want to put now this values in I'm thinking what's the best way of actually saving it as Json or as um a CSC file uh maybe let's uh let's save it as a CSC file so I'll import pandas and we'll create a uh that frame so this answer from them answer original answer um and we will see um yeah interesting that it also includes some names um so um this is the our document and I'll call it uh GPT for o right and and let's save it um actually I want to take a look at the sample uh let's take a look at five documents and I'll uh turn it into a dictionary uh just to see what's there right so on midterm at cstone projects and meant to be solo projects uh yes student works on their own midterm and cstone projects as they meant to be slow projects and I realize that one of the things that could have been useful here is the question so let me actually um includeed here um so um how\",\n",
       "  'timecode_text': 'Looking at results',\n",
       "  'description': 'In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.',\n",
       "  'link': 'https://www.youtube.com/watch?v=yTO5sRw6x78&t=797s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.3 - Offline RAG Evaluation 1 hour it took two or even almost three hours um and one of the things I did I put answers in a separate cell when we Define the variable and the reason for that is um it broke a few times when I was executing it so I needed to re execute and I didn't want to erase all the answers and right now uh if we take a look at the answers uh yeah so for each of the records we have an answer and this is how it looks like maybe I should not do this yeah so basically for each of the indices from the ground Ruth we have an answer and what we're interested in is values from here and I want to put now this values in I'm thinking what's the best way of actually saving it as Json or as um a CSC file uh maybe let's uh let's save it as a CSC file so I'll import pandas and we'll create a uh that frame so this answer from them answer original answer um and we will see um yeah interesting that it also includes some names um so um this is the our document and I'll call it uh GPT for o right and and let's save it um actually I want to take a look at the sample uh let's take a look at five documents and I'll uh turn it into a dictionary uh just to see what's there right so on midterm at cstone projects and meant to be solo projects uh yes student works on their own midterm and cstone projects as they meant to be slow projects and I realize that one of the things that could have been useful here is the question so let me actually um includeed here um so um how In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.\",\n",
       "  'id': '428d5be649132f01114a6f6366d0a8b0'},\n",
       " {'vid_id': 'yTO5sRw6x78',\n",
       "  'title': 'LLM Zoomcamp 4.3 - Offline RAG Evaluation',\n",
       "  'timecode': '15:38',\n",
       "  'text': \"does our ground roof look like so maybe this is something we can also include in the in here um so this is record uh question here record right then record course and as for the document we already have the document because it could have been quite useful for this uh record course what I can do now in actually in retrospect is uh prepare the results I'll call it uh results uh GPD 40 um first of all I'll um initialize it with u so I want to make sure that the\",\n",
       "  'timecode_text': 'Adding more information',\n",
       "  'description': 'In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.',\n",
       "  'link': 'https://www.youtube.com/watch?v=yTO5sRw6x78&t=938s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.3 - Offline RAG Evaluation does our ground roof look like so maybe this is something we can also include in the in here um so this is record uh question here record right then record course and as for the document we already have the document because it could have been quite useful for this uh record course what I can do now in actually in retrospect is uh prepare the results I'll call it uh results uh GPD 40 um first of all I'll um initialize it with u so I want to make sure that the In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.\",\n",
       "  'id': '31aa55d6ee3e128fc89ce70bd7d87d31'},\n",
       " {'vid_id': 'yTO5sRw6x78',\n",
       "  'title': 'LLM Zoomcamp 4.3 - Offline RAG Evaluation',\n",
       "  'timecode': '16:40',\n",
       "  'text': \"order is the same here if we look at these answers right so we have the this thing index and while they are in order yeah I think it should be fine so let's uh uh so I want to create an empty list first and then for each of the indices there I want to put the answer there and also put the original data so it will be um first we create an empty list with um just NS there uh this is something I should have thought before yeah so you see there is nothing but nuns and now we go over um I and value in um answer items and then here uh I equals value and then also um what we will do I think we have um so in dictionary I don't remember so if it's a dictionary I think we have a function that is called extend or update upate yeah that takes another dictionary so this is something we will use now so we'll have a create a copy uh does dictionary actually have copy method yeah so we create a copy first too much stuff uh copy and then we update with um ground truth okay so hopefully this will work now great um yeah well maybe like if you don't understand what's happening here um don't worry ask chpt to explain uh\",\n",
       "  'timecode_text': 'Putting the results to list (optional)',\n",
       "  'description': 'In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.',\n",
       "  'link': 'https://www.youtube.com/watch?v=yTO5sRw6x78&t=1000s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.3 - Offline RAG Evaluation order is the same here if we look at these answers right so we have the this thing index and while they are in order yeah I think it should be fine so let's uh uh so I want to create an empty list first and then for each of the indices there I want to put the answer there and also put the original data so it will be um first we create an empty list with um just NS there uh this is something I should have thought before yeah so you see there is nothing but nuns and now we go over um I and value in um answer items and then here uh I equals value and then also um what we will do I think we have um so in dictionary I don't remember so if it's a dictionary I think we have a function that is called extend or update upate yeah that takes another dictionary so this is something we will use now so we'll have a create a copy uh does dictionary actually have copy method yeah so we create a copy first too much stuff uh copy and then we update with um ground truth okay so hopefully this will work now great um yeah well maybe like if you don't understand what's happening here um don't worry ask chpt to explain uh In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.\",\n",
       "  'id': '1700527d6658db583affdc95538dcc5f'},\n",
       " {'vid_id': 'yTO5sRw6x78',\n",
       "  'title': 'LLM Zoomcamp 4.3 - Offline RAG Evaluation',\n",
       "  'timecode': '19:00',\n",
       "  'text': \"but right now we have it in a better format and we also have the question and the course which now I can use here to see so the question was what adjustment can improve my model accuracy in this homework and yeah so this these are the answers uh the answer from llm and the answer the original one so yeah looks like there are some good results and we can now save the results to CSV what's happening to CSV and they'll call I'll create a folder uh data and I'll save data uh results GPT or o CSV index FS all right so this is what we have in this file so we have uh answer Fromm original answer document question and course cool um now we can compute cign similarity but even before we do that\",\n",
       "  'timecode_text': 'Saving the results',\n",
       "  'description': 'In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.',\n",
       "  'link': 'https://www.youtube.com/watch?v=yTO5sRw6x78&t=1140s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.3 - Offline RAG Evaluation but right now we have it in a better format and we also have the question and the course which now I can use here to see so the question was what adjustment can improve my model accuracy in this homework and yeah so this these are the answers uh the answer from llm and the answer the original one so yeah looks like there are some good results and we can now save the results to CSV what's happening to CSV and they'll call I'll create a folder uh data and I'll save data uh results GPT or o CSV index FS all right so this is what we have in this file so we have uh answer Fromm original answer document question and course cool um now we can compute cign similarity but even before we do that In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.\",\n",
       "  'id': '72a4ab5b1b4cff0375a9651d4964f055'},\n",
       " {'vid_id': 'yTO5sRw6x78',\n",
       "  'title': 'LLM Zoomcamp 4.3 - Offline RAG Evaluation',\n",
       "  'timecode': '20:27',\n",
       "  'text': \"what I want to do is also uh first of all show you how to speed this up and remember I said that this is something we shouldn't do uh cuz we quickly hit the rate limits but maybe if you already has use some corporate uh um key then probably your rate limits are much higher than uh or much slower you can execute more queries than I can and I want to actually try use another model I want to use GPT uh 3.5 turbo which should be cheaper and faster and the rate limits there are not as strict as for gbt 40 so we can uh also at the same time time compare the results from GPT 40 with GPT 3.5 and by the way I also promised you to tell you the cost so the cost for this uh for executing for evaluating this was almost 10 bucks so already a bit costly so what we can do is also can compare now um with uh the cost for 3.5 and if we see that the difference is not that High um significant between these two in terms of uh quality but maybe 3.5 is cheaper and faster uh maybe this is what we can use for for us right um so so uh let us modify the code a little bit so here we will add model and default one B GPT 40 and here for rock we will also have model which by default will be dpd4 all and here we pass model so let me execute this and this um okay and now let us check uh Rock for um uh where do we use it here and I'll call this section uh evaluating uh 3.5 GPT 3.5 um model uh GPT um I actually don't remember so let's see GPT 40 so this one GPT 3.5 turbo GPT 3.5 turbo right um yeah so it works um hopefully it was turbo not uh that one what if I put some nonsense here or not nonsource TPT this one yeah okay good so it actually works um so now let us uh do the same thing and I promise to show how to speed this up and I have this handy function that I copy from Project to project maybe you have seen it in the uh starter notebook\",\n",
       "  'timecode_text': 'Using gpt-3.5-turbo',\n",
       "  'description': 'In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.',\n",
       "  'link': 'https://www.youtube.com/watch?v=yTO5sRw6x78&t=1227s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.3 - Offline RAG Evaluation what I want to do is also uh first of all show you how to speed this up and remember I said that this is something we shouldn't do uh cuz we quickly hit the rate limits but maybe if you already has use some corporate uh um key then probably your rate limits are much higher than uh or much slower you can execute more queries than I can and I want to actually try use another model I want to use GPT uh 3.5 turbo which should be cheaper and faster and the rate limits there are not as strict as for gbt 40 so we can uh also at the same time time compare the results from GPT 40 with GPT 3.5 and by the way I also promised you to tell you the cost so the cost for this uh for executing for evaluating this was almost 10 bucks so already a bit costly so what we can do is also can compare now um with uh the cost for 3.5 and if we see that the difference is not that High um significant between these two in terms of uh quality but maybe 3.5 is cheaper and faster uh maybe this is what we can use for for us right um so so uh let us modify the code a little bit so here we will add model and default one B GPT 40 and here for rock we will also have model which by default will be dpd4 all and here we pass model so let me execute this and this um okay and now let us check uh Rock for um uh where do we use it here and I'll call this section uh evaluating uh 3.5 GPT 3.5 um model uh GPT um I actually don't remember so let's see GPT 40 so this one GPT 3.5 turbo GPT 3.5 turbo right um yeah so it works um hopefully it was turbo not uh that one what if I put some nonsense here or not nonsource TPT this one yeah okay good so it actually works um so now let us uh do the same thing and I promise to show how to speed this up and I have this handy function that I copy from Project to project maybe you have seen it in the uh starter notebook In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.\",\n",
       "  'id': '953454b7253ce1c307dc26d37fc52053'},\n",
       " {'vid_id': 'yTO5sRw6x78',\n",
       "  'title': 'LLM Zoomcamp 4.3 - Offline RAG Evaluation',\n",
       "  'timecode': '23:42',\n",
       "  'text': \"for the competition we also used it there so here what it does uh so this is a very convenient function for executing things in parallel for parallel processing um again I don't want to spend too much time on that so if you want to understand how it works ask chpt the important thing here is the degree of parallelization so here we will uh use three six concurrent threats in order to execute our uh function right and this thing expects uh three parameters so this is what we will use so first is the pool that is going to be executing uh this in multiple threats but it doesn't matter what this thing is like this is the think that will do multi threadit execution parallel execution so this is the sequence on which we want to invoke the function and this is the function we want to invoke so as for the function this is what we put inside the um like this is what was inside right inside our Loop so I'll call it process uh record and then uh here we have record uh we have model which is um this one right so then uh model equals model uh document ID this is it uh uh yeah and this is this is our result so let's see if it works so it just return uh the whole thing for a record so let's say this is our record so now we get there um like everything we need and now I just want to execute that this function maybe let me use a different cell I don't know if uh six is actually maybe too high but let's see uh and I hope that actually parallel execution will work and we not have any problems cuz like one of the dangersous here is if it breaks then we have to redo the whole thing so this sequential execution is slower but more reliable in a sense if something breaks then we can continue uh but here this one will um actually ruin the whole thing so here we have our pool the sequence is this ground Ruth and the function that we want to apply is process record and we call the results um uh GPT 3.5 so let's execute that I hope it should be fast well so let's see how fast it finishes I'll put it on pause and hopefully it um finishes without errors so see you soon so it finished only in 6 minutes 32\",\n",
       "  'timecode_text': 'Parallel processing',\n",
       "  'description': 'In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.',\n",
       "  'link': 'https://www.youtube.com/watch?v=yTO5sRw6x78&t=1422s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.3 - Offline RAG Evaluation for the competition we also used it there so here what it does uh so this is a very convenient function for executing things in parallel for parallel processing um again I don't want to spend too much time on that so if you want to understand how it works ask chpt the important thing here is the degree of parallelization so here we will uh use three six concurrent threats in order to execute our uh function right and this thing expects uh three parameters so this is what we will use so first is the pool that is going to be executing uh this in multiple threats but it doesn't matter what this thing is like this is the think that will do multi threadit execution parallel execution so this is the sequence on which we want to invoke the function and this is the function we want to invoke so as for the function this is what we put inside the um like this is what was inside right inside our Loop so I'll call it process uh record and then uh here we have record uh we have model which is um this one right so then uh model equals model uh document ID this is it uh uh yeah and this is this is our result so let's see if it works so it just return uh the whole thing for a record so let's say this is our record so now we get there um like everything we need and now I just want to execute that this function maybe let me use a different cell I don't know if uh six is actually maybe too high but let's see uh and I hope that actually parallel execution will work and we not have any problems cuz like one of the dangersous here is if it breaks then we have to redo the whole thing so this sequential execution is slower but more reliable in a sense if something breaks then we can continue uh but here this one will um actually ruin the whole thing so here we have our pool the sequence is this ground Ruth and the function that we want to apply is process record and we call the results um uh GPT 3.5 so let's execute that I hope it should be fast well so let's see how fast it finishes I'll put it on pause and hopefully it um finishes without errors so see you soon so it finished only in 6 minutes 32 In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.\",\n",
       "  'id': '87969ee145d8a602955e51f160a6794a'},\n",
       " {'vid_id': 'yTO5sRw6x78',\n",
       "  'title': 'LLM Zoomcamp 4.3 - Offline RAG Evaluation',\n",
       "  'timecode': '27:10',\n",
       "  'text': \"seconds uh if you compare it to 3 hours of chat of GPT 40 that's a lot faster of course like we did it in uh multi- threading like in uh in parallel but still like it's a lot faster right and then I also want to check the price uh so let's see uh and it costed 79 so like 10 times even more than 10 times cheaper um so if it's 10 times faster and 10 times cheaper um so I'm wondering how much performance we uh lose when we go to 3.5 and this is actually one of the things we can now have an answer to CU we have a way of evaluating um uh the quality of our response and by Computing the cign similarity but it will have to wait a little bit cuz I realized that this video is already quite long so what I will do right now is I will sa safe the results and then in the next video we will compute the cign similarity and we will compute the other metric uh this LM as a judge okay so this is what we did with uh uh where it is okay so this is what we did before um so I'll just call it 3.5 3.5 and here 3.5 and this is the results and I'll again look at um edit um okay yeah I don't know what is that um but this is what we have right um so the results are saved now um yeah here we did quite a lot of things so what we did is we used our gold standard data set uh ground through data set and then um there we know uh the original answer and we know the question we generated for this answer so now we can compare the original answer and the answer that we generated answering the question that sounds very confusing but like basically we have the original answer the answer generated with an LM now we can compute and see how similar they are and this is something we will do next so see you soon\",\n",
       "  'timecode_text': 'Results for gpt-3.5-turbo',\n",
       "  'description': 'In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.',\n",
       "  'link': 'https://www.youtube.com/watch?v=yTO5sRw6x78&t=1630s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.3 - Offline RAG Evaluation seconds uh if you compare it to 3 hours of chat of GPT 40 that's a lot faster of course like we did it in uh multi- threading like in uh in parallel but still like it's a lot faster right and then I also want to check the price uh so let's see uh and it costed 79 so like 10 times even more than 10 times cheaper um so if it's 10 times faster and 10 times cheaper um so I'm wondering how much performance we uh lose when we go to 3.5 and this is actually one of the things we can now have an answer to CU we have a way of evaluating um uh the quality of our response and by Computing the cign similarity but it will have to wait a little bit cuz I realized that this video is already quite long so what I will do right now is I will sa safe the results and then in the next video we will compute the cign similarity and we will compute the other metric uh this LM as a judge okay so this is what we did with uh uh where it is okay so this is what we did before um so I'll just call it 3.5 3.5 and here 3.5 and this is the results and I'll again look at um edit um okay yeah I don't know what is that um but this is what we have right um so the results are saved now um yeah here we did quite a lot of things so what we did is we used our gold standard data set uh ground through data set and then um there we know uh the original answer and we know the question we generated for this answer so now we can compare the original answer and the answer that we generated answering the question that sounds very confusing but like basically we have the original answer the answer generated with an LM now we can compute and see how similar they are and this is something we will do next so see you soon In this video, we evaluate the quality of our RAG. We compare answers generated by LLM with original answers, compute similarity, and assess overall performance before production.\",\n",
       "  'id': '27d731b6e106a4eeca2919cbc5dd06d3'},\n",
       " {'vid_id': 'LlXclbD3pms',\n",
       "  'title': 'LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity',\n",
       "  'timecode': '00:00',\n",
       "  'text': 'hi everyone welcome back this is LM Zoom camp and here we talk about offline Rock evaluation and um by offline here I mean metrix we calculate offline and one of such metrics is by offline again here this is before we deploy to production so we calculate it in our test environment or in our development environment and once we see the metric and we think these are good metrics now we can push to production and do online evaluation and we already um in the previous video here I have the notebook that we created there um so here we loaded the ground roof data we generated previously and then for each of the question there we know the original answer we generated answer from the llm using our rck function here we did it for two models 3.5 and 40 and we store all the results in a CCV file so now we can load the CCV files I simply will continue uh using this notebook but if you what you can do is just load these files and now we can compute first metric cosign similarity and uh the reason I you I use it this way I call it AQA cign similarity',\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.',\n",
       "  'link': 'https://www.youtube.com/watch?v=LlXclbD3pms&t=0s',\n",
       "  'text_vector': 'LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity hi everyone welcome back this is LM Zoom camp and here we talk about offline Rock evaluation and um by offline here I mean metrix we calculate offline and one of such metrics is by offline again here this is before we deploy to production so we calculate it in our test environment or in our development environment and once we see the metric and we think these are good metrics now we can push to production and do online evaluation and we already um in the previous video here I have the notebook that we created there um so here we loaded the ground roof data we generated previously and then for each of the question there we know the original answer we generated answer from the llm using our rck function here we did it for two models 3.5 and 40 and we store all the results in a CCV file so now we can load the CCV files I simply will continue uh using this notebook but if you what you can do is just load these files and now we can compute first metric cosign similarity and uh the reason I you I use it this way I call it AQA cign similarity In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.',\n",
       "  'id': 'd0e4ec400f55d6a0567594c3c0bde278'},\n",
       " {'vid_id': 'LlXclbD3pms',\n",
       "  'title': 'LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity',\n",
       "  'timecode': '01:30',\n",
       "  'text': \"I don't know if it's a thing I just made it up so a is the original uh answer to the question from that original answer we synthetically generated a question right and then from this question using this question that we synthetically generated we get an answer from L an llm and now we can compute cign between a original and a thingy the derived answer or whatever like the answer generated by nlm uh so we can compute this AQA cosign similarity yeah well so I I just made it up but I like how it sounds um so this is what we are going to do right now and we will do it for uh two data sets um for two models for GPT 4 and for GPT uh 3.5 so let's start with GPT Forum so this is uh this is what we have and right now we can add another column here uh the column will be so first let me actually turn it back to um uh\",\n",
       "  'timecode_text': 'AQA cosine similarity',\n",
       "  'description': 'In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.',\n",
       "  'link': 'https://www.youtube.com/watch?v=LlXclbD3pms&t=90s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity I don't know if it's a thing I just made it up so a is the original uh answer to the question from that original answer we synthetically generated a question right and then from this question using this question that we synthetically generated we get an answer from L an llm and now we can compute cign between a original and a thingy the derived answer or whatever like the answer generated by nlm uh so we can compute this AQA cosign similarity yeah well so I I just made it up but I like how it sounds um so this is what we are going to do right now and we will do it for uh two data sets um for two models for GPT 4 and for GPT uh 3.5 so let's start with GPT Forum so this is uh this is what we have and right now we can add another column here uh the column will be so first let me actually turn it back to um uh In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.\",\n",
       "  'id': 'd654890717de083a727ed5df3087b7de'},\n",
       " {'vid_id': 'LlXclbD3pms',\n",
       "  'title': 'LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity',\n",
       "  'timecode': '02:50',\n",
       "  'text': \"dictionaries uh Records Orient records so you can pretend like if you if you do it uh in in a separate notebook you can just load it and turn it into dictionaries if you continue in the same notebook as I did before like there is no reason to turn it into dictionaries again CU here we recently turned it from dictionaries to a data frame but anyways um a okay so here we have the results right and now for each of these records we can compute similarity um I'll call it first let's let's just uh do it like that so we have a record um and we have um I think we already have some code for that right uh did I miss it I think I did yeah here right so answer the original answer would be how does record look like so have answer or and then answer llm is this one answer LM and we compute the similarity and this is what we have so now we can just call it compute similarity or record and this is what we return and we can simply execute it for all the records um so yeah uh similarity uh so let's execute it for record in um we'll use tqdm again uh and um similarity compute similarity for the record and we append this similarity to the results right um so it might take some time uh of course there are ways to speed it up because we can think that uh the original answer um actually there are five records with the same answer in our uh data set right because uh from each record from each answer we generated five questions right there are five different questions for the same answer uh so potentially we can speed it up by pre-calculating the embeddings for the original answer and then reusing them I'll just keep things simpler and do do it like that uh and now I put it on pause and then let's wait till it finishes okay it finished to 5 minutes\",\n",
       "  'timecode_text': 'Computing cosine similarity for gpt-4o',\n",
       "  'description': 'In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.',\n",
       "  'link': 'https://www.youtube.com/watch?v=LlXclbD3pms&t=170s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity dictionaries uh Records Orient records so you can pretend like if you if you do it uh in in a separate notebook you can just load it and turn it into dictionaries if you continue in the same notebook as I did before like there is no reason to turn it into dictionaries again CU here we recently turned it from dictionaries to a data frame but anyways um a okay so here we have the results right and now for each of these records we can compute similarity um I'll call it first let's let's just uh do it like that so we have a record um and we have um I think we already have some code for that right uh did I miss it I think I did yeah here right so answer the original answer would be how does record look like so have answer or and then answer llm is this one answer LM and we compute the similarity and this is what we have so now we can just call it compute similarity or record and this is what we return and we can simply execute it for all the records um so yeah uh similarity uh so let's execute it for record in um we'll use tqdm again uh and um similarity compute similarity for the record and we append this similarity to the results right um so it might take some time uh of course there are ways to speed it up because we can think that uh the original answer um actually there are five records with the same answer in our uh data set right because uh from each record from each answer we generated five questions right there are five different questions for the same answer uh so potentially we can speed it up by pre-calculating the embeddings for the original answer and then reusing them I'll just keep things simpler and do do it like that uh and now I put it on pause and then let's wait till it finishes okay it finished to 5 minutes In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.\",\n",
       "  'id': 'f466ff0aef6d9f2209df669ffb030016'},\n",
       " {'vid_id': 'LlXclbD3pms',\n",
       "  'title': 'LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity',\n",
       "  'timecode': '06:00',\n",
       "  'text': \"and now we can compute the mean score so I'll use uh oh we can use pandas for that um so what I'll do is we already have this data frame right so I'll add cosign here and we'll assign this thing there so now for each uh answer answer pair we have cign similarity for some reason it's sometimes negative but uh yeah so probably here they are very nonsimilar and yeah I'm actually curious to see what is there um dictionary so original answer the purpose of this yeah so here we have like bad paring um and H okay yeah of course that's why I like uh uh yeah it's not it's not um good data anyways so let us actually see uh how good we do on average so I'll use describe uh we don't have describe yes crap yeah so here it gives us some statistics the most interesting one is the mean um so this is how good we are in general uh we can also look at the median and other things um maybe even median is more informative than the mean but in general we can see that um if we select one and stick to it then this could be our metric um what can also be interesting is uh looking at the\",\n",
       "  'timecode_text': 'Evaluation results for gpt-4o',\n",
       "  'description': 'In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.',\n",
       "  'link': 'https://www.youtube.com/watch?v=LlXclbD3pms&t=360s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity and now we can compute the mean score so I'll use uh oh we can use pandas for that um so what I'll do is we already have this data frame right so I'll add cosign here and we'll assign this thing there so now for each uh answer answer pair we have cign similarity for some reason it's sometimes negative but uh yeah so probably here they are very nonsimilar and yeah I'm actually curious to see what is there um dictionary so original answer the purpose of this yeah so here we have like bad paring um and H okay yeah of course that's why I like uh uh yeah it's not it's not um good data anyways so let us actually see uh how good we do on average so I'll use describe uh we don't have describe yes crap yeah so here it gives us some statistics the most interesting one is the mean um so this is how good we are in general uh we can also look at the median and other things um maybe even median is more informative than the mean but in general we can see that um if we select one and stick to it then this could be our metric um what can also be interesting is uh looking at the In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.\",\n",
       "  'id': '50e8e51b694ba4a6df71a72fdc785cba'},\n",
       " {'vid_id': 'LlXclbD3pms',\n",
       "  'title': 'LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity',\n",
       "  'timecode': '07:58',\n",
       "  'text': \"distribution uh I don't know if I have installed have I if I have caborn this is a library for visualization see born uh we can look at the distribution of how it looks like in general um so let me install it and then we can plot and see how it looks like but I would just go I would select uh the mean um cosign similarity as the overall metric we want to use for evaluating the performance of our um llm and this is what we can look at when we compare to models um I don't know why it's not installing uh yeah actually it was doing something I just it looks like it's installed it I don't know let me check ver uh this plot so I'll do that okay so this is how it looks like so we see that uh it's like most of the records have actually quite High uh value here so which is a good thing okay so now this is GPT for all and now now we will see how worse GPT 3.5 is so we will do the same thing now uh\",\n",
       "  'timecode_text': 'Looking at the scores distribution',\n",
       "  'description': 'In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.',\n",
       "  'link': 'https://www.youtube.com/watch?v=LlXclbD3pms&t=478s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity distribution uh I don't know if I have installed have I if I have caborn this is a library for visualization see born uh we can look at the distribution of how it looks like in general um so let me install it and then we can plot and see how it looks like but I would just go I would select uh the mean um cosign similarity as the overall metric we want to use for evaluating the performance of our um llm and this is what we can look at when we compare to models um I don't know why it's not installing uh yeah actually it was doing something I just it looks like it's installed it I don't know let me check ver uh this plot so I'll do that okay so this is how it looks like so we see that uh it's like most of the records have actually quite High uh value here so which is a good thing okay so now this is GPT for all and now now we will see how worse GPT 3.5 is so we will do the same thing now uh In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.\",\n",
       "  'id': 'b15c12e5ed36f6373c1756a55f21eefe'},\n",
       " {'vid_id': 'LlXclbD3pms',\n",
       "  'title': 'LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity',\n",
       "  'timecode': '09:34',\n",
       "  'text': \"so first this is GPT or0 right and now let's do the same for gpt3 and GPT 3.5 turbo right so I will copy this code bt35 um wait um yeah I don't read it from uh file system here cuz I already have it in memory but this is something you can do if you're just loading it and here let's have let's do it like that similarity 3.5 okay I hope I did not forget anything so let's execute that again it will take some time so I'll put it on pause and come back when it finishes yeah so it finished and now we simply copy uh the same code put it in one cell so it's easier to copy and of course I use uh this thing and I use this thing\",\n",
       "  'timecode_text': 'Computing cosine similarity for gpt-3.5-turbo',\n",
       "  'description': 'In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.',\n",
       "  'link': 'https://www.youtube.com/watch?v=LlXclbD3pms&t=574s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity so first this is GPT or0 right and now let's do the same for gpt3 and GPT 3.5 turbo right so I will copy this code bt35 um wait um yeah I don't read it from uh file system here cuz I already have it in memory but this is something you can do if you're just loading it and here let's have let's do it like that similarity 3.5 okay I hope I did not forget anything so let's execute that again it will take some time so I'll put it on pause and come back when it finishes yeah so it finished and now we simply copy uh the same code put it in one cell so it's easier to copy and of course I use uh this thing and I use this thing In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.\",\n",
       "  'id': '9137be55f03165020dcad1de41d4a96e'},\n",
       " {'vid_id': 'LlXclbD3pms',\n",
       "  'title': 'LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity',\n",
       "  'timecode': '11:00',\n",
       "  'text': \"and from what we can see the mean is it is it lower like 2% uh what is also interesting is looking at uh this distribution so let's actually put both of them together at the same plot um so this one and this one so we can compare them I'll also add uh label and this one is 40 and this one is 3.5 uh I'll also need to import m m plot lip because I want to show the legend import um plot lip uh it's been a while since I imported this pip plot as PLT something like this okay\",\n",
       "  'timecode_text': 'Evaluation results for gpt-3.5-turbo',\n",
       "  'description': 'In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.',\n",
       "  'link': 'https://www.youtube.com/watch?v=LlXclbD3pms&t=660s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity and from what we can see the mean is it is it lower like 2% uh what is also interesting is looking at uh this distribution so let's actually put both of them together at the same plot um so this one and this one so we can compare them I'll also add uh label and this one is 40 and this one is 3.5 uh I'll also need to import m m plot lip because I want to show the legend import um plot lip uh it's been a while since I imported this pip plot as PLT something like this okay In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.\",\n",
       "  'id': 'da4b939826a9da44b891b3eb60f61f70'},\n",
       " {'vid_id': 'LlXclbD3pms',\n",
       "  'title': 'LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity',\n",
       "  'timecode': '12:00',\n",
       "  'text': \"Legend okay so here the blue one is um um 40 and this 3.5 like I would say they are super super similar so from this distribution I would say that like there's probably no significant difference yeah sometimes there are answers that are worse but the majority of good answers are here and we can see this by looking at the median and mean which are almost the same so when it comes to median yeah in both cases is 2% worse um again like this is it might depend on your specific case but from what I see here like when it comes to uh wait what I want so when it comes to price uh this is 10 times cheaper right so you can see it's just a tiny fraction um it's much faster and it's only a little bit worse so maybe 3.5 is actually good and actually while this similarity was\",\n",
       "  'timecode_text': 'Comparing the results',\n",
       "  'description': 'In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.',\n",
       "  'link': 'https://www.youtube.com/watch?v=LlXclbD3pms&t=720s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity Legend okay so here the blue one is um um 40 and this 3.5 like I would say they are super super similar so from this distribution I would say that like there's probably no significant difference yeah sometimes there are answers that are worse but the majority of good answers are here and we can see this by looking at the median and mean which are almost the same so when it comes to median yeah in both cases is 2% worse um again like this is it might depend on your specific case but from what I see here like when it comes to uh wait what I want so when it comes to price uh this is 10 times cheaper right so you can see it's just a tiny fraction um it's much faster and it's only a little bit worse so maybe 3.5 is actually good and actually while this similarity was In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.\",\n",
       "  'id': '4acec9477f8f213fc8d2e0f62a3e8ec4'},\n",
       " {'vid_id': 'LlXclbD3pms',\n",
       "  'title': 'LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity',\n",
       "  'timecode': '13:17',\n",
       "  'text': \"comput being computed while uh it was Computing it I got an announcement in my um in my mailbox that there's a new model which is called GPT 40 mini which is uh it says it's cheaper even cheaper than 3.5 it's fast and it's good so what I want to do now is quickly compute the same thing for uh this new GPT GPT uh 40 mini and see if it's actually better than what we had before or not and how cheap it is so this is this is something we can do quickly um so I'll just have here process record and for all mini process record for all mini right and then we're uh let's just quickly test that it works no yeah it does so now I am going to execute the same thing as we did before uh here I'll quote for all mini um and we of course use this one so I'm going to execute it and while while it's executing I want to already um get this thing so I can save the result um 40 mini underscore um so this is the result or mini right so now I put it on pause then of course uh yeah like maybe I should already just copy all the code here cuz it's not that uh much and there is a problem okay great limits well unfortunately we cannot do it very fast um it seems uh I will uh compute it later um using uh the usual for Loop I'm wondering how much money was it I didn't even appear yet or did it like I can see that it's there like this tiny thing anyways yeah so rate\",\n",
       "  'timecode_text': 'Exploring gpt-4o-mini',\n",
       "  'description': 'In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.',\n",
       "  'link': 'https://www.youtube.com/watch?v=LlXclbD3pms&t=797s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity comput being computed while uh it was Computing it I got an announcement in my um in my mailbox that there's a new model which is called GPT 40 mini which is uh it says it's cheaper even cheaper than 3.5 it's fast and it's good so what I want to do now is quickly compute the same thing for uh this new GPT GPT uh 40 mini and see if it's actually better than what we had before or not and how cheap it is so this is this is something we can do quickly um so I'll just have here process record and for all mini process record for all mini right and then we're uh let's just quickly test that it works no yeah it does so now I am going to execute the same thing as we did before uh here I'll quote for all mini um and we of course use this one so I'm going to execute it and while while it's executing I want to already um get this thing so I can save the result um 40 mini underscore um so this is the result or mini right so now I put it on pause then of course uh yeah like maybe I should already just copy all the code here cuz it's not that uh much and there is a problem okay great limits well unfortunately we cannot do it very fast um it seems uh I will uh compute it later um using uh the usual for Loop I'm wondering how much money was it I didn't even appear yet or did it like I can see that it's there like this tiny thing anyways yeah so rate In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.\",\n",
       "  'id': '5e97600f553d057e9119e100c9e17db5'},\n",
       " {'vid_id': 'LlXclbD3pms',\n",
       "  'title': 'LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity',\n",
       "  'timecode': '16:20',\n",
       "  'text': \"limits uh are still a thing for this um so I cannot just bombard it with queries uh like I do with 3 uh uh five turbo so I need to do it in one thread probably but yeah so I'll just put some code here uh and I'll execute it um later right now I wanted to do LM as a judge but maybe I'll just record another video and what I can do is finish this one CU I'm really curious how well it performs and then for llm as a judge I'll have to shift these videos again so one video that I originally wanted to record here turns into uh four but I guess it's okay as long as you have fun uh that will probably be a lot of content for you so I hope you still enjoy what I'm doing um so let me quickly rewrite it as uh um a loop so I'll just do for um record in ground truth I'll do tqdm here and then result process record and then append I might hit um rate limits again um yeah and if this happens then I just I guess I'll just need to run this tomorrow and then see how uh it works yeah indeed so then I'll run it um sometime tomorrow and by that time my rate limits um are cleared so I can just execute it consequently in one threat without any multiprocessing um so yeah yeah so it's been fun um and we can see from this video that uh now we have a way to evaluate the different llms also we didn't change prompt but if we wanted to change a prompt we can also tweak The Prompt uh and then run it for the entire data set ground proof data set and see how well um the changes um to what increase or decrease in performance these changes lead um okay that's all for now and I'll finish recording this and add it to this video later see you soon um hopefully the rate limits are reset by now and I can execute this one\",\n",
       "  'timecode_text': 'Making it slower to avoid rate limits',\n",
       "  'description': 'In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.',\n",
       "  'link': 'https://www.youtube.com/watch?v=LlXclbD3pms&t=980s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity limits uh are still a thing for this um so I cannot just bombard it with queries uh like I do with 3 uh uh five turbo so I need to do it in one thread probably but yeah so I'll just put some code here uh and I'll execute it um later right now I wanted to do LM as a judge but maybe I'll just record another video and what I can do is finish this one CU I'm really curious how well it performs and then for llm as a judge I'll have to shift these videos again so one video that I originally wanted to record here turns into uh four but I guess it's okay as long as you have fun uh that will probably be a lot of content for you so I hope you still enjoy what I'm doing um so let me quickly rewrite it as uh um a loop so I'll just do for um record in ground truth I'll do tqdm here and then result process record and then append I might hit um rate limits again um yeah and if this happens then I just I guess I'll just need to run this tomorrow and then see how uh it works yeah indeed so then I'll run it um sometime tomorrow and by that time my rate limits um are cleared so I can just execute it consequently in one threat without any multiprocessing um so yeah yeah so it's been fun um and we can see from this video that uh now we have a way to evaluate the different llms also we didn't change prompt but if we wanted to change a prompt we can also tweak The Prompt uh and then run it for the entire data set ground proof data set and see how well um the changes um to what increase or decrease in performance these changes lead um okay that's all for now and I'll finish recording this and add it to this video later see you soon um hopefully the rate limits are reset by now and I can execute this one In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.\",\n",
       "  'id': 'd74ac52dd446e00e193fccffeed4d56d'},\n",
       " {'vid_id': 'LlXclbD3pms',\n",
       "  'title': 'LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity',\n",
       "  'timecode': '19:20',\n",
       "  'text': \"more time and see how fast it is yeah it's uh so one 2 seconds per iteration It's relatively um so this was uh I think the same speed as uh for GPT 40 maybe a little bit faster cuz at the end for GPT 40 it took uh 3 hours for this one we'll see so now I putting this on pause and uh then coming back when it when it finishes um yeah so there was a problem while exec in um something happened with my elastic search so I had to it stopped so I had to continue so I wrote this um disc code for that but right now I'll just remove it and we can see that um what I have at the end is the same uh amount of uh Records in both lists and of course we can now save the results Toc file well um I don't know how long it took approximately 1 hour or less and we can also check the cost so the cost is 28 cents really so it's indeed cheaper than um than GPT 3.4 CU um yeah this is a different day I was doing it uh and the other the beginning of the video yesterday and now it's uh another day and it's indeed cheaper so that's good now let's compute the similarity with these results and do we use the right one\",\n",
       "  'timecode_text': 'Getting the results for gpt-4o-mini',\n",
       "  'description': 'In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.',\n",
       "  'link': 'https://www.youtube.com/watch?v=LlXclbD3pms&t=1160s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity more time and see how fast it is yeah it's uh so one 2 seconds per iteration It's relatively um so this was uh I think the same speed as uh for GPT 40 maybe a little bit faster cuz at the end for GPT 40 it took uh 3 hours for this one we'll see so now I putting this on pause and uh then coming back when it when it finishes um yeah so there was a problem while exec in um something happened with my elastic search so I had to it stopped so I had to continue so I wrote this um disc code for that but right now I'll just remove it and we can see that um what I have at the end is the same uh amount of uh Records in both lists and of course we can now save the results Toc file well um I don't know how long it took approximately 1 hour or less and we can also check the cost so the cost is 28 cents really so it's indeed cheaper than um than GPT 3.4 CU um yeah this is a different day I was doing it uh and the other the beginning of the video yesterday and now it's uh another day and it's indeed cheaper so that's good now let's compute the similarity with these results and do we use the right one In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.\",\n",
       "  'id': 'ceac79c69f86393382b24afa69ffcf57'},\n",
       " {'vid_id': 'LlXclbD3pms',\n",
       "  'title': 'LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity',\n",
       "  'timecode': '21:15',\n",
       "  'text': \"okay so yeah now it will take some time um I will also now get this code so this this this one uh this right um and when it finishes um so now I'll stop it when it finishes we'll execute it and see at and look at the performance um so it has finished let's see how well it works and I also need to have this one for comparison I think so let me put it here this will this is GPT 4 oh um so we\",\n",
       "  'timecode_text': 'AQA cosine for gpt-4o-mini',\n",
       "  'description': 'In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.',\n",
       "  'link': 'https://www.youtube.com/watch?v=LlXclbD3pms&t=1275s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity okay so yeah now it will take some time um I will also now get this code so this this this one uh this right um and when it finishes um so now I'll stop it when it finishes we'll execute it and see at and look at the performance um so it has finished let's see how well it works and I also need to have this one for comparison I think so let me put it here this will this is GPT 4 oh um so we In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.\",\n",
       "  'id': '0815a5b8690c8ac37dfe402c8cbc4a02'},\n",
       " {'vid_id': 'LlXclbD3pms',\n",
       "  'title': 'LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity',\n",
       "  'timecode': '22:04',\n",
       "  'text': \"can see that um it's uh it's not even one% wait is it actually better looks like it's better um yeah so median is slightly worse like uh in the third digit and the mean is in one digit in the third digit better so I mean like from practical Point standpoint they are almost identical we can also plot all three of them from here let's put all three this will be 40 min right and yeah so this one uh so the 40 models they are very close and this one is slightly worse so from what I see it's clear that we should go with 40 mini so the performance is almost the same it's very cheap it's cheaper than 3.5 the only thing is um that I noticed is the rate liit so I cannot just bombard it with requests so I have to stick to rate limits which makes it slower um but if you work at organization you have some uh you have a key from this organization probably your rate limits are much higher um meaning you can send more requests than uh me so then maybe it's not really a problem okay that's all um looks quite cool so yeah um now we computed cosign similarity and in the next video we will see how to do to use LM to evalate the quality of an LM so see you soon\",\n",
       "  'timecode_text': 'Comparing gpt-4o-mini with gpt-4o and gpt-3.5-turbo',\n",
       "  'description': 'In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.',\n",
       "  'link': 'https://www.youtube.com/watch?v=LlXclbD3pms&t=1324s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.4 - Offline RAG Evaluation: Cosine Similarity can see that um it's uh it's not even one% wait is it actually better looks like it's better um yeah so median is slightly worse like uh in the third digit and the mean is in one digit in the third digit better so I mean like from practical Point standpoint they are almost identical we can also plot all three of them from here let's put all three this will be 40 min right and yeah so this one uh so the 40 models they are very close and this one is slightly worse so from what I see it's clear that we should go with 40 mini so the performance is almost the same it's very cheap it's cheaper than 3.5 the only thing is um that I noticed is the rate liit so I cannot just bombard it with requests so I have to stick to rate limits which makes it slower um but if you work at organization you have some uh you have a key from this organization probably your rate limits are much higher um meaning you can send more requests than uh me so then maybe it's not really a problem okay that's all um looks quite cool so yeah um now we computed cosign similarity and in the next video we will see how to do to use LM to evalate the quality of an LM so see you soon In this video, we evaluate model metrics using AQA cosine similarity. We compare model performance and explore the results for GPT-4o and GPT-3.5.\",\n",
       "  'id': '5ea8e934d58c2d1baa32a749cee6335d'},\n",
       " {'vid_id': 'IB6jePK1s58',\n",
       "  'title': 'LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge',\n",
       "  'timecode': '00:00',\n",
       "  'text': 'welcome back this is LM Zoom camp and in this video in this series of videos we are evaluating our Rock system meaning that um we want to have a way of evaluating all three steps in the r flow search prompt and llm in one go and for that we use different evaluation metrics we already talked about cign similarity and this is what we uh talked about in this notebook so we evaluated three models uh GPT 3.5 turbo GPT 40 and 4 o mini right and our conclusion was that 4 o mini uh gives the best results when it comes to cosine similarity uh so this is the model we want to use right uh another alternative approach could be instead of using cosine similarity we can just ask um we can just ask an llm if uh what we have is actually good enough or not uh so instead of using cosign similarity we just ask LM so this is the original question uh this is generated question uh so this is original answer this is a generated question and this is a generated answer how good they are and',\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, we use LLM as a judge method for evaluating RAG flow quality.',\n",
       "  'link': 'https://www.youtube.com/watch?v=IB6jePK1s58&t=0s',\n",
       "  'text_vector': 'LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge welcome back this is LM Zoom camp and in this video in this series of videos we are evaluating our Rock system meaning that um we want to have a way of evaluating all three steps in the r flow search prompt and llm in one go and for that we use different evaluation metrics we already talked about cign similarity and this is what we uh talked about in this notebook so we evaluated three models uh GPT 3.5 turbo GPT 40 and 4 o mini right and our conclusion was that 4 o mini uh gives the best results when it comes to cosine similarity uh so this is the model we want to use right uh another alternative approach could be instead of using cosine similarity we can just ask um we can just ask an llm if uh what we have is actually good enough or not uh so instead of using cosign similarity we just ask LM so this is the original question uh this is generated question uh so this is original answer this is a generated question and this is a generated answer how good they are and In this video, we use LLM as a judge method for evaluating RAG flow quality.',\n",
       "  'id': '353ac66e36b3c982cba06336a2e085fe'},\n",
       " {'vid_id': 'IB6jePK1s58',\n",
       "  'title': 'LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge',\n",
       "  'timecode': '01:10',\n",
       "  'text': \"uh before recording this video I asked GPT for a prompt so this is the prompt I used to ask for prompt I I say you're a prompt engineer uh expert and we work on a rock project I want to use the LM as a judge method for evaluating the quality of my rlow I have a data set with fq for each record to generated multiple questions using llm and now I want to use these questions to see if I can recover my original answer right and then I ask I need help with two prompts in the first case I want I have all three I have the original answer I have a generated question and I have um the generated answer right and in in the second case I only have the question and generated answer and you might wonder why do we need two cases so in the first case in case of online offline evaluation we have access to the original uh answer from which we generated the question uh but also um we don't always have that so for example when we already start using the data in production we still want the our system in production and get traffic from users we still want to be able to assess the quality of this and do this on the Fly uh and also display uh this um let's say in our dashboard and for that we don't have the original answer right so we only have questions and we can still use anlm to judge how good the answers are uh that's why we have the second one right we can compute both now on offline but then for the second one we can also use it uh online um so this was my prompt and then it suggested uh two prompts um so this your an expert avator um so which I think is quite good oops I accidentally pressed back anyways um so um it's quite good but then at the end uh this is how it asks to Output it I did not like it that's why I asked it um let's make sure the output is is in Json format so then we asked it to analyze uh provide variation in Json format we'll probably need to tweak the the bronze a little bit um so to make sure it actually returns Jason U sometimes it will not and we already have this experience when we generated uh our data our ground Ro data set right um but here are the prompts and I will simply copy these prompts uh prompt one template it's kind of long so oops let me do it like this so I want to see the entire thing um and then prompt two template I'll copy it from here um okay okay so cool so these are two prompt templates and I don't want to now execute it for the entire ground through data set of course it's possible to do it but um I we will just need to wait too long and then also well it's not as expensive within new GPT 40 Mini model but still I don't feel like I want to do\",\n",
       "  'timecode_text': 'Designing a prompt',\n",
       "  'description': 'In this video, we use LLM as a judge method for evaluating RAG flow quality.',\n",
       "  'link': 'https://www.youtube.com/watch?v=IB6jePK1s58&t=70s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge uh before recording this video I asked GPT for a prompt so this is the prompt I used to ask for prompt I I say you're a prompt engineer uh expert and we work on a rock project I want to use the LM as a judge method for evaluating the quality of my rlow I have a data set with fq for each record to generated multiple questions using llm and now I want to use these questions to see if I can recover my original answer right and then I ask I need help with two prompts in the first case I want I have all three I have the original answer I have a generated question and I have um the generated answer right and in in the second case I only have the question and generated answer and you might wonder why do we need two cases so in the first case in case of online offline evaluation we have access to the original uh answer from which we generated the question uh but also um we don't always have that so for example when we already start using the data in production we still want the our system in production and get traffic from users we still want to be able to assess the quality of this and do this on the Fly uh and also display uh this um let's say in our dashboard and for that we don't have the original answer right so we only have questions and we can still use anlm to judge how good the answers are uh that's why we have the second one right we can compute both now on offline but then for the second one we can also use it uh online um so this was my prompt and then it suggested uh two prompts um so this your an expert avator um so which I think is quite good oops I accidentally pressed back anyways um so um it's quite good but then at the end uh this is how it asks to Output it I did not like it that's why I asked it um let's make sure the output is is in Json format so then we asked it to analyze uh provide variation in Json format we'll probably need to tweak the the bronze a little bit um so to make sure it actually returns Jason U sometimes it will not and we already have this experience when we generated uh our data our ground Ro data set right um but here are the prompts and I will simply copy these prompts uh prompt one template it's kind of long so oops let me do it like this so I want to see the entire thing um and then prompt two template I'll copy it from here um okay okay so cool so these are two prompt templates and I don't want to now execute it for the entire ground through data set of course it's possible to do it but um I we will just need to wait too long and then also well it's not as expensive within new GPT 40 Mini model but still I don't feel like I want to do In this video, we use LLM as a judge method for evaluating RAG flow quality.\",\n",
       "  'id': '1010f80d0961eabb313981d716e94bc1'},\n",
       " {'vid_id': 'IB6jePK1s58',\n",
       "  'title': 'LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge',\n",
       "  'timecode': '05:00',\n",
       "  'text': \"it for all the for all the data so what I'm going to do is I'm going to sample um 100 records or maybe 200 right uh I also want to set seat to make sure the results are reproducible random State one so now every time let's have it like that um actually this is not what I wanted to sample right so this is uh no no this is correct so now we have the answer from LM original answer question so this is what we need uh I'll call it data frame sample all right so this is our sample and I will turn it into uh dictionaries which I'll just call sample right and then now for each of these samples uh samples uh sample will be or record for each of the sample uh for each of these records I want to use that prompt um to um yeah to ask an LM to judge the results so uh I'll replace this Fireballs answer llm generated answer and then uh question and original answer is this is how we call it right and I will also change it here and I will not I will remove question it's not generated question it's not necessarily generated question it can be just question um okay um so we have this template now from the template I want to create a prompt format inde did I not\",\n",
       "  'timecode_text': 'Preparing the data',\n",
       "  'description': 'In this video, we use LLM as a judge method for evaluating RAG flow quality.',\n",
       "  'link': 'https://www.youtube.com/watch?v=IB6jePK1s58&t=300s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge it for all the for all the data so what I'm going to do is I'm going to sample um 100 records or maybe 200 right uh I also want to set seat to make sure the results are reproducible random State one so now every time let's have it like that um actually this is not what I wanted to sample right so this is uh no no this is correct so now we have the answer from LM original answer question so this is what we need uh I'll call it data frame sample all right so this is our sample and I will turn it into uh dictionaries which I'll just call sample right and then now for each of these samples uh samples uh sample will be or record for each of the sample uh for each of these records I want to use that prompt um to um yeah to ask an LM to judge the results so uh I'll replace this Fireballs answer llm generated answer and then uh question and original answer is this is how we call it right and I will also change it here and I will not I will remove question it's not generated question it's not necessarily generated question it can be just question um okay um so we have this template now from the template I want to create a prompt format inde did I not In this video, we use LLM as a judge method for evaluating RAG flow quality.\",\n",
       "  'id': 'f35fedeb0962cd8ee7cf6ab00abb5a2b'},\n",
       " {'vid_id': 'IB6jePK1s58',\n",
       "  'title': 'LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge',\n",
       "  'timecode': '07:15',\n",
       "  'text': \"updated error relevance what um I don't understand what's happening um um okay yeah maybe this one right so we should do some sort of screening perhaps no I I will need to ask chpt how to actually do this so I asked chpt I described the problem um and said that this is the error I have and then it suggested uh to use this synex right so let's use that and let's see if it works okay it does work this is our prompt so this is our prompt right um okay and um now let's use um our we have llm function right which\",\n",
       "  'timecode_text': 'Fixing the template',\n",
       "  'description': 'In this video, we use LLM as a judge method for evaluating RAG flow quality.',\n",
       "  'link': 'https://www.youtube.com/watch?v=IB6jePK1s58&t=435s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge updated error relevance what um I don't understand what's happening um um okay yeah maybe this one right so we should do some sort of screening perhaps no I I will need to ask chpt how to actually do this so I asked chpt I described the problem um and said that this is the error I have and then it suggested uh to use this synex right so let's use that and let's see if it works okay it does work this is our prompt so this is our prompt right um okay and um now let's use um our we have llm function right which In this video, we use LLM as a judge method for evaluating RAG flow quality.\",\n",
       "  'id': '68c50567fe647936ea4e0ce0d1947420'},\n",
       " {'vid_id': 'IB6jePK1s58',\n",
       "  'title': 'LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge',\n",
       "  'timecode': '08:40',\n",
       "  'text': \"only takes prompt and model will be in our case um GPT or all mini okay so it looks uh Parable so now if do I have Json imported uh import Jason and then um so answer Jason Lo s answer okay so the generate answer directly addresses the question so now we will do this for um all the record in the sample uh and for sample and we will do this answer and we will put we will put the answers uh here answer then I will pass the answer answers to qdm I'll pass the answers and uh we will look at the distribution of relevant in our data set so let's execute that there could be a problem with uh parsing as we know like it does not necessarily um return always return what we want um but let's see what happens so right now I'm putting this on pause and we come back when uh it\",\n",
       "  'timecode_text': 'Evaluating the results with gpt',\n",
       "  'description': 'In this video, we use LLM as a judge method for evaluating RAG flow quality.',\n",
       "  'link': 'https://www.youtube.com/watch?v=IB6jePK1s58&t=520s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge only takes prompt and model will be in our case um GPT or all mini okay so it looks uh Parable so now if do I have Json imported uh import Jason and then um so answer Jason Lo s answer okay so the generate answer directly addresses the question so now we will do this for um all the record in the sample uh and for sample and we will do this answer and we will put we will put the answers uh here answer then I will pass the answer answers to qdm I'll pass the answers and uh we will look at the distribution of relevant in our data set so let's execute that there could be a problem with uh parsing as we know like it does not necessarily um return always return what we want um but let's see what happens so right now I'm putting this on pause and we come back when uh it In this video, we use LLM as a judge method for evaluating RAG flow quality.\",\n",
       "  'id': 'c9970febd355febd16d4bbd724ac4f85'},\n",
       " {'vid_id': 'IB6jePK1s58',\n",
       "  'title': 'LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge',\n",
       "  'timecode': '10:17',\n",
       "  'text': \"finishes okay so it finished only in 3 minutes let's take a look at answers so for example here uh so actually it looks like it's Parable Json in most cases so I'll just have uh Jason answers and then we'll iterate our answer string answer in answers and then we will um Json we'll parse it Json Lo s we pass the Think answer and then we will append the Json answer okay so it does not always work let's see what is the issue um yeah so it actually included a code block so we want to not do this um so we can ask the we can addit this prompt that do not output code blocks cuz we want to make sure that Json is Parable um so this is the same prompt we used before I'll just ask GPT to correct it uh let's correct there I'll put it on pause and finish and show you the results yeah so what I asked is um let's\",\n",
       "  'timecode_text': 'Parsing JSON',\n",
       "  'description': 'In this video, we use LLM as a judge method for evaluating RAG flow quality.',\n",
       "  'link': 'https://www.youtube.com/watch?v=IB6jePK1s58&t=617s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge finishes okay so it finished only in 3 minutes let's take a look at answers so for example here uh so actually it looks like it's Parable Json in most cases so I'll just have uh Jason answers and then we'll iterate our answer string answer in answers and then we will um Json we'll parse it Json Lo s we pass the Think answer and then we will append the Json answer okay so it does not always work let's see what is the issue um yeah so it actually included a code block so we want to not do this um so we can ask the we can addit this prompt that do not output code blocks cuz we want to make sure that Json is Parable um so this is the same prompt we used before I'll just ask GPT to correct it uh let's correct there I'll put it on pause and finish and show you the results yeah so what I asked is um let's In this video, we use LLM as a judge method for evaluating RAG flow quality.\",\n",
       "  'id': '24296e498a4ed4a771c13612eed4174e'},\n",
       " {'vid_id': 'IB6jePK1s58',\n",
       "  'title': 'LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge',\n",
       "  'timecode': '11:50',\n",
       "  'text': \"correct the prompt I want to make sure the output is always Parable Jason doesn't include code blocks but it does not really uh change anything so uh what I want so what I do is I'll take the prompt we had before in the previous um video when we also were using llm so here the prom was in Parable Json without using Code blocks this is what I will add here to but I don't want to re um recompute it right now J without uh using Cod BL I don't want to recompute it right now so what I will do is I will edit it um here we probably also need to know where exactly at which um index it is uh and so now we also know the ID of this thing so answers uh I'll just I'm I'll just correct it manually so I'll use something like this here um I don't know okay I'll just use this okay so does it uh does it work now now yeah should work uh now again we have a problem uh what is e uh yeah so I guess we will need to do it um a few times uh but like we corrected the prompt so so it shouldn't be an issue next time I hope we do not need to do itth too much cuz it is getting pretty annoying no I don't want to do this like for every record if it's that often so I'll just rebuild the prompts uh so let's see how the prompt looks like now in Parable Json without\",\n",
       "  'timecode_text': 'Changing the prompt to always return parsable JSON',\n",
       "  'description': 'In this video, we use LLM as a judge method for evaluating RAG flow quality.',\n",
       "  'link': 'https://www.youtube.com/watch?v=IB6jePK1s58&t=710s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge correct the prompt I want to make sure the output is always Parable Jason doesn't include code blocks but it does not really uh change anything so uh what I want so what I do is I'll take the prompt we had before in the previous um video when we also were using llm so here the prom was in Parable Json without using Code blocks this is what I will add here to but I don't want to re um recompute it right now J without uh using Cod BL I don't want to recompute it right now so what I will do is I will edit it um here we probably also need to know where exactly at which um index it is uh and so now we also know the ID of this thing so answers uh I'll just I'm I'll just correct it manually so I'll use something like this here um I don't know okay I'll just use this okay so does it uh does it work now now yeah should work uh now again we have a problem uh what is e uh yeah so I guess we will need to do it um a few times uh but like we corrected the prompt so so it shouldn't be an issue next time I hope we do not need to do itth too much cuz it is getting pretty annoying no I don't want to do this like for every record if it's that often so I'll just rebuild the prompts uh so let's see how the prompt looks like now in Parable Json without In this video, we use LLM as a judge method for evaluating RAG flow quality.\",\n",
       "  'id': 'f56bb169e564d4127f6740422e1f72bb'},\n",
       " {'vid_id': 'IB6jePK1s58',\n",
       "  'title': 'LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge',\n",
       "  'timecode': '14:10',\n",
       "  'text': \"using Code blocks um let's see how it works now okay and I'll just redo it one more time it finish it hopefully the results are par ible it looks like that so it just finished uh executing that so that's pretty cool and now we can turn this into a data frame um data frame answers I don't know if I should call it answers or evaluations probably evaluations is better um maybe I don't want to rename it but like answers is more like like answer to our question but here I should call this variable evaluations anyways now we can count uh relevance uh count I think it's count values and I don't remember I think it's count Val but for some reason it does not um I don't have autocomplete here I don't know why okay so maybe describe what work and like why it doesn't show me count value counts okay value count okay all of them are relevant maybe uh so like if it's the case then yeah it doesn't seem super useful um but still maybe sometimes there could be irrelevant uh answers but also Let's do let's execute the second prompt right uh wait I think what I did was incorrect\",\n",
       "  'timecode_text': 'Rerunning it with the new prompt',\n",
       "  'description': 'In this video, we use LLM as a judge method for evaluating RAG flow quality.',\n",
       "  'link': 'https://www.youtube.com/watch?v=IB6jePK1s58&t=850s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge using Code blocks um let's see how it works now okay and I'll just redo it one more time it finish it hopefully the results are par ible it looks like that so it just finished uh executing that so that's pretty cool and now we can turn this into a data frame um data frame answers I don't know if I should call it answers or evaluations probably evaluations is better um maybe I don't want to rename it but like answers is more like like answer to our question but here I should call this variable evaluations anyways now we can count uh relevance uh count I think it's count values and I don't remember I think it's count Val but for some reason it does not um I don't have autocomplete here I don't know why okay so maybe describe what work and like why it doesn't show me count value counts okay value count okay all of them are relevant maybe uh so like if it's the case then yeah it doesn't seem super useful um but still maybe sometimes there could be irrelevant uh answers but also Let's do let's execute the second prompt right uh wait I think what I did was incorrect In this video, we use LLM as a judge method for evaluating RAG flow quality.\",\n",
       "  'id': '72be617163981e79c73186eb42f6dd2e'},\n",
       " {'vid_id': 'IB6jePK1s58',\n",
       "  'title': 'LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge',\n",
       "  'timecode': '16:40',\n",
       "  'text': \"right so I did not build the yeah of course I did not build the prompt so I just wasted I don't know how many uh cents on that cuz I just sent the same thing over and over again which is weird right that's why we have all the cases in all the cases it's relevant so let's see what happens when we redo it and I'll put it on post and also change the names I don't really like uh that it's answers I want to um Quality valuations and then we will see what what are the results okay it finished executing I\",\n",
       "  'timecode_text': 'Fixing an error',\n",
       "  'description': 'In this video, we use LLM as a judge method for evaluating RAG flow quality.',\n",
       "  'link': 'https://www.youtube.com/watch?v=IB6jePK1s58&t=1000s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge right so I did not build the yeah of course I did not build the prompt so I just wasted I don't know how many uh cents on that cuz I just sent the same thing over and over again which is weird right that's why we have all the cases in all the cases it's relevant so let's see what happens when we redo it and I'll put it on post and also change the names I don't really like uh that it's answers I want to um Quality valuations and then we will see what what are the results okay it finished executing I In this video, we use LLM as a judge method for evaluating RAG flow quality.\",\n",
       "  'id': 'cf92d8fbb3dc91738f67b1b49e68e465'},\n",
       " {'vid_id': 'IB6jePK1s58',\n",
       "  'title': 'LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge',\n",
       "  'timecode': '17:20',\n",
       "  'text': \"also finished renaming and we can see that now it looks more like something useful and we can actually see now what are the examples of non-relevant U evaluations um relevance non [Music] relevant um and I'll turn it into um I'll turn it into dictionary so then we can read the whole thing varant records um so let's see see the generate answer discuss a pit version error related to Second learn which is not directly connected to the regional answer focus on Docker build error so cool let's see this is four um sample or right and I think we have DF sample right or we can probably read it here um so the question was what might be the cost of P version error in this week's serverless deep learning session the original answer this one and then yeah so this is good right um so now we can actually see um the relevant and non-relevant answers um of course we probably should do it on a larger sample but now we get some ideas uh what's wrong and with explanation actually it makes our job simpler so we can see what is wrong maybe we did not retrieve the answers we needed maybe um maybe something else is missing right um so this is quite helpful okay so now we want to do the\",\n",
       "  'timecode_text': 'Results for prompt #1',\n",
       "  'description': 'In this video, we use LLM as a judge method for evaluating RAG flow quality.',\n",
       "  'link': 'https://www.youtube.com/watch?v=IB6jePK1s58&t=1040s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge also finished renaming and we can see that now it looks more like something useful and we can actually see now what are the examples of non-relevant U evaluations um relevance non [Music] relevant um and I'll turn it into um I'll turn it into dictionary so then we can read the whole thing varant records um so let's see see the generate answer discuss a pit version error related to Second learn which is not directly connected to the regional answer focus on Docker build error so cool let's see this is four um sample or right and I think we have DF sample right or we can probably read it here um so the question was what might be the cost of P version error in this week's serverless deep learning session the original answer this one and then yeah so this is good right um so now we can actually see um the relevant and non-relevant answers um of course we probably should do it on a larger sample but now we get some ideas uh what's wrong and with explanation actually it makes our job simpler so we can see what is wrong maybe we did not retrieve the answers we needed maybe um maybe something else is missing right um so this is quite helpful okay so now we want to do the In this video, we use LLM as a judge method for evaluating RAG flow quality.\",\n",
       "  'id': '2104a968ea6f2d4059caf7cf0bf868dc'},\n",
       " {'vid_id': 'IB6jePK1s58',\n",
       "  'title': 'LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge',\n",
       "  'timecode': '19:11',\n",
       "  'text': \"other thing so we have two prompts right uh the first prompt was when we have original answer and we want to compare original answer and generated answer but also we can uh look at the question and generated answer and see if uh they match uh so before I do before I apply to all the records I will just uh yeah I need to also do it here um I just want to test it before we apply to all the records uh so this is the second prompt question generated answer and where's the llm yeah so I'll copy this thing the entire thing so so next time I don't forget to include prompt but also in the meantime I want to show the results of this relation okay let's see uh relevance the gener generated answer directly address modification made to uh what was the question what modification was made to the median Okay so now let's run it for all the um all the data I think I'll call itation two um okay so let's run it and also in the meantime we can do uh the same thing right parsing loation to which will do this and also this part right I will also save uh these results so then and cign similarity from the previous videos so then you can also take a look at the files uh but probably for this one since we're doing this on Sample you can do it yourself too if you of course have an account open a account okay right now I'm going to pause and come back when it finishes so it finished and all the answers are\",\n",
       "  'timecode_text': 'QA evaluation',\n",
       "  'description': 'In this video, we use LLM as a judge method for evaluating RAG flow quality.',\n",
       "  'link': 'https://www.youtube.com/watch?v=IB6jePK1s58&t=1151s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge other thing so we have two prompts right uh the first prompt was when we have original answer and we want to compare original answer and generated answer but also we can uh look at the question and generated answer and see if uh they match uh so before I do before I apply to all the records I will just uh yeah I need to also do it here um I just want to test it before we apply to all the records uh so this is the second prompt question generated answer and where's the llm yeah so I'll copy this thing the entire thing so so next time I don't forget to include prompt but also in the meantime I want to show the results of this relation okay let's see uh relevance the gener generated answer directly address modification made to uh what was the question what modification was made to the median Okay so now let's run it for all the um all the data I think I'll call itation two um okay so let's run it and also in the meantime we can do uh the same thing right parsing loation to which will do this and also this part right I will also save uh these results so then and cign similarity from the previous videos so then you can also take a look at the files uh but probably for this one since we're doing this on Sample you can do it yourself too if you of course have an account open a account okay right now I'm going to pause and come back when it finishes so it finished and all the answers are In this video, we use LLM as a judge method for evaluating RAG flow quality.\",\n",
       "  'id': '29a72b79088e6558de467ead5cc1aea6'},\n",
       " {'vid_id': 'IB6jePK1s58',\n",
       "  'title': 'LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge',\n",
       "  'timecode': '21:30',\n",
       "  'text': \"parsable all the evaluation results so let's look at results uh again relevance uh what it value counts um so relevant partly relevant nonrelevant so let's take a look at the non-relevant results so you see even without the original answer we can see that um it's not always relevant uh it it feels kind of weird to use an llm to evalate the results of an nlm um but still right we can see that um even without the original answer when we don't compare them it can detect cases of non-relevant answers of course we still need to take a look at these answers uh uh at this at these things maybe they are um okay let's take a look what command should I use to start Docker demon on Linux and the answer the provide context does CL commands to start Docker demon on Linux uh okay yeah so this is not a useful answer and of course here we can uh try to understand what happened what was the reason probably the relevant document was not retrieved yeah so this is a very good use case uh very good example of uh um like when our llm was able to uh catch that and this is something we definitely need to uh look out for uh later when we uh take our system we deploy it we start using it on real users um so there will be cases like that and we want to record these cases in uh a database somewhere so then we can analyze this and see um so this is what I wanted to show in this video and in the next one we will start with um we'll start with online evaluation online monitoring too uh we will first uh put everything we did so far uh in one place uh so we'll create a postgress database uh we will uh take our streamly app we will put um we'll start putting things to our postgress database and then later we add grafana to it so then we can also see in real life when our llm generates starts generating non-relevant answers and which of the answers are non-relevant okay so that's uh it for now and see you soon\",\n",
       "  'timecode_text': 'QA evaluation results',\n",
       "  'description': 'In this video, we use LLM as a judge method for evaluating RAG flow quality.',\n",
       "  'link': 'https://www.youtube.com/watch?v=IB6jePK1s58&t=1290s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.5 - Offline RAG Evaluation: LLM as a Judge parsable all the evaluation results so let's look at results uh again relevance uh what it value counts um so relevant partly relevant nonrelevant so let's take a look at the non-relevant results so you see even without the original answer we can see that um it's not always relevant uh it it feels kind of weird to use an llm to evalate the results of an nlm um but still right we can see that um even without the original answer when we don't compare them it can detect cases of non-relevant answers of course we still need to take a look at these answers uh uh at this at these things maybe they are um okay let's take a look what command should I use to start Docker demon on Linux and the answer the provide context does CL commands to start Docker demon on Linux uh okay yeah so this is not a useful answer and of course here we can uh try to understand what happened what was the reason probably the relevant document was not retrieved yeah so this is a very good use case uh very good example of uh um like when our llm was able to uh catch that and this is something we definitely need to uh look out for uh later when we uh take our system we deploy it we start using it on real users um so there will be cases like that and we want to record these cases in uh a database somewhere so then we can analyze this and see um so this is what I wanted to show in this video and in the next one we will start with um we'll start with online evaluation online monitoring too uh we will first uh put everything we did so far uh in one place uh so we'll create a postgress database uh we will uh take our streamly app we will put um we'll start putting things to our postgress database and then later we add grafana to it so then we can also see in real life when our llm generates starts generating non-relevant answers and which of the answers are non-relevant okay so that's uh it for now and see you soon In this video, we use LLM as a judge method for evaluating RAG flow quality.\",\n",
       "  'id': 'f9f88f80056ea1d627758b8f701f3659'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hi welcome back uh in this video what we want to do is we want to capture user feedback and monitor it and for that we need to um well have an ability have a user interface with uh what I want to do is add a thumbs down thumbs up button so the the person the user can tell if they like the answer or not and I'm going to do a little experiment what I have here is uh clot which is I think similar to chpt um I thought like why since it's a course about L LMS so why don't we just use an L lamp to create what we uh want and let's see how good like I haven't really used it that much so I recently I'm only experimenting with this people say it's better than Char GPT so this is something we can test right now uh but I\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback hi welcome back uh in this video what we want to do is we want to capture user feedback and monitor it and for that we need to um well have an ability have a user interface with uh what I want to do is add a thumbs down thumbs up button so the the person the user can tell if they like the answer or not and I'm going to do a little experiment what I have here is uh clot which is I think similar to chpt um I thought like why since it's a course about L LMS so why don't we just use an L lamp to create what we uh want and let's see how good like I haven't really used it that much so I recently I'm only experimenting with this people say it's better than Char GPT so this is something we can test right now uh but I In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': '8bc689a4d824bbd9cbe27f006e928120'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '01:00',\n",
       "  'text': \"I think if you use chpt the answers should be roughly similar right um so the first thing I want to do is I want to go to the second module where we created uh the stream L application and I want to copy this right and I want to start um so I I'll put it on pause write some prompt and then we will discuss it okay I tried to get a bit creative so this is what I wrote I have a stream L app and I want to add plus one and plus minus buttons there also I want to put the logic for rock and remember so that we put everything here well we have a llama here um maybe I'll we shouldn't have Lama here oh well that's a m i don't know what happened um okay I'll just leave it like that uh so um I want to put all this logic outside uh in a separate model so not to have uh this m here um so finally I want to select the course for which I asked the question so remember we have three courses well now actually we have four but in this uh data that we deal with it's only three and then I want to use postgress for storing the data so I want for every conversation I want to save the question that user asks and the answer it gets from the llm and then if the user uses thumbs up or thumbs down um also capture that so that's one thing right and then another thing I want to do is I want to adjust our Docker compost file so this is our Docker compost where it is uh it's here right um I'll just write here so I I also want to adjust our Docker compost file I'll put it in pause again and then you'll see what I wrote yeah so this is what I wrote actually I put back the this thing I don't know why I deleted it maybe I just wanted to use I I thought maybe we just use open AI but I think it's good if it we stay with a llama right um so this is what I wrote I also want to adjust our Docker compost file make data in elastic search persistent so I don't need to re index the data every time I run it I add postgress container and the container add a container for the streamly t uh and a Docker file for it right so let's execute it and see what\",\n",
       "  'timecode_text': 'Using LLM for coding - creating the prompt',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=60s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback I think if you use chpt the answers should be roughly similar right um so the first thing I want to do is I want to go to the second module where we created uh the stream L application and I want to copy this right and I want to start um so I I'll put it on pause write some prompt and then we will discuss it okay I tried to get a bit creative so this is what I wrote I have a stream L app and I want to add plus one and plus minus buttons there also I want to put the logic for rock and remember so that we put everything here well we have a llama here um maybe I'll we shouldn't have Lama here oh well that's a m i don't know what happened um okay I'll just leave it like that uh so um I want to put all this logic outside uh in a separate model so not to have uh this m here um so finally I want to select the course for which I asked the question so remember we have three courses well now actually we have four but in this uh data that we deal with it's only three and then I want to use postgress for storing the data so I want for every conversation I want to save the question that user asks and the answer it gets from the llm and then if the user uses thumbs up or thumbs down um also capture that so that's one thing right and then another thing I want to do is I want to adjust our Docker compost file so this is our Docker compost where it is uh it's here right um I'll just write here so I I also want to adjust our Docker compost file I'll put it in pause again and then you'll see what I wrote yeah so this is what I wrote actually I put back the this thing I don't know why I deleted it maybe I just wanted to use I I thought maybe we just use open AI but I think it's good if it we stay with a llama right um so this is what I wrote I also want to adjust our Docker compost file make data in elastic search persistent so I don't need to re index the data every time I run it I add postgress container and the container add a container for the streamly t uh and a Docker file for it right so let's execute it and see what In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': '69fdb2cbcf516e37d245fa580eb178f1'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '03:44',\n",
       "  'text': \"happens so this is the streamly tab that's cool um yeah I'll just put it on pause again and then come back when it finishes um yeah so looks like we have all the files we need um so this is the stream L app so it added the plus one and plus minus uh buttons um then uh we have two things I actually don't know about this if it will work so um let's call The Rock module differently cuz like there could be conflict maybe I don't know uh and then uh also yeah looks okay right so we put all the logic uh here outside this is our Rock module we will modify it uh maybe we will use uh um vectors here or let's see like that doesn't matter what we use here so we can change it as we like right um then database model I see that we have uh passwords um like this information hardcoded so let's use in environment of enironment variables for this okay so I don't think it's a good idea to initialize the database uh when the module is imported import imported so maybe we just uh we should do it only once so let's not do it every time we import the module uh actually this what I wanted to do oops and what else do we have uh updated Docker compos model and um and Docker file and also also let's create the requirements txt file because we don't have it no well it looks cool so probably we can start uh assistant okay that's a good name [Music] um so it also started using environment variables for AMA okay so now I will just start\",\n",
       "  'timecode_text': 'Initial code from LLM and tweaking',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=224s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback happens so this is the streamly tab that's cool um yeah I'll just put it on pause again and then come back when it finishes um yeah so looks like we have all the files we need um so this is the stream L app so it added the plus one and plus minus uh buttons um then uh we have two things I actually don't know about this if it will work so um let's call The Rock module differently cuz like there could be conflict maybe I don't know uh and then uh also yeah looks okay right so we put all the logic uh here outside this is our Rock module we will modify it uh maybe we will use uh um vectors here or let's see like that doesn't matter what we use here so we can change it as we like right um then database model I see that we have uh passwords um like this information hardcoded so let's use in environment of enironment variables for this okay so I don't think it's a good idea to initialize the database uh when the module is imported import imported so maybe we just uh we should do it only once so let's not do it every time we import the module uh actually this what I wanted to do oops and what else do we have uh updated Docker compos model and um and Docker file and also also let's create the requirements txt file because we don't have it no well it looks cool so probably we can start uh assistant okay that's a good name [Music] um so it also started using environment variables for AMA okay so now I will just start In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': '56dd2c14f369b0b70cd8d096f8e22912'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '06:50',\n",
       "  'text': \"copying these things where it is assistant modu so I will I created this app folder assistant did I spell assistant correctly or was it assistant assistant yeah I always forget how to spell it uh DB so I don't think it actually changed change that but we will adjust it and the stream lead app so like it's the same code as we had previously uh but it added a few things here cool uh requirements uh okay I don't think this is what we we actually need so I'll just remove this cuz like I I I'm not sure this is the right versions of oops no I'll just do like that so let's just install the last one do we actually use do okay anyway and and then updated Docker compose file um yeah so this is cool so what we can do now is we can um I think it will take some time so not to waste your time so what want I want to do next is is let me actually uh already do Docker compose up in this directory compose up uh what I want to do is uh I'll create another [Music] file py so this is something we will run once before we um so we do Docker compose up and we have a clean we don't\",\n",
       "  'timecode_text': 'Creating the code',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=410s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback copying these things where it is assistant modu so I will I created this app folder assistant did I spell assistant correctly or was it assistant assistant yeah I always forget how to spell it uh DB so I don't think it actually changed change that but we will adjust it and the stream lead app so like it's the same code as we had previously uh but it added a few things here cool uh requirements uh okay I don't think this is what we we actually need so I'll just remove this cuz like I I I'm not sure this is the right versions of oops no I'll just do like that so let's just install the last one do we actually use do okay anyway and and then updated Docker compose file um yeah so this is cool so what we can do now is we can um I think it will take some time so not to waste your time so what want I want to do next is is let me actually uh already do Docker compose up in this directory compose up uh what I want to do is uh I'll create another [Music] file py so this is something we will run once before we um so we do Docker compose up and we have a clean we don't In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': '985e96ae31e262e8b5aad18e0fec442a'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '09:30',\n",
       "  'text': \"have anything so here I want to put in the index in logic and I want to put the logic for creating a table in the database so we do toer compose up we run this thing and we have an environment that is ready to use so I'll stop right now write some code and then we will walk through this code okay I'm back so I put um so what I did I just took the code from uh this notebook that we prepared earlier I just copied all the code there I have not run it yet um so like it's ugly and then I also from DB imported this in DB and I removed it from here so you remember it was the initial initialized part was here I removed it from there cuz I want to initialize only once and um this code is kind of ugly but it should work another thing I did cuz I already have an instance of elastic search running in Docker so I changed the port mapping to this one and then I also adjusted it like that so I did Docker compose up uh okay CU like there's another elastic search with this name so what I can do is we can just not sure why we need this container name can just remove that and I'll run it one more time so if I remove container name that it uh yeah what is that cannot um is it yeah I think it's from the host machine to The Container right yeah so when I don't specify uh the container name then it comes up with like a container name itself um okay things seem to work um so the first thing thing we need to do is uh [Music] run so I need to create a new terminal for that so we want to run this prep script I hope it works but if it doesn't um yeah we will see now uh when this terminal appears of course if it so I go Toom camp and then run this I should have added loging because right now I have no idea what's happening uh maybe it works maybe it does not let's see and actually this in DB I don't think it will work because postgress host should be um because we are not running it inside the docker compost Network it should be Local Host so probably it will not work um I should not have stopped it um but I will now do import uh local yeah course expert so yeah let's uh wait um actually while we're waiting so what I wanted to do here is I want to add some locks here\",\n",
       "  'timecode_text': 'Database preparation script',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=570s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback have anything so here I want to put in the index in logic and I want to put the logic for creating a table in the database so we do toer compose up we run this thing and we have an environment that is ready to use so I'll stop right now write some code and then we will walk through this code okay I'm back so I put um so what I did I just took the code from uh this notebook that we prepared earlier I just copied all the code there I have not run it yet um so like it's ugly and then I also from DB imported this in DB and I removed it from here so you remember it was the initial initialized part was here I removed it from there cuz I want to initialize only once and um this code is kind of ugly but it should work another thing I did cuz I already have an instance of elastic search running in Docker so I changed the port mapping to this one and then I also adjusted it like that so I did Docker compose up uh okay CU like there's another elastic search with this name so what I can do is we can just not sure why we need this container name can just remove that and I'll run it one more time so if I remove container name that it uh yeah what is that cannot um is it yeah I think it's from the host machine to The Container right yeah so when I don't specify uh the container name then it comes up with like a container name itself um okay things seem to work um so the first thing thing we need to do is uh [Music] run so I need to create a new terminal for that so we want to run this prep script I hope it works but if it doesn't um yeah we will see now uh when this terminal appears of course if it so I go Toom camp and then run this I should have added loging because right now I have no idea what's happening uh maybe it works maybe it does not let's see and actually this in DB I don't think it will work because postgress host should be um because we are not running it inside the docker compost Network it should be Local Host so probably it will not work um I should not have stopped it um but I will now do import uh local yeah course expert so yeah let's uh wait um actually while we're waiting so what I wanted to do here is I want to add some locks here In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': '30de32be5076144e66ef723b8e32e03b'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '13:15',\n",
       "  'text': \"and it's it kind of looks ugly right CU like it's just one big file so I want to ask this thing let's see how it deals with that um let's refactor this code and I'll tell it uh what I want I create the move Imports to top create the separate functions uh for each um logical block and also add print statements logging um what else yep that's cool so yeah now I will copy it uh yeah we don't do anything with this but looks quite good I'll probably format it a little bit I like when there are there is more space uh but yeah it's the same code um I don't think uh I need to rerun [Music] it okay so I'll just save it and let's see what's happening here um right now we are executing um what are we doing actually set up ah we're doing this index documents part yeah so this is that part um okay so far so good I'll put it on pause and let's wait till it finishes I am really hoping that INB Works cuz I have not tested it um and maybe what we can also ask it to Also let's replace this with uh dropping first if exists and then creating so this is the same thing as we do with indexes so first we delet it if it exists and then if it doesn't and yeah so let me copy this thing and I'll form it um okay so let's wait till it finishes and yeah see what happens next um so yeah it finished indexing and\",\n",
       "  'timecode_text': 'Refactoring the preparation script',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=795s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback and it's it kind of looks ugly right CU like it's just one big file so I want to ask this thing let's see how it deals with that um let's refactor this code and I'll tell it uh what I want I create the move Imports to top create the separate functions uh for each um logical block and also add print statements logging um what else yep that's cool so yeah now I will copy it uh yeah we don't do anything with this but looks quite good I'll probably format it a little bit I like when there are there is more space uh but yeah it's the same code um I don't think uh I need to rerun [Music] it okay so I'll just save it and let's see what's happening here um right now we are executing um what are we doing actually set up ah we're doing this index documents part yeah so this is that part um okay so far so good I'll put it on pause and let's wait till it finishes I am really hoping that INB Works cuz I have not tested it um and maybe what we can also ask it to Also let's replace this with uh dropping first if exists and then creating so this is the same thing as we do with indexes so first we delet it if it exists and then if it doesn't and yeah so let me copy this thing and I'll form it um okay so let's wait till it finishes and yeah see what happens next um so yeah it finished indexing and In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': '78fdce6a60c73bd19a202242218e2913'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '16:27',\n",
       "  'text': \"now I I have this error uh no module import pyop pg2 and um so what I need to do is install this thing P install this and I don't really want to now reindex um with elastic search like all the stuff we did uh so what I'll do is I'll just command that so we can only execute that and when this is done I'll UNC commmand these things so let's run that and I don't know if anything is happening because it should um yeah interesting let me just write that uh to this thing um I have this problem I just installed this so yeah I have no idea what's uh the problem to be honest uh like these days coding feels like cheating uh like with all the LMS you just give it a like error copy and then keep your fingers crossed hope for the best that it works uh let's see uh so I don't want this so I'll remove that and I'll just format it um I don't know if something changed probably something did change let's see so I'm wondering like why uh um takes a little bit at the beginning yeah I don't have any postgress clients right now maybe we can quickly check PG C uh that should work I\",\n",
       "  'timecode_text': 'Fixing psycopg2 errors',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=987s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback now I I have this error uh no module import pyop pg2 and um so what I need to do is install this thing P install this and I don't really want to now reindex um with elastic search like all the stuff we did uh so what I'll do is I'll just command that so we can only execute that and when this is done I'll UNC commmand these things so let's run that and I don't know if anything is happening because it should um yeah interesting let me just write that uh to this thing um I have this problem I just installed this so yeah I have no idea what's uh the problem to be honest uh like these days coding feels like cheating uh like with all the LMS you just give it a like error copy and then keep your fingers crossed hope for the best that it works uh let's see uh so I don't want this so I'll remove that and I'll just format it um I don't know if something changed probably something did change let's see so I'm wondering like why uh um takes a little bit at the beginning yeah I don't have any postgress clients right now maybe we can quickly check PG C uh that should work I In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': '4cb31f49189d9055eab370e38e58e5c5'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '18:40',\n",
       "  'text': \"think uh so I just want to see if I can now connect to um this database and um see what is there um how can I connect to to this uh DB using PG CLI give me the entire command with uh DB um password typ was I think it will figure it out cool aim what okay no not this one yeah this is what I want to do I have no idea what W is for and that's my password okay and now I think it's DT right it's like that I don't remember how do I list databases how do I list databases and tables ing L okay so we have this course assistant cool um wait where it is uh DT list all the tables aha so I need uh to connect to uh this course assistant dat base and now I do DT to see the tables okay so these are the tables we created so it means that it's successful um that's cool so I can just um leave it um what else do we need so I think if I look at the requirements I don't see like first of all we don't use p n maybe we should so so um let's create a\",\n",
       "  'timecode_text': 'Using pgcli to check the tables',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=1120s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback think uh so I just want to see if I can now connect to um this database and um see what is there um how can I connect to to this uh DB using PG CLI give me the entire command with uh DB um password typ was I think it will figure it out cool aim what okay no not this one yeah this is what I want to do I have no idea what W is for and that's my password okay and now I think it's DT right it's like that I don't remember how do I list databases how do I list databases and tables ing L okay so we have this course assistant cool um wait where it is uh DT list all the tables aha so I need uh to connect to uh this course assistant dat base and now I do DT to see the tables okay so these are the tables we created so it means that it's successful um that's cool so I can just um leave it um what else do we need so I think if I look at the requirements I don't see like first of all we don't use p n maybe we should so so um let's create a In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': 'a2f205fc2802334ed1ae3fc9abc2c2e4'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '21:00',\n",
       "  'text': \"to file since we use by n um well I hope it diges out what I meant by that um so python 10 okay updated main so what is main uh maybe tap yeah I think this is the this prep think you in T and here from import let's format it um and I think I deleted too much so let me actually undo it and then this so uh elastic uh we don't need that model name index name and then base URL stays the same okay and this is all is in end file that's cool uh elastic search URL so yeah here we have a problem cuz uh like for local plastic search here let's call it local and and we also have just elastic URL which will be uh elastic search this one right so this is something we will use in another um here in doer compos so this this one elastic search Euro and we let's actually make sure we have the right name here elastic URL elastic search URL uh where do we configure it probably in um we did not copy one of the things um not this thing right so I think at the very beginning streamly tab rack modu which we called was it assistant yeah assistant right um or it's elastic Ur and I'll refractor it a little bit elastic uro and then this one will be all Lama URL and we will put now elastic URL here and llama URL here cool so far so good so now um well things are running there uh I I think I'll right now I I don't actually know what do I need to do for so I want to make sure now that our Docker compose uh picks up the files from um uh from this do for Docker compos to load data from and so probably we just need to somehow tell it that uh this is where you need to get so we already have it it's too fast um okay I don't understand like I think uh you need to make a few adjustments uh yeah I think the only thing we need like it says if it's already in your uh directory where your R doer composes you don't need to worry about anything um okay so just in case I'll copy it because maybe it did some useful things and I think we call it elastic URL right um okay and elastic Port uh okay let's also use it cuz I think uh what it did it uh uh okay let's use it uh elastic uh local your local will be Local Host oh so many things I hope it still works so I'll right now what I'm going to do is I'm going to stop this thing hope it's not a serious error and now run it one more time um right now we um here what the next thing we need to do we need to go to our Lama and pull the model and um so our postgress is already initialized our elastic search is already initialized we did that together uh for stream lead I think we will still need to add um to change it a little bit CU right now it uses the query search right um and we want to use Query I meant text search and we want to use Vector search so we will need to adjust that um for now I think we can just test it with uh text search and then adjust it later um yeah and when we add Vector\",\n",
       "  'timecode_text': 'Using .evn file for configuration',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=1260s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback to file since we use by n um well I hope it diges out what I meant by that um so python 10 okay updated main so what is main uh maybe tap yeah I think this is the this prep think you in T and here from import let's format it um and I think I deleted too much so let me actually undo it and then this so uh elastic uh we don't need that model name index name and then base URL stays the same okay and this is all is in end file that's cool uh elastic search URL so yeah here we have a problem cuz uh like for local plastic search here let's call it local and and we also have just elastic URL which will be uh elastic search this one right so this is something we will use in another um here in doer compos so this this one elastic search Euro and we let's actually make sure we have the right name here elastic URL elastic search URL uh where do we configure it probably in um we did not copy one of the things um not this thing right so I think at the very beginning streamly tab rack modu which we called was it assistant yeah assistant right um or it's elastic Ur and I'll refractor it a little bit elastic uro and then this one will be all Lama URL and we will put now elastic URL here and llama URL here cool so far so good so now um well things are running there uh I I think I'll right now I I don't actually know what do I need to do for so I want to make sure now that our Docker compose uh picks up the files from um uh from this do for Docker compos to load data from and so probably we just need to somehow tell it that uh this is where you need to get so we already have it it's too fast um okay I don't understand like I think uh you need to make a few adjustments uh yeah I think the only thing we need like it says if it's already in your uh directory where your R doer composes you don't need to worry about anything um okay so just in case I'll copy it because maybe it did some useful things and I think we call it elastic URL right um okay and elastic Port uh okay let's also use it cuz I think uh what it did it uh uh okay let's use it uh elastic uh local your local will be Local Host oh so many things I hope it still works so I'll right now what I'm going to do is I'm going to stop this thing hope it's not a serious error and now run it one more time um right now we um here what the next thing we need to do we need to go to our Lama and pull the model and um so our postgress is already initialized our elastic search is already initialized we did that together uh for stream lead I think we will still need to add um to change it a little bit CU right now it uses the query search right um and we want to use Query I meant text search and we want to use Vector search so we will need to adjust that um for now I think we can just test it with uh text search and then adjust it later um yeah and when we add Vector In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': '763b7dda2b8793ac204f1419418f74ca'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '28:00',\n",
       "  'text': \"search we will need to add things here too okay so it started uh and we need to go to Local Host and open this port something is happening um okay this is the same problem um so we need to rebuild this um and I don't remember the commment that we need to run um how do I uh Force Docker compose to rebuild images because this is the same error we had before um maybe I could have stopped on this the stream lead up but I can just stop the entire thing um rebuil without start without starting what ah okay so hopefully it will okay it's reing it now um uh did it actually do anything requirement it's cashed so yeah it's not good uh let me check uh doer file copy yeah so the last step is actually took the correct file so it should hopefully work let us\",\n",
       "  'timecode_text': 'Rebuilding it',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=1680s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback search we will need to add things here too okay so it started uh and we need to go to Local Host and open this port something is happening um okay this is the same problem um so we need to rebuild this um and I don't remember the commment that we need to run um how do I uh Force Docker compose to rebuild images because this is the same error we had before um maybe I could have stopped on this the stream lead up but I can just stop the entire thing um rebuil without start without starting what ah okay so hopefully it will okay it's reing it now um uh did it actually do anything requirement it's cashed so yeah it's not good uh let me check uh doer file copy yeah so the last step is actually took the correct file so it should hopefully work let us In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': 'a8bb47583c6f74cccf1243703d275477'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '29:45',\n",
       "  'text': \"see and now with a llama I need to do Docker PS to know how do I go tolama and this is this thing Docker minus it a Lama bsh and llama po B3 so we want to download the model that we will use um okay let me check if the stream L up is now working that's cool okay uh what is my question um I just disco discovered the course can I still join it uh T model okay let's wait till it downloads um okay it has pulled this thing um so now we we can try using it let's see if there are errors uh I can open maybe BLS um so llama oh llama is doing something hopefully uh I probably we should have added more locks so maybe let us ask the\",\n",
       "  'timecode_text': 'Running it',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=1785s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback see and now with a llama I need to do Docker PS to know how do I go tolama and this is this thing Docker minus it a Lama bsh and llama po B3 so we want to download the model that we will use um okay let me check if the stream L up is now working that's cool okay uh what is my question um I just disco discovered the course can I still join it uh T model okay let's wait till it downloads um okay it has pulled this thing um so now we we can try using it let's see if there are errors uh I can open maybe BLS um so llama oh llama is doing something hopefully uh I probably we should have added more locks so maybe let us ask the In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': '7fbdf4f15f14f1c4cfbed583dec5a945'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '31:25',\n",
       "  'text': \"the thing the AI to have more locks so then we know what's happening um and we add more locks here so we know something is happening I don't like using login so I will ask now to replace login with print statements um but yeah like it's fine to use uh login to um I don't like login let's just use PR um I mean ideally of course you should use um loggers not print statements um cuz like you have more control over what you output and what you don't but for this simple application like I don't want to deal with um with box um and this prints time when you run streamly up if you're running inside a Docker container you can view the prints um yeah so I don't want to I just want to see this in Docker uh when I run in Docker it um yeah but let's see maybe it finished already no uh maybe it's right in a poem but I also don't know if it's uh if llm is doing something or maybe something else is like maybe it's trying to connect to a postgress database maybe it's trying to connect to elastic and then there are sign timeouts or I don't know what so right now I will um stop\",\n",
       "  'timecode_text': 'Adding more logs (optional)',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=1885s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback the thing the AI to have more locks so then we know what's happening um and we add more locks here so we know something is happening I don't like using login so I will ask now to replace login with print statements um but yeah like it's fine to use uh login to um I don't like login let's just use PR um I mean ideally of course you should use um loggers not print statements um cuz like you have more control over what you output and what you don't but for this simple application like I don't want to deal with um with box um and this prints time when you run streamly up if you're running inside a Docker container you can view the prints um yeah so I don't want to I just want to see this in Docker uh when I run in Docker it um yeah but let's see maybe it finished already no uh maybe it's right in a poem but I also don't know if it's uh if llm is doing something or maybe something else is like maybe it's trying to connect to a postgress database maybe it's trying to connect to elastic and then there are sign timeouts or I don't know what so right now I will um stop In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': '2473a74c2e4bb45048c551a5d41c292f'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '33:30',\n",
       "  'text': \"it and the way I can stop it is um Docker Docker compose down and then just select the service I want to stop uh so I don't want to stop the entire thing I just want to stop stream load uh doesn't work like that um okay let me just ask this thing I only want to stop and rebuild without stopping the rest of the services in Docker compost stop okay so not down but stop so we stopped it now we build it when it stops uh cuz it's doing something and then uh maybe if I just can I force it okay stopped now we build it two add loocks there and now we start yeah yeah now we will run it again and hopefully we will see uh I didn't even check what kind of logs it uh okay um I think uh Docker PS I want to see the locks uh stream L where it is uh this one right Docker locks and then minus F I think minus F will will we will follow the box right um I actually don't see this starting the course assistant application so perhaps um uh uh we should have done something okay I just\",\n",
       "  'timecode_text': 'Restarting and rebuilding the streamlit service',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=2010s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback it and the way I can stop it is um Docker Docker compose down and then just select the service I want to stop uh so I don't want to stop the entire thing I just want to stop stream load uh doesn't work like that um okay let me just ask this thing I only want to stop and rebuild without stopping the rest of the services in Docker compost stop okay so not down but stop so we stopped it now we build it when it stops uh cuz it's doing something and then uh maybe if I just can I force it okay stopped now we build it two add loocks there and now we start yeah yeah now we will run it again and hopefully we will see uh I didn't even check what kind of logs it uh okay um I think uh Docker PS I want to see the locks uh stream L where it is uh this one right Docker locks and then minus F I think minus F will will we will follow the box right um I actually don't see this starting the course assistant application so perhaps um uh uh we should have done something okay I just In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': '43217230acfe4a8e7ec6e869782c4a97'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '36:01',\n",
       "  'text': \"discovered discovered the course can I still join it um yeah probably like I should have read more what it wrote cuz I don't see any locks here uh but okay so I is it actually doing something uh I need to press ask uh what I'll do is I'll just let it run for some time I don't know if it's if it's working or not like uh this is something we need to figure out and this is something I'll figure it out so I'll probably just spend some time reading what it wrote there while it's\",\n",
       "  'timecode_text': 'Asking the assistant using Ollama Phi3',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=2161s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback discovered discovered the course can I still join it um yeah probably like I should have read more what it wrote cuz I don't see any locks here uh but okay so I is it actually doing something uh I need to press ask uh what I'll do is I'll just let it run for some time I don't know if it's if it's working or not like uh this is something we need to figure out and this is something I'll figure it out so I'll probably just spend some time reading what it wrote there while it's In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': '2fd80a31374256bd7b7ffa90b67bc235'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '36:41',\n",
       "  'text': \"running so I'll put it on pause okay i r what it wrote and uh looks like it should just work so then I said okay like I don't see locks in Docker and then it suggested a few things like to add The Flash statement here so I'll do it now and then we also it's suggest to add python buffer it here I will add it too but here so the reason I added here and not here is because I don't want to rebuild this part so we don't need we already have it here I don't want to rebuild it but actually I think the better place would would be to put it here um we will rebuild it anyways uh later when we add sentence Transformers there but for now let me leave it here and um yeah I think it's added a few more things here you am okay I don't know what it does um and then we just rebuild restart uh yeah so this is something we know how to do so I'll stop it cuz like it takes ages I don't know what it's doing uh probably the prompt is quite large right uh so that's why um yeah it takes some time um so I need to do [Music] Docker over here right Docker compose uh stop stream lit um I just press contrl C to stop it uh to force stop it cuz probably it's like still quitting uh uh I know actually like actually I see that uh Lama says there something happening so probably it was uh actually executing something but I want to see the logs that uh it is actually doing something cuz like just thinking is it doing something is it not doing something is kind of not fun and now I will put it here so I don't forget next time to to move it um okay so we reild it and now we start again so let's see hopefully this time I see logs if I don't see locks I will just execute it and will not spend more time I just score join Locker PS I want to see locks um here Docker walks um yeah no looks okay I don't know so probably need to figure out I don't want to spend more time I think from the locks uh we saw from the locks from a llama that was it was actually doing something so so right now I'll put it on pause and wait um maybe it's actually a good idea to replace it with open AI so then we don't need uh to wait that long or with uh Gro or whatever um right now I just put it on pause and wait for I don't know 5 minutes or how long it needs to finish executing it um oh did finish okay yeah it was this time it was just uh 30 seconds I don't\",\n",
       "  'timecode_text': 'Getting logs to appear (optional)',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=2201s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback running so I'll put it on pause okay i r what it wrote and uh looks like it should just work so then I said okay like I don't see locks in Docker and then it suggested a few things like to add The Flash statement here so I'll do it now and then we also it's suggest to add python buffer it here I will add it too but here so the reason I added here and not here is because I don't want to rebuild this part so we don't need we already have it here I don't want to rebuild it but actually I think the better place would would be to put it here um we will rebuild it anyways uh later when we add sentence Transformers there but for now let me leave it here and um yeah I think it's added a few more things here you am okay I don't know what it does um and then we just rebuild restart uh yeah so this is something we know how to do so I'll stop it cuz like it takes ages I don't know what it's doing uh probably the prompt is quite large right uh so that's why um yeah it takes some time um so I need to do [Music] Docker over here right Docker compose uh stop stream lit um I just press contrl C to stop it uh to force stop it cuz probably it's like still quitting uh uh I know actually like actually I see that uh Lama says there something happening so probably it was uh actually executing something but I want to see the logs that uh it is actually doing something cuz like just thinking is it doing something is it not doing something is kind of not fun and now I will put it here so I don't forget next time to to move it um okay so we reild it and now we start again so let's see hopefully this time I see logs if I don't see locks I will just execute it and will not spend more time I just score join Locker PS I want to see locks um here Docker walks um yeah no looks okay I don't know so probably need to figure out I don't want to spend more time I think from the locks uh we saw from the locks from a llama that was it was actually doing something so so right now I'll put it on pause and wait um maybe it's actually a good idea to replace it with open AI so then we don't need uh to wait that long or with uh Gro or whatever um right now I just put it on pause and wait for I don't know 5 minutes or how long it needs to finish executing it um oh did finish okay yeah it was this time it was just uh 30 seconds I don't In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': 'a621dca90c99f76b72d60e3131b72c2f'},\n",
       " {'vid_id': 'XapKKBUMQ4M',\n",
       "  'title': 'LLM Zoomcamp 4.6 - Capturing User Feedback',\n",
       "  'timecode': '41:00',\n",
       "  'text': \"know why it didn't complete um earlier previously stream lead still does not have blocks I don't know probably need to Google and see uh what to do with this but we can see that um there is an answer right and then we can say okay this is a good answer well looks like like the answer is gone right okay I think we can leave with that and now the other thing I want to do is uh where is the command for PG CLI I want to quickly find it um can I quickly find it um PGC um yeah like I was hoping uh it's probably in my clipboard and I just don't want to accidentally expose any sensitive information so I'll put it on pause and then find it yeah so this is I didn't find it in my um clipboard but I found it here uh password was your password or your let me copy it so I'll to take this command and put it to read me file so then you don't need to do this and the password is in ourn this is my password and then we do c course assistant um then we do DT right DT and then uh we should have um something in uh from uh ations right conversations um so let's see what we have here I just discovered the course and we have the answer okay and what was the second table feedback from feedback feedback so feedback was positive for this conversation so now we can uh use this ID here to join and get um the feedback and um that's it for this video so let me know what you thought about this cuz I did a lot of improvisation and um if you like this style of doing or not um cuz like I also needed to debug a few things along the way and and we will see each other soon because right now we put everything in a database and what we can do now with this information is to uh display it on the dashboard and this is what we'll do soon in the next video we will also connect grafana and we will see uh all the metrics uh that are important for for our application like user feedback and other things how exactly it's used so see you soon\",\n",
       "  'timecode_text': 'Results',\n",
       "  'description': 'In this video, we save user feedback with thumbs up/down buttons in postgres.',\n",
       "  'link': 'https://www.youtube.com/watch?v=XapKKBUMQ4M&t=2460s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6 - Capturing User Feedback know why it didn't complete um earlier previously stream lead still does not have blocks I don't know probably need to Google and see uh what to do with this but we can see that um there is an answer right and then we can say okay this is a good answer well looks like like the answer is gone right okay I think we can leave with that and now the other thing I want to do is uh where is the command for PG CLI I want to quickly find it um can I quickly find it um PGC um yeah like I was hoping uh it's probably in my clipboard and I just don't want to accidentally expose any sensitive information so I'll put it on pause and then find it yeah so this is I didn't find it in my um clipboard but I found it here uh password was your password or your let me copy it so I'll to take this command and put it to read me file so then you don't need to do this and the password is in ourn this is my password and then we do c course assistant um then we do DT right DT and then uh we should have um something in uh from uh ations right conversations um so let's see what we have here I just discovered the course and we have the answer okay and what was the second table feedback from feedback feedback so feedback was positive for this conversation so now we can uh use this ID here to join and get um the feedback and um that's it for this video so let me know what you thought about this cuz I did a lot of improvisation and um if you like this style of doing or not um cuz like I also needed to debug a few things along the way and and we will see each other soon because right now we put everything in a database and what we can do now with this information is to uh display it on the dashboard and this is what we'll do soon in the next video we will also connect grafana and we will see uh all the metrics uh that are important for for our application like user feedback and other things how exactly it's used so see you soon In this video, we save user feedback with thumbs up/down buttons in postgres.\",\n",
       "  'id': '3efb0078863fc47cdc3e0c4e243221c0'},\n",
       " {'vid_id': 'BG8MlbidatA',\n",
       "  'title': 'LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"um even though it works there are still a few things that I think um we should do and maybe we should do it together so I want to in this assistant I want to be able to use uh text search or semantic search so maybe we can have a tole or something like that where we can select what kind of search we want to use and then we also need to add um this sentence Transformer Library here um so we can also use Vector search from stream lead right so these are the two things we want to do um and another thing is uh we saw how slow of Umama for me so what I want to do is I want to replace it with um open AI with GPT 40 mini right or also have some sort of toggle where we can say okay like um I want to use Ama or I want to use uh GPT uh so this is what I want to tell it now um so let me type it and we'll come back\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.',\n",
       "  'link': 'https://www.youtube.com/watch?v=BG8MlbidatA&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2 um even though it works there are still a few things that I think um we should do and maybe we should do it together so I want to in this assistant I want to be able to use uh text search or semantic search so maybe we can have a tole or something like that where we can select what kind of search we want to use and then we also need to add um this sentence Transformer Library here um so we can also use Vector search from stream lead right so these are the two things we want to do um and another thing is uh we saw how slow of Umama for me so what I want to do is I want to replace it with um open AI with GPT 40 mini right or also have some sort of toggle where we can say okay like um I want to use Ama or I want to use uh GPT uh so this is what I want to tell it now um so let me type it and we'll come back In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.\",\n",
       "  'id': '104f5e5badd6ed0293a06f1b679c0a89'},\n",
       " {'vid_id': 'BG8MlbidatA',\n",
       "  'title': 'LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2',\n",
       "  'timecode': '01:08',\n",
       "  'text': \"okay so this is what I wrote I want to do a few things have a way to choose between AMA and top um so let's have a drop down list in stream lead of what we use also I have I have a toggle between vector and text search currently we use text search I'll copy the code for Vector search below so this is uh I took from the notebook for evaluation um so this is what we have and I also just in case copied the code for the assistant uh right this is the current one that we have and let me execute that okay so uh updated assistant okay or Lama client open open air client looks reasonable uh and also it included sentence Transformers so amazing so let me take this one and put it here right so if text\",\n",
       "  'timecode_text': 'Prompt with requirements',\n",
       "  'description': 'In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.',\n",
       "  'link': 'https://www.youtube.com/watch?v=BG8MlbidatA&t=68s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2 okay so this is what I wrote I want to do a few things have a way to choose between AMA and top um so let's have a drop down list in stream lead of what we use also I have I have a toggle between vector and text search currently we use text search I'll copy the code for Vector search below so this is uh I took from the notebook for evaluation um so this is what we have and I also just in case copied the code for the assistant uh right this is the current one that we have and let me execute that okay so uh updated assistant okay or Lama client open open air client looks reasonable uh and also it included sentence Transformers so amazing so let me take this one and put it here right so if text In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.\",\n",
       "  'id': '63d41209a9a2b0f9b501e9d4dbe2ff3e'},\n",
       " {'vid_id': 'BG8MlbidatA',\n",
       "  'title': 'LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2',\n",
       "  'timecode': '02:38',\n",
       "  'text': \"search type Vector I'll use U lowercase uh then this is what we use uh question text Vector cool otherwise it's elastic search maybe call it text elastic search text then we built prompt then um so this is our rack function right uh yeah so it becomes a little bit more complicated but it's still the same uh function okay let us see how it works um to add little bit more space here um and arrange it slightly and then also we\",\n",
       "  'timecode_text': 'Adding vector search and OpenAI',\n",
       "  'description': 'In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.',\n",
       "  'link': 'https://www.youtube.com/watch?v=BG8MlbidatA&t=158s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2 search type Vector I'll use U lowercase uh then this is what we use uh question text Vector cool otherwise it's elastic search maybe call it text elastic search text then we built prompt then um so this is our rack function right uh yeah so it becomes a little bit more complicated but it's still the same uh function okay let us see how it works um to add little bit more space here um and arrange it slightly and then also we In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.\",\n",
       "  'id': '92a703b5d4f9da8df89d3092d89a8097'},\n",
       " {'vid_id': 'BG8MlbidatA',\n",
       "  'title': 'LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2',\n",
       "  'timecode': '03:32',\n",
       "  'text': \"need to update the requirements um it came up with uh I think yeah this one too so it came up with what the list of requirements we need to do um we already have open AI here um pandas why why do we need pandas uh do we actually need pandas no we do not uh maybe uh let me see this one Stream It app no we also don't use pandas here I don't know why I decided to add pandas we will not add pandas here um model selection and text Vector uh I made it um lower case here right uh I think I did here and also here what's happening open AI um so here um model Choice lower search tab over uh form a document okay so I formatted it a little bit cool so now let me check what kind of version of Transformer I have I'll do peep freeze so I want to make sure that\",\n",
       "  'timecode_text': 'Updating app.py',\n",
       "  'description': 'In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.',\n",
       "  'link': 'https://www.youtube.com/watch?v=BG8MlbidatA&t=212s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2 need to update the requirements um it came up with uh I think yeah this one too so it came up with what the list of requirements we need to do um we already have open AI here um pandas why why do we need pandas uh do we actually need pandas no we do not uh maybe uh let me see this one Stream It app no we also don't use pandas here I don't know why I decided to add pandas we will not add pandas here um model selection and text Vector uh I made it um lower case here right uh I think I did here and also here what's happening open AI um so here um model Choice lower search tab over uh form a document okay so I formatted it a little bit cool so now let me check what kind of version of Transformer I have I'll do peep freeze so I want to make sure that In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.\",\n",
       "  'id': '9431a369ca5bf129edd80b0cc0aeb5be'},\n",
       " {'vid_id': 'BG8MlbidatA',\n",
       "  'title': 'LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2',\n",
       "  'timecode': '05:15',\n",
       "  'text': \"NPI uh I have the right version of NPI and right version of sentence Transformer uh this um so P free just outputs all the um all the versions I have and it's here so I let me find npy so this is the version we need to use and we can also pin other versions and then for elastic search I use this one stream lead I don't have stream lead here um but this one pyth so I don't have it and then we want it to have um numai and what was that sentence Transformer this one right okay um I hope I did not forget anything let me check um the requirements I don't know why it need pandas why it needs requests and tqdm let me check uh we don't use requests here we don't use a requests here we don't use tqdm and here too so yeah probably it it's coming from here from prep but this is something we run outside of Docker to prepare the database so it's\",\n",
       "  'timecode_text': 'Updating requirements.txt',\n",
       "  'description': 'In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.',\n",
       "  'link': 'https://www.youtube.com/watch?v=BG8MlbidatA&t=315s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2 NPI uh I have the right version of NPI and right version of sentence Transformer uh this um so P free just outputs all the um all the versions I have and it's here so I let me find npy so this is the version we need to use and we can also pin other versions and then for elastic search I use this one stream lead I don't have stream lead here um but this one pyth so I don't have it and then we want it to have um numai and what was that sentence Transformer this one right okay um I hope I did not forget anything let me check um the requirements I don't know why it need pandas why it needs requests and tqdm let me check uh we don't use requests here we don't use a requests here we don't use tqdm and here too so yeah probably it it's coming from here from prep but this is something we run outside of Docker to prepare the database so it's In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.\",\n",
       "  'id': '6f3799cd11ebfdaa1e5885d96be8b228'},\n",
       " {'vid_id': 'BG8MlbidatA',\n",
       "  'title': 'LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2',\n",
       "  'timecode': '07:10',\n",
       "  'text': \"fine okay so now we need to rebuild this so first we stop stream lead we stop stream lead first then we rebuild it then we run it again and uh compost built and I'm also curious if now for\",\n",
       "  'timecode_text': 'Rebuilding the image',\n",
       "  'description': 'In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.',\n",
       "  'link': 'https://www.youtube.com/watch?v=BG8MlbidatA&t=430s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2 fine okay so now we need to rebuild this so first we stop stream lead we stop stream lead first then we rebuild it then we run it again and uh compost built and I'm also curious if now for In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.\",\n",
       "  'id': '47d120bc4ba39ae3dfe3fe88885c9101'},\n",
       " {'vid_id': 'BG8MlbidatA',\n",
       "  'title': 'LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2',\n",
       "  'timecode': '07:40',\n",
       "  'text': \"Docker compost we we probably need to propagate this two um Docker compost oops I I'll also do let's update Docker compost to pass the and AI key here my curent Docker compost um because we need to edit here right elastic URL Alama URL open EG okay and one more think um in the assistant um we I want to use uh or all mini okay so let it finished oh no it did not finish building yeah so these\",\n",
       "  'timecode_text': 'Adding OpenAI key',\n",
       "  'description': 'In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.',\n",
       "  'link': 'https://www.youtube.com/watch?v=BG8MlbidatA&t=460s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2 Docker compost we we probably need to propagate this two um Docker compost oops I I'll also do let's update Docker compost to pass the and AI key here my curent Docker compost um because we need to edit here right elastic URL Alama URL open EG okay and one more think um in the assistant um we I want to use uh or all mini okay so let it finished oh no it did not finish building yeah so these In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.\",\n",
       "  'id': 'c91a8d8a985d62c6be8a2e70e8b175a4'},\n",
       " {'vid_id': 'BG8MlbidatA',\n",
       "  'title': 'LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2',\n",
       "  'timecode': '08:53',\n",
       "  'text': \"are heavy dependencies uh so it needs pie torch NP um CA of vector search right so I'll put it on pause while it's building and then we come back when it's over so while it was building I noticed that it downloads a lot of uh Cuda stuff for GPU so this is not what we actually need because we want to use CPU only so I needed to do a little bit of Googling and also talking to this thing um to find out how to do this and and um I updated the requirements.txt so I included where it needs to download U cuz torch for CPU only is not available on the usual python index uh P Pi uh so I need to specify that uh for this Library you need to uh look here and I need to specify the version of py torch with CPU only so right now it's building it again and um yeah I'm putting this on pause again and for some reasons uh still tries to download pandas but maybe because uh sentence Transformers use pandas I don't know but yeah I think we still have to leave with pandas but anyways uh putting this on pause again and uh I'll see how it works so it finished let me try to um run it now and also I don't know if this way of installing Cuda will actually work because I have never done it this way I just noticed that it starts started pulling heavy dependencies so hopefully our app is still functioning but this is something we will uh check right now so I go to Local Host what is the part this one okay looks like it's working but for some reasons I don't see the new um the the new things we also should have uh model choice and search type should refresh it weird okay I think I should stop the whole thing um and start again I don't know if it helped cuz like this is super strange let's restart it yeah I think it's not interesting for you to watch me debug this thing so I'll just put it on pause and figure it out oh actually restart helped so I didn't\",\n",
       "  'timecode_text': 'PyTorch CPU only',\n",
       "  'description': 'In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.',\n",
       "  'link': 'https://www.youtube.com/watch?v=BG8MlbidatA&t=533s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2 are heavy dependencies uh so it needs pie torch NP um CA of vector search right so I'll put it on pause while it's building and then we come back when it's over so while it was building I noticed that it downloads a lot of uh Cuda stuff for GPU so this is not what we actually need because we want to use CPU only so I needed to do a little bit of Googling and also talking to this thing um to find out how to do this and and um I updated the requirements.txt so I included where it needs to download U cuz torch for CPU only is not available on the usual python index uh P Pi uh so I need to specify that uh for this Library you need to uh look here and I need to specify the version of py torch with CPU only so right now it's building it again and um yeah I'm putting this on pause again and for some reasons uh still tries to download pandas but maybe because uh sentence Transformers use pandas I don't know but yeah I think we still have to leave with pandas but anyways uh putting this on pause again and uh I'll see how it works so it finished let me try to um run it now and also I don't know if this way of installing Cuda will actually work because I have never done it this way I just noticed that it starts started pulling heavy dependencies so hopefully our app is still functioning but this is something we will uh check right now so I go to Local Host what is the part this one okay looks like it's working but for some reasons I don't see the new um the the new things we also should have uh model choice and search type should refresh it weird okay I think I should stop the whole thing um and start again I don't know if it helped cuz like this is super strange let's restart it yeah I think it's not interesting for you to watch me debug this thing so I'll just put it on pause and figure it out oh actually restart helped so I didn't In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.\",\n",
       "  'id': '796520eaa63af51393674466db098580'},\n",
       " {'vid_id': 'BG8MlbidatA',\n",
       "  'title': 'LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2',\n",
       "  'timecode': '12:22',\n",
       "  'text': \"need to do anything uh so here let me select open AI and first we will use text search I think I need to ask to to click ask okay cool and what if I use Vector search so both work cool I think this is the state where I want it to be so let me say plus one and this is the state where I want to be except maybe when I click plus one or minus one the text does not disappear um but I will not do it um right now okay so this is the code um now I will commit everything and uh yeah uh with this one we are done and for the next one uh think we don't need this file for the next one we will do uh we will set up grafana and start monitoring so see you soon\",\n",
       "  'timecode_text': 'Running the application',\n",
       "  'description': 'In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.',\n",
       "  'link': 'https://www.youtube.com/watch?v=BG8MlbidatA&t=742s',\n",
       "  'text_vector': \"LLM Zoomcamp 4.6.2 - Capturing User Feedback, Part 2 need to do anything uh so here let me select open AI and first we will use text search I think I need to ask to to click ask okay cool and what if I use Vector search so both work cool I think this is the state where I want it to be so let me say plus one and this is the state where I want to be except maybe when I click plus one or minus one the text does not disappear um but I will not do it um right now okay so this is the code um now I will commit everything and uh yeah uh with this one we are done and for the next one uh think we don't need this file for the next one we will do uh we will set up grafana and start monitoring so see you soon In this video, we add OpenAI integration to our UI in addition to Ollama and also add vector search.\",\n",
       "  'id': '4b6b30792b26ea6c7259d6753edf19d6'},\n",
       " {'vid_id': 'BQN0TOi2Rew',\n",
       "  'title': 'LLM Zoomcamp 4.7 - Monitoring the System',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"welcome back this is LM Zoom camp and in this series of videos we're talking about evaluation and monitoring we have talked a lot about offline evaluation and in the last few videos we talked about online evaluation but mostly in the last two videos we talked about capturing using feedback and for that we uh put together uh everything we did so far plus uh like we put it in Doos plus we uh started putting all the uh in postgress database so now we know what exactly is happening we also can capture thumbs down thumbs up from users uh so this way we can get user feedback we also talked about uh different types of um uh using llm as a judge for evaluating the relevancy of the responses and one of them was QA relevancy or question answer relevancy where we can ask an llm give it to uh things question and answer and then it will tell us if this answer is actually relevant to the question or not so now we can uh add all of this into our monitoring system and this is what we're going to do uh today and we can build a dashboard that shows that in the last hour uh this many uh requests to ANM um the responses were were relevant the answers were relevant or not how many thumbs UPS thumbs down are there um how many input tokens how many prompt and uh completion tokens whe there how many how much time how much money we spend and things like that so this is what we are going to do in this video we are going to set up grafana which is a tool for creating a dashboard uh where all of these will be displayed in real time as and as a backend we will use postgress the database we created previously so in the previous video I used nlm to actually generate uh to guide me through what I should do and here I also want to do the same uh but I already prepared uh The Prompt that I will send to CLA and uh this is the prompt so first of all instead of just saying I want to use Ama or open AI I want to have more Choice what I use for the prompt and then I say that I want to add monitoring with grafana where I want to display display response time how much time it took to respond from llm right how many answers are relevant and quote for that is below but that is below and this is exactly our llm as a judge so this is called for relevancy um then starts for thumbs on thumbs up down then we just want to also show uh last five conversations um or last five uh conversations yeah like basically question and answers from all them then what kind of models we use uh then how many tokens we use and the costs right so this is all what we want to display and I just here say that we want to use postgress as the back end for that because in grafana there are could be many backends uh we already store our data in uh postgress so we don't need any other database for that and here I give the code that we used before and uh yeah so here I say that let's include all that so I have repared this prompt and let us put it here and see what happens yep so the assistant uh we need to modify of course a few things uh so here llm yeah so the first one is we choose different uh let me just copy and then we will analyze what exactly we changed so where is the llm thing so here it is right so instead of just having a Lama and open AI we have something more so we have I need to add import time so for llm now we have uh um answer which is the text but also uh number of tokens right uh how much time it took atend so now it actually returns these three things um okay so next thing uh let me see assistant uh yeah we have aate relevancy right uh so this is something new uh I also need to add uh Json Json and note that uh what it also added is this TR accept thing so when it cannot uh parse the output so it says that the relevancy is unknown right um and finally this get answer so it is uh here okay and yeah so this is a cost calculation so for cost calculation perhaps we should actually move it to separate uh function um model Choice um tokens return cost okay and then of course yeah we can check how much it actually costs um like this values are maybe not real cuz like for me it's definitely less uh but for now I'll just keep these values as um as this and we can replace them later cost search text uh and for the cost yeah open I cost okay so um now in addition to answer we return a lot of extra stuff right response time relevance explanation model used how many tokens we used uh and so on right um I would yeah it's okay I mean I would uh maybe another thing is we can uh lock each invocation of llm where we would lock tokens cuz here we actually had two invocations right uh so first uh this one and then evaluate relevancy that was another invocation of an llm but it's okay we can leave it like that so of course uh um there are many different ways of doing the same thing okay now what do we have here uh so we have a new uh let me just copy the entire thing and we'll see what actually has changed of course we don't want this thing so get feedback stats uh this is a new I don't know why we need to to get it cuz um in grafana I don't think we will be invoking these functions but okay whatever uh save feedback so this is uh we already had it right and then save conversation there is much more that we save now uh yeah so everything is here so all these things we save into the database and now uh yeah so our schema is uh contains more things so these are all the things we look and the rest I don't think uh changed much what did we have here um yeah just empty empty lines and here I think we uh replaced uh yeah so nothing is interesting here okay so this is it the next one updating the stream lit app so I will again copy the entire thing and put it here and see what changes um okay so um here yeah the choices for models are different uh here yeah we display some monitor information I don't think it's actually um that needed but okay why not right um since we save it to the database we might as well just also display it this is not something we would display to an Exel user but for us uh in order to see it why not uh feedback buttons um okay displayer I I I Now understand why we have this if we want to see this immediately in stream lead up why not right and then why did it stop okay continue so I think it uh it was just generating too much uh code that's why maybe it stopped um so yeah let's see where it finished okay so almost finished okay and now where are we updated toer compos with grafana okay so here we have grafana so this is uh the Tool uh we are going to use for build in the dashboard and yeah gra one data okay and now what we can do is just do do compose up and um run it so let's see this what up uh yeah I'm not running it so it should restart everything and first um the thir thing we will need to do is we will need to uh regenerate our data uh and that we by regenerating our data I mean um we need to run the script again so then we um where it is so we initialize the database right so I do python uh prep probably it will complain now about uh uh I don't know if it can connect to uh the database right so here this is so we need to set it to Local Host so so I'll do hey expert Local Host now run it one more time it kind of take it just take ages I don't know if it's doing something or it's trying to connect but cannot um yeah so it's it actually should be running we see some postgress locks yeah so I see some locks from postgress okay so it has initialized the database um so we dropped everything we had and created a new database with a new schema right and the next thing we want to do is I want to check if um this is still working so I want to go to Local Host um what was the Port this one and I want to quickly ask something so just to make sure it works so I don't think this is the this is the old version yeah so we did not rebuild it so let's rebuild it uh we can do Docker compos stop stream lit do compos built stream L because we updated the app right um and now we do Docker compose start stream L okay and I will restart it and yeah here we have the new data and I think um actually from what I notice I remember I was struggling with locks with same locks I can see the locks now so it's kind of cool so now I can enter a question so let's use Vector search and use GPT 40 mini to ask a question about data engineer zom camp and let's ask so yeah we can see some logs that's cool uh then uh we see some stats um and this is not correct I think cuz we actually use GPT for all mini but this is something we can adjust later I don't want to to change it right now um we'll do it later and this is something that is kind of cool but not really needed right now so in principle we just see that it works okay so where did we stop so now we need to access grafana and the default one uh is admin admin so let me then set new password to admin admin and now we I want to add my first data source which is postgress postgress and uh so it's postgress database name course assistant is it course assistant uh where is our course assistant uh postgress user your username your password uh we don't need SSL and save it as okay it works um go away no okay now we need to create our first dashboard so for this dashboard we will will add a new pel and here we have some SQL um SQL queries that we want to um show uh here um from what I see the problem with the these SQL queries okay I can copy it here uh where is the Run button okay run button yeah we don't have anything that's one thing uh so maybe before I explain what the problem is with these SQL queries we need to generate some uh some data right so right now nothing is there uh I want to have another uh another script like we have this prep script uh the one that generates um PL data I think I have a prompt I also prepared The Prompt for that here let's create a script that generates on synthetic data uh first put data in previous periods between now and 6 hours ago um so we want uh to retroactively put some data there like for the last 6 hours so we can see the charts and also add some test data that is in being inserted right now so we can see data changing in grafana so let's do that and then we will come back and update uh the scripts okay so first it was uh generating synthetic data uh for like previous time period uh where did it go data it's kind of weird but okay like I was hoping it would just use safe conversation and put the time stamp where there but it's trying to do this strange thing but okay I will copy it now and call it uh generate data by um okay and I will invoke it here python generate data okay I was hoping to have more locks uh but okay let's see if we actually have anything in our data yeah now we do and here if we change it to last five minutes or last 30 minutes yeah it actually works so there are no problems so I think it actually grafana modifies the queries right so it also adds limits there this is what I wanted to add here like because there are some you can get start time and end time uh so for example from now - 3 hours to to now- 2 hours yeah it actually works fine so forget what I said this query is great um how we will call it so this is response time so this will be our first uh 6 hours be our first um pel so we will add another one here another one is relevance distribution panel so select code here I click this button and then um so here we have relevant non relevant uh do oh no no no no maybe okay so nonrelevant relevant partly relevant um it's count uh yeah counts are good too like we can also uh use uh perhaps fractions but also why not counts you can adjust this query if you want and by the way I think I should be also save in this so then you can later use it to so I'll all this um graph queries MD and this is the first query the second is this so this response time panel right then relevance distribution panel let me just copy all of them and I'll edit it right now so relevance distribution panel that was it right SQL so this this is something we will do now right so let's uh add this one and create a new another one so again I use code here I put this code here run query and is it again relance um yeah I don't want this cuz we just did that right SQL so this is this one model used and yeah let's go with this one um so panel title will be model used so here we forgot to change the title will be relevance see and you see it changed because the data is being generated and it is uh updated it so we can actually refresh dashboard or keep it automatically updated every 5 every 5 seconds or whatever we want like every minute uh to make it more interactive let's put it to 5 seconds so the next one is a token usage panel right so this is our tokens so this how many tokens um we use it's total tokens we can of course also split it uh into um like different typ types of tokens apply uh okay what else um actually I'm here open ey cost panel so this is the cost we can also add a breakdown here by uh different costs of different models um but I will leave it just total cost this is something you can do too and the last one is recent conversations maybe it's the last one I don't remember no no it's not the last one but this one is just last five conversations add a new panel vot run query and this will be a table right uh last five conversations okay so does it like it should I think there is incorrect time stamp because right now for me it's actually 20 yeah so I think this is something we will fix later but like it's there are dates from the future uh and feedback so this one okay let's go with maybe pie chart I'll call it like that okay so this is this looks okay of course we can uh arrange it a little bit maybe oops where did you go so we can arrange it like we want can you go here okay tokens model used uh yeah maybe like that and then of course like if we want to check it over the last 30 minutes this is how we do it last 5 minutes uh last 6 hours okay so it's more or less done the only thing that is left is uh fixing this one uh so here we don't we cannot properly see the last answers and this is a back I think it's a back in uh our um where it is generate data because let me check how we add time stamp here um Cent time yeah maybe the way we do this I think we need to add time Z on cuz clearly I see here from the time stamps that this is data from the past or from the future right not not the current one so probably it's adding you stuff here but um yeah even though it's heading there are records with um more current time stamps okay so let's uh ask it to correct it there is a problem with I'll just put it on pause finish the prompt and then we'll see it uh so this The Prompt he said that it's a problem with the data generator since it doesn't use the time zone some of the generated data previously has future time stop and then I say where I am and it should now modify the script uh we should also recreate a database right okay so while it's finishing that I will stop generating the data and then I'll run this so we clean the we withraw the table and create a new one so we don't have any dat there and when we do this our dashboard will stop working I think let me actually save it first and we can call it um course assistant but yeah there is no data now right cuz we recreated it and now oh where is it oh I also need to install by TZ okay is there any way to avoid that like hope I have it inst can we do it with without byz um yeah so right now I have um python 12 so this one should work um but also note that in um in streamly tab I think we use 3.9 yeah so since it's available from 3.9 it should be fine okay so let me copy it I will update now and I also kind of want to see some logs here let's add print statements so I see what's happening because right now I just say okay I generated some data but um like it doesn't really say uh what what kind of things it outputs um so right now it's creating this file so I did not commit it to get the that's why I actually don't know now when I over over wrote this file I just did contrl a contrl v so I don't know which part it actually changed so sometimes I think it could be uh a bit dangerous right CU um like when you copy the entire file content but yeah let's see dangerous in a sense that um yeah you don't know what changed so um let me run it I think yeah this one okay so now yeah it is actually showing what it's doing better and now we can yeah already and now it's good so now if we it like I wanted to see new stuff here some reasons it it's not working so I don't know why so there there's there are still some um yeah it's weird so I I'll debug it and then uh um I'll put it on pause debug it and then tell you what to do okay so I just asked it that I have this weird case I didn't really debug it so I will just use an updated code and see and hope for the best right so here where we do have it generate synthetic data and and then right okay I'll stop it um so this one and then we also it also updated the DB file did it update the entire file okay so it also added time zone info there DB which means I will need to which means I'll need to rebuild the this thing I guess um like I don't know why I just can't take my date uh from my computer for some reasons it cannot um yeah so now I'll rebuild it re execute and then show you the results okay so I just rebuild it and here when I refresh it we can see new data right and um so this is generated in the by the script I want to stop the script so the script is stopped and now I want to actually use our app and we should be able to see what we did now so this is the question I asked let's use a vector search uh engine and then ask the query and we get the response uh with some statistics and now if I update it I should see I should see it I don't for some reasons uh okay yeah cuz um yeah like too much on um AI it's probably not great but at least I know what to do now um I don't think I should uh torture you now with like rebuilding it again you can just I'll finish the code I'll commit the code and you can check it uh like the main thing the main outcome of this module is this dashboard so now we can see last conversations we can see user feedback we can see relevancy as detected by LM a judge we can track costs we can track tokens we can see what model is used the response time too um so now I'll fix that I'll commit the code and this thing is will be working so you can just um use it okay I hope it was fun uh still I was hoping I would need to do less live debugging than in the previous video yeah this video is again 40 minutes long well I hope hope this is entertaining I hope you learned a lot and this is it for this moduel it's been a lot of information um so probably will take some time for you to digest it and now yeah I I'll need to prepare some homework so you will have fun with the homework too so see you soon\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=BQN0TOi2Rew',\n",
       "  'text_vector': \"LLM Zoomcamp 4.7 - Monitoring the System welcome back this is LM Zoom camp and in this series of videos we're talking about evaluation and monitoring we have talked a lot about offline evaluation and in the last few videos we talked about online evaluation but mostly in the last two videos we talked about capturing using feedback and for that we uh put together uh everything we did so far plus uh like we put it in Doos plus we uh started putting all the uh in postgress database so now we know what exactly is happening we also can capture thumbs down thumbs up from users uh so this way we can get user feedback we also talked about uh different types of um uh using llm as a judge for evaluating the relevancy of the responses and one of them was QA relevancy or question answer relevancy where we can ask an llm give it to uh things question and answer and then it will tell us if this answer is actually relevant to the question or not so now we can uh add all of this into our monitoring system and this is what we're going to do uh today and we can build a dashboard that shows that in the last hour uh this many uh requests to ANM um the responses were were relevant the answers were relevant or not how many thumbs UPS thumbs down are there um how many input tokens how many prompt and uh completion tokens whe there how many how much time how much money we spend and things like that so this is what we are going to do in this video we are going to set up grafana which is a tool for creating a dashboard uh where all of these will be displayed in real time as and as a backend we will use postgress the database we created previously so in the previous video I used nlm to actually generate uh to guide me through what I should do and here I also want to do the same uh but I already prepared uh The Prompt that I will send to CLA and uh this is the prompt so first of all instead of just saying I want to use Ama or open AI I want to have more Choice what I use for the prompt and then I say that I want to add monitoring with grafana where I want to display display response time how much time it took to respond from llm right how many answers are relevant and quote for that is below but that is below and this is exactly our llm as a judge so this is called for relevancy um then starts for thumbs on thumbs up down then we just want to also show uh last five conversations um or last five uh conversations yeah like basically question and answers from all them then what kind of models we use uh then how many tokens we use and the costs right so this is all what we want to display and I just here say that we want to use postgress as the back end for that because in grafana there are could be many backends uh we already store our data in uh postgress so we don't need any other database for that and here I give the code that we used before and uh yeah so here I say that let's include all that so I have repared this prompt and let us put it here and see what happens yep so the assistant uh we need to modify of course a few things uh so here llm yeah so the first one is we choose different uh let me just copy and then we will analyze what exactly we changed so where is the llm thing so here it is right so instead of just having a Lama and open AI we have something more so we have I need to add import time so for llm now we have uh um answer which is the text but also uh number of tokens right uh how much time it took atend so now it actually returns these three things um okay so next thing uh let me see assistant uh yeah we have aate relevancy right uh so this is something new uh I also need to add uh Json Json and note that uh what it also added is this TR accept thing so when it cannot uh parse the output so it says that the relevancy is unknown right um and finally this get answer so it is uh here okay and yeah so this is a cost calculation so for cost calculation perhaps we should actually move it to separate uh function um model Choice um tokens return cost okay and then of course yeah we can check how much it actually costs um like this values are maybe not real cuz like for me it's definitely less uh but for now I'll just keep these values as um as this and we can replace them later cost search text uh and for the cost yeah open I cost okay so um now in addition to answer we return a lot of extra stuff right response time relevance explanation model used how many tokens we used uh and so on right um I would yeah it's okay I mean I would uh maybe another thing is we can uh lock each invocation of llm where we would lock tokens cuz here we actually had two invocations right uh so first uh this one and then evaluate relevancy that was another invocation of an llm but it's okay we can leave it like that so of course uh um there are many different ways of doing the same thing okay now what do we have here uh so we have a new uh let me just copy the entire thing and we'll see what actually has changed of course we don't want this thing so get feedback stats uh this is a new I don't know why we need to to get it cuz um in grafana I don't think we will be invoking these functions but okay whatever uh save feedback so this is uh we already had it right and then save conversation there is much more that we save now uh yeah so everything is here so all these things we save into the database and now uh yeah so our schema is uh contains more things so these are all the things we look and the rest I don't think uh changed much what did we have here um yeah just empty empty lines and here I think we uh replaced uh yeah so nothing is interesting here okay so this is it the next one updating the stream lit app so I will again copy the entire thing and put it here and see what changes um okay so um here yeah the choices for models are different uh here yeah we display some monitor information I don't think it's actually um that needed but okay why not right um since we save it to the database we might as well just also display it this is not something we would display to an Exel user but for us uh in order to see it why not uh feedback buttons um okay displayer I I I Now understand why we have this if we want to see this immediately in stream lead up why not right and then why did it stop okay continue so I think it uh it was just generating too much uh code that's why maybe it stopped um so yeah let's see where it finished okay so almost finished okay and now where are we updated toer compos with grafana okay so here we have grafana so this is uh the Tool uh we are going to use for build in the dashboard and yeah gra one data okay and now what we can do is just do do compose up and um run it so let's see this what up uh yeah I'm not running it so it should restart everything and first um the thir thing we will need to do is we will need to uh regenerate our data uh and that we by regenerating our data I mean um we need to run the script again so then we um where it is so we initialize the database right so I do python uh prep probably it will complain now about uh uh I don't know if it can connect to uh the database right so here this is so we need to set it to Local Host so so I'll do hey expert Local Host now run it one more time it kind of take it just take ages I don't know if it's doing something or it's trying to connect but cannot um yeah so it's it actually should be running we see some postgress locks yeah so I see some locks from postgress okay so it has initialized the database um so we dropped everything we had and created a new database with a new schema right and the next thing we want to do is I want to check if um this is still working so I want to go to Local Host um what was the Port this one and I want to quickly ask something so just to make sure it works so I don't think this is the this is the old version yeah so we did not rebuild it so let's rebuild it uh we can do Docker compos stop stream lit do compos built stream L because we updated the app right um and now we do Docker compose start stream L okay and I will restart it and yeah here we have the new data and I think um actually from what I notice I remember I was struggling with locks with same locks I can see the locks now so it's kind of cool so now I can enter a question so let's use Vector search and use GPT 40 mini to ask a question about data engineer zom camp and let's ask so yeah we can see some logs that's cool uh then uh we see some stats um and this is not correct I think cuz we actually use GPT for all mini but this is something we can adjust later I don't want to to change it right now um we'll do it later and this is something that is kind of cool but not really needed right now so in principle we just see that it works okay so where did we stop so now we need to access grafana and the default one uh is admin admin so let me then set new password to admin admin and now we I want to add my first data source which is postgress postgress and uh so it's postgress database name course assistant is it course assistant uh where is our course assistant uh postgress user your username your password uh we don't need SSL and save it as okay it works um go away no okay now we need to create our first dashboard so for this dashboard we will will add a new pel and here we have some SQL um SQL queries that we want to um show uh here um from what I see the problem with the these SQL queries okay I can copy it here uh where is the Run button okay run button yeah we don't have anything that's one thing uh so maybe before I explain what the problem is with these SQL queries we need to generate some uh some data right so right now nothing is there uh I want to have another uh another script like we have this prep script uh the one that generates um PL data I think I have a prompt I also prepared The Prompt for that here let's create a script that generates on synthetic data uh first put data in previous periods between now and 6 hours ago um so we want uh to retroactively put some data there like for the last 6 hours so we can see the charts and also add some test data that is in being inserted right now so we can see data changing in grafana so let's do that and then we will come back and update uh the scripts okay so first it was uh generating synthetic data uh for like previous time period uh where did it go data it's kind of weird but okay like I was hoping it would just use safe conversation and put the time stamp where there but it's trying to do this strange thing but okay I will copy it now and call it uh generate data by um okay and I will invoke it here python generate data okay I was hoping to have more locks uh but okay let's see if we actually have anything in our data yeah now we do and here if we change it to last five minutes or last 30 minutes yeah it actually works so there are no problems so I think it actually grafana modifies the queries right so it also adds limits there this is what I wanted to add here like because there are some you can get start time and end time uh so for example from now - 3 hours to to now- 2 hours yeah it actually works fine so forget what I said this query is great um how we will call it so this is response time so this will be our first uh 6 hours be our first um pel so we will add another one here another one is relevance distribution panel so select code here I click this button and then um so here we have relevant non relevant uh do oh no no no no maybe okay so nonrelevant relevant partly relevant um it's count uh yeah counts are good too like we can also uh use uh perhaps fractions but also why not counts you can adjust this query if you want and by the way I think I should be also save in this so then you can later use it to so I'll all this um graph queries MD and this is the first query the second is this so this response time panel right then relevance distribution panel let me just copy all of them and I'll edit it right now so relevance distribution panel that was it right SQL so this this is something we will do now right so let's uh add this one and create a new another one so again I use code here I put this code here run query and is it again relance um yeah I don't want this cuz we just did that right SQL so this is this one model used and yeah let's go with this one um so panel title will be model used so here we forgot to change the title will be relevance see and you see it changed because the data is being generated and it is uh updated it so we can actually refresh dashboard or keep it automatically updated every 5 every 5 seconds or whatever we want like every minute uh to make it more interactive let's put it to 5 seconds so the next one is a token usage panel right so this is our tokens so this how many tokens um we use it's total tokens we can of course also split it uh into um like different typ types of tokens apply uh okay what else um actually I'm here open ey cost panel so this is the cost we can also add a breakdown here by uh different costs of different models um but I will leave it just total cost this is something you can do too and the last one is recent conversations maybe it's the last one I don't remember no no it's not the last one but this one is just last five conversations add a new panel vot run query and this will be a table right uh last five conversations okay so does it like it should I think there is incorrect time stamp because right now for me it's actually 20 yeah so I think this is something we will fix later but like it's there are dates from the future uh and feedback so this one okay let's go with maybe pie chart I'll call it like that okay so this is this looks okay of course we can uh arrange it a little bit maybe oops where did you go so we can arrange it like we want can you go here okay tokens model used uh yeah maybe like that and then of course like if we want to check it over the last 30 minutes this is how we do it last 5 minutes uh last 6 hours okay so it's more or less done the only thing that is left is uh fixing this one uh so here we don't we cannot properly see the last answers and this is a back I think it's a back in uh our um where it is generate data because let me check how we add time stamp here um Cent time yeah maybe the way we do this I think we need to add time Z on cuz clearly I see here from the time stamps that this is data from the past or from the future right not not the current one so probably it's adding you stuff here but um yeah even though it's heading there are records with um more current time stamps okay so let's uh ask it to correct it there is a problem with I'll just put it on pause finish the prompt and then we'll see it uh so this The Prompt he said that it's a problem with the data generator since it doesn't use the time zone some of the generated data previously has future time stop and then I say where I am and it should now modify the script uh we should also recreate a database right okay so while it's finishing that I will stop generating the data and then I'll run this so we clean the we withraw the table and create a new one so we don't have any dat there and when we do this our dashboard will stop working I think let me actually save it first and we can call it um course assistant but yeah there is no data now right cuz we recreated it and now oh where is it oh I also need to install by TZ okay is there any way to avoid that like hope I have it inst can we do it with without byz um yeah so right now I have um python 12 so this one should work um but also note that in um in streamly tab I think we use 3.9 yeah so since it's available from 3.9 it should be fine okay so let me copy it I will update now and I also kind of want to see some logs here let's add print statements so I see what's happening because right now I just say okay I generated some data but um like it doesn't really say uh what what kind of things it outputs um so right now it's creating this file so I did not commit it to get the that's why I actually don't know now when I over over wrote this file I just did contrl a contrl v so I don't know which part it actually changed so sometimes I think it could be uh a bit dangerous right CU um like when you copy the entire file content but yeah let's see dangerous in a sense that um yeah you don't know what changed so um let me run it I think yeah this one okay so now yeah it is actually showing what it's doing better and now we can yeah already and now it's good so now if we it like I wanted to see new stuff here some reasons it it's not working so I don't know why so there there's there are still some um yeah it's weird so I I'll debug it and then uh um I'll put it on pause debug it and then tell you what to do okay so I just asked it that I have this weird case I didn't really debug it so I will just use an updated code and see and hope for the best right so here where we do have it generate synthetic data and and then right okay I'll stop it um so this one and then we also it also updated the DB file did it update the entire file okay so it also added time zone info there DB which means I will need to which means I'll need to rebuild the this thing I guess um like I don't know why I just can't take my date uh from my computer for some reasons it cannot um yeah so now I'll rebuild it re execute and then show you the results okay so I just rebuild it and here when I refresh it we can see new data right and um so this is generated in the by the script I want to stop the script so the script is stopped and now I want to actually use our app and we should be able to see what we did now so this is the question I asked let's use a vector search uh engine and then ask the query and we get the response uh with some statistics and now if I update it I should see I should see it I don't for some reasons uh okay yeah cuz um yeah like too much on um AI it's probably not great but at least I know what to do now um I don't think I should uh torture you now with like rebuilding it again you can just I'll finish the code I'll commit the code and you can check it uh like the main thing the main outcome of this module is this dashboard so now we can see last conversations we can see user feedback we can see relevancy as detected by LM a judge we can track costs we can track tokens we can see what model is used the response time too um so now I'll fix that I'll commit the code and this thing is will be working so you can just um use it okay I hope it was fun uh still I was hoping I would need to do less live debugging than in the previous video yeah this video is again 40 minutes long well I hope hope this is entertaining I hope you learned a lot and this is it for this moduel it's been a lot of information um so probably will take some time for you to digest it and now yeah I I'll need to prepare some homework so you will have fun with the homework too so see you soon Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '24620ef6819f9ccf45aae51f67700a15'},\n",
       " {'vid_id': 'qGFAX5ra1G8',\n",
       "  'title': 'LLM Zoomcamp 4.7.2 - Extra Grafana video',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"everyone welcome back I know this has been a long module but I realized that there are a few still a few things that I want to mention um so first of all in the previous video I had some problems with um at at the end uh with saving so the way I solved it was adding this time stamp which is optional and when it's none then I set it to current time I do it for both safe feedback and save conversation so then it works fine and actually in grafana I can see the latest uh uh questions so this is something I asked can I take the course if I don't know math can I take the course if I don't don't know Docker so that's one thing then another thing I wanted to show you is these intervals right so when I change it to last 5 minutes um so here these time series um panels they do update right so they are smart enough for that right but these things don't change so they actually are for the entire um database right so like from the beginning till now they always show so we don't really we cannot really influence the behavior of these things with um like with these plots with the intervals right so what we can do instead uh what we can do to fix it is use grafana variables so here when we do select some from whatever we can add an where statement and you see that it shows uh this uh these things so these are graph variables so what we can say is we're interested in the conversation where I actually have the query prepared it's in graphon MD um so of course I asked so I showed these queries that we have and asked AI clot to add uh the the variables right and then this particular one is+ one minus one so this is feedback I think it's somewhere at the end um yeah so this feedback statistics so what we need is this where statement where time stamp is between time from and time two now when we run it we can actually see that well maybe last 5 minutes there was nothing cuz I don't have the script running right now uh we will once we modify them we will run it right now there is nothing so this is actually expected right so then I save it and the same for relevancies so here we add this we run it and then uh yeah so now it gets updated uh with respect to the data in this specified interal and last thing we want to change is this one so it's the same actually the same thing so apply okay and now I will run uh I will run um this generate data now I can update it and I see new data and then let's see what's happening in the last 5 minutes right um so now it actually works um I don't want to change these last five conversations based on the interval so we can also change it based on the interval like last five conversations in this interval and you can of course add the same statement there but for me like last five conversations means like last the last ones but yeah of course we can also adjust that oh why I can just do this here right so then yeah this is how it works um but let's say here instead of now um we want to have it now- 6 hours and now - 5 hours right and then now it changes okay here you go I just updated this even though I did not want okay this is um this is the grafana variables that we can use and we should use um and the other thing I wanted to show you is how we we uh save and load grafana dashboards so let me show like there are multiple ways you can do this so first you click on the settings and then you go to Json model and here actually this is our uh this our dashboard as Json so we can copy this paste to our git and then um yeah make sure that change are synced right that's one of the things uh one of the ways then we can share this go to export and then save it to file so now I have this which we can again save to G or whatever and in order to load it we go here to dashboards import yeah first I want to save the dashboard let me save it so when I go here and oops in then I can just upload this file that we saved right there is a programmatic way of doing that instead of clicking the mouse you can um use API to do that and this is something you can talk to a to figure out how to do this it's up to you which way you prefer um so this is just simpler and the model has been long so I just wanted to show you only that and that's all I wanted to talk about in this module it's been long so now um yeah we're over I promise uh let's see if I can keep it anyways now time to do homework so see you soon\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=qGFAX5ra1G8',\n",
       "  'text_vector': \"LLM Zoomcamp 4.7.2 - Extra Grafana video everyone welcome back I know this has been a long module but I realized that there are a few still a few things that I want to mention um so first of all in the previous video I had some problems with um at at the end uh with saving so the way I solved it was adding this time stamp which is optional and when it's none then I set it to current time I do it for both safe feedback and save conversation so then it works fine and actually in grafana I can see the latest uh uh questions so this is something I asked can I take the course if I don't know math can I take the course if I don't don't know Docker so that's one thing then another thing I wanted to show you is these intervals right so when I change it to last 5 minutes um so here these time series um panels they do update right so they are smart enough for that right but these things don't change so they actually are for the entire um database right so like from the beginning till now they always show so we don't really we cannot really influence the behavior of these things with um like with these plots with the intervals right so what we can do instead uh what we can do to fix it is use grafana variables so here when we do select some from whatever we can add an where statement and you see that it shows uh this uh these things so these are graph variables so what we can say is we're interested in the conversation where I actually have the query prepared it's in graphon MD um so of course I asked so I showed these queries that we have and asked AI clot to add uh the the variables right and then this particular one is+ one minus one so this is feedback I think it's somewhere at the end um yeah so this feedback statistics so what we need is this where statement where time stamp is between time from and time two now when we run it we can actually see that well maybe last 5 minutes there was nothing cuz I don't have the script running right now uh we will once we modify them we will run it right now there is nothing so this is actually expected right so then I save it and the same for relevancies so here we add this we run it and then uh yeah so now it gets updated uh with respect to the data in this specified interal and last thing we want to change is this one so it's the same actually the same thing so apply okay and now I will run uh I will run um this generate data now I can update it and I see new data and then let's see what's happening in the last 5 minutes right um so now it actually works um I don't want to change these last five conversations based on the interval so we can also change it based on the interval like last five conversations in this interval and you can of course add the same statement there but for me like last five conversations means like last the last ones but yeah of course we can also adjust that oh why I can just do this here right so then yeah this is how it works um but let's say here instead of now um we want to have it now- 6 hours and now - 5 hours right and then now it changes okay here you go I just updated this even though I did not want okay this is um this is the grafana variables that we can use and we should use um and the other thing I wanted to show you is how we we uh save and load grafana dashboards so let me show like there are multiple ways you can do this so first you click on the settings and then you go to Json model and here actually this is our uh this our dashboard as Json so we can copy this paste to our git and then um yeah make sure that change are synced right that's one of the things uh one of the ways then we can share this go to export and then save it to file so now I have this which we can again save to G or whatever and in order to load it we go here to dashboards import yeah first I want to save the dashboard let me save it so when I go here and oops in then I can just upload this file that we saved right there is a programmatic way of doing that instead of clicking the mouse you can um use API to do that and this is something you can talk to a to figure out how to do this it's up to you which way you prefer um so this is just simpler and the model has been long so I just wanted to show you only that and that's all I wanted to talk about in this module it's been long so now um yeah we're over I promise uh let's see if I can keep it anyways now time to do homework so see you soon Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '7350c8bc3ff52e5d49c1840843527235'},\n",
       " {'vid_id': 'gP2ZOsG9Umg',\n",
       "  'title': 'LLM Zoomcamp - LLM Orchestration 5.0 Module overview',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"welcome to llm orchestration today and for the entire module we will be covering brag orchestration particularly the data preparation side where we ingest documents we transform them and then we eventually store them into a vector database now if you are starting up Mage for the first time you'll want to create a pipeline and so we'll go to the homepage or the the main overview page and this is what it'll look like from here you can click this drop down add a new Pipeline and select retrieval augmented generation and it'll take you to the building experience now if you already have a pipeline that you've created or you are coming back after uh after saving your pipeline you can go to the pipelines page page that will list out all your pipelines and you could rightclick on one of them and just click on open Pipeline and now'll take you to the building experience so you that you can resume your work now in a rag pipeline there are two main pipelines there's the pipeline that prepars the data and then there's the inference pipeline that handles the user prompt and the retrieval of the documents etc for this module will be focusing on the data preparation side in a future course will cover the inference side in the data preparation pipeline there's a few stages in the load stage that particularly handles ingesting your documents in the transform stage we handle chunking the documents into smaller pieces of text then we tokenize those chunks which prepares them for what we embed those chunks or those tokens and then we save those embeddings as vectors in a vector database we'll be using elastic search as the vector database since that is something you are already familiar with from previous modules now let's go ahead and get started\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'Join Mage CEO and Co-founder, Tommy Dang, as he goes over LLM Orchestration using Mage. ',\n",
       "  'link': 'https://www.youtube.com/watch?v=gP2ZOsG9Umg',\n",
       "  'text_vector': \"LLM Zoomcamp - LLM Orchestration 5.0 Module overview welcome to llm orchestration today and for the entire module we will be covering brag orchestration particularly the data preparation side where we ingest documents we transform them and then we eventually store them into a vector database now if you are starting up Mage for the first time you'll want to create a pipeline and so we'll go to the homepage or the the main overview page and this is what it'll look like from here you can click this drop down add a new Pipeline and select retrieval augmented generation and it'll take you to the building experience now if you already have a pipeline that you've created or you are coming back after uh after saving your pipeline you can go to the pipelines page page that will list out all your pipelines and you could rightclick on one of them and just click on open Pipeline and now'll take you to the building experience so you that you can resume your work now in a rag pipeline there are two main pipelines there's the pipeline that prepars the data and then there's the inference pipeline that handles the user prompt and the retrieval of the documents etc for this module will be focusing on the data preparation side in a future course will cover the inference side in the data preparation pipeline there's a few stages in the load stage that particularly handles ingesting your documents in the transform stage we handle chunking the documents into smaller pieces of text then we tokenize those chunks which prepares them for what we embed those chunks or those tokens and then we save those embeddings as vectors in a vector database we'll be using elastic search as the vector database since that is something you are already familiar with from previous modules now let's go ahead and get started Join Mage CEO and Co-founder, Tommy Dang, as he goes over LLM Orchestration using Mage. \",\n",
       "  'id': '5c9135a7b0adea3271a398bf9c80fc91'},\n",
       " {'vid_id': '9BJppvgLINc',\n",
       "  'title': 'LLM Zoomcamp - LLM Orchestration 5.1 Ingest',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"the first thing we'll do is we will ingest documents let's go into the load stage and we will add blocks to ingest we can use a predefined template or we can use our custom code you can write any code you want to ingest documents since the documents we are using live in GitHub you've ingested it uh from a previous module we will just use the API data loader to fetch that data so we can paste in the URL and this is this the URL from GitHub the documents. Json fog you worked with in the past and we can run this to then get the output we can see that there are three rows one for each course and each row has multiple documents within it we will split this up in a later stage now if you want to look at the code that's running underneath the hood or let's say you wanted to customize the code yourself you can click edit here and then you can open up the code editor and make any changes you want here\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'In this section, we cover the ingestion of documents from a single data source.',\n",
       "  'link': 'https://www.youtube.com/watch?v=9BJppvgLINc',\n",
       "  'text_vector': \"LLM Zoomcamp - LLM Orchestration 5.1 Ingest the first thing we'll do is we will ingest documents let's go into the load stage and we will add blocks to ingest we can use a predefined template or we can use our custom code you can write any code you want to ingest documents since the documents we are using live in GitHub you've ingested it uh from a previous module we will just use the API data loader to fetch that data so we can paste in the URL and this is this the URL from GitHub the documents. Json fog you worked with in the past and we can run this to then get the output we can see that there are three rows one for each course and each row has multiple documents within it we will split this up in a later stage now if you want to look at the code that's running underneath the hood or let's say you wanted to customize the code yourself you can click edit here and then you can open up the code editor and make any changes you want here In this section, we cover the ingestion of documents from a single data source.\",\n",
       "  'id': 'e3d6e7a4fb41d4d300c6131ac41960ab'},\n",
       " {'vid_id': 'H2oq5GSCKhM',\n",
       "  'title': 'LLM Zoomcamp - LLM Orchestration 5.2 Chunk',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"now we will chunk our documents let's go to the chunking operation and here we will use custom code there are many different ways to chunk your documents but the documents that we just ingested are almost chunked already they're chunked by quite question core section which is pretty good for now now depending on your actual use case you might want to use different strategies you might even combine multiple chunking strategies so here I'm just going to paste in some custom code and I can run this great now we can see that if we were to break up those three courses and then each of those documents in each course we get a total of 9948 documents and what we're doing here is we're simply combining the value of the cor section question answer into one string or one chunk separated by new lines you can see some sample chunks here here's the original document and here is the document ID we're giving it a document ID composed of core section question Etc and this allows us to identify the chunk alongside the document which is then used for retrieval in a later stage\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'Once data is ingested, we break it into manageable chunks. This section explains the importance of chunking data and various techniques.',\n",
       "  'link': 'https://www.youtube.com/watch?v=H2oq5GSCKhM',\n",
       "  'text_vector': \"LLM Zoomcamp - LLM Orchestration 5.2 Chunk now we will chunk our documents let's go to the chunking operation and here we will use custom code there are many different ways to chunk your documents but the documents that we just ingested are almost chunked already they're chunked by quite question core section which is pretty good for now now depending on your actual use case you might want to use different strategies you might even combine multiple chunking strategies so here I'm just going to paste in some custom code and I can run this great now we can see that if we were to break up those three courses and then each of those documents in each course we get a total of 9948 documents and what we're doing here is we're simply combining the value of the cor section question answer into one string or one chunk separated by new lines you can see some sample chunks here here's the original document and here is the document ID we're giving it a document ID composed of core section question Etc and this allows us to identify the chunk alongside the document which is then used for retrieval in a later stage Once data is ingested, we break it into manageable chunks. This section explains the importance of chunking data and various techniques.\",\n",
       "  'id': 'cacab7335d5bc367c38c67c75265f65a'},\n",
       " {'vid_id': 'hrMrqRgZryg',\n",
       "  'title': 'LLM Zoomcamp - LLM Orchestration 5.3 Tokenization',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"now that we're done breaking up our documents into smaller chunks we will tokenize them here we have many available templates strategies for tokenizing text for our example or use case we will use Spacey and we will limiti those chunks now this template contains the necessary code to tokenize your chunk however I have some custom code just to print out the progress as it tokenizes because in some cases it could take some time to process the entire set of chunks great we see that it's running and in a little bit it'll finish great now it finished we can preview the results and see that we have the original chunk the document ID and the list of tokens that was generated ated from this chunk of text\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'Tokenization is a crucial step in text processing and preparing the data for effective retrieval.',\n",
       "  'link': 'https://www.youtube.com/watch?v=hrMrqRgZryg',\n",
       "  'text_vector': \"LLM Zoomcamp - LLM Orchestration 5.3 Tokenization now that we're done breaking up our documents into smaller chunks we will tokenize them here we have many available templates strategies for tokenizing text for our example or use case we will use Spacey and we will limiti those chunks now this template contains the necessary code to tokenize your chunk however I have some custom code just to print out the progress as it tokenizes because in some cases it could take some time to process the entire set of chunks great we see that it's running and in a little bit it'll finish great now it finished we can preview the results and see that we have the original chunk the document ID and the list of tokens that was generated ated from this chunk of text Tokenization is a crucial step in text processing and preparing the data for effective retrieval.\",\n",
       "  'id': 'ba376e766d099b5155dd133e63918a9e'},\n",
       " {'vid_id': '8wrArv0DEKc',\n",
       "  'title': 'LLM Zoomcamp - LLM Orchestration 5.4 Embed',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"now that we've tokenized each Chunk we can create embeddings from them let's go into the embed operation and from here we can select from a multitude of templates that will embed those tokens now depending on your domain your use case you may want to use different embedding models for our module we will use spaces embeddings now this operation can take quite some time so I wrote some custom code to print out the progress as it creates those embeddings now this can take some time so I'll go grab some water you should go grab some water and we'll come back when it's done so that took almost 30 seconds for 100 chunks let's go get more water awesome now that it's done creating a beddings you can preview the results so here we see the chunk we see the document and we see the embedding it's a vector with 96 dimensions and the dimensions depend on the embedding model you use\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'Embedding data translates text into numerical vectors that can be processed by models.',\n",
       "  'link': 'https://www.youtube.com/watch?v=8wrArv0DEKc',\n",
       "  'text_vector': \"LLM Zoomcamp - LLM Orchestration 5.4 Embed now that we've tokenized each Chunk we can create embeddings from them let's go into the embed operation and from here we can select from a multitude of templates that will embed those tokens now depending on your domain your use case you may want to use different embedding models for our module we will use spaces embeddings now this operation can take quite some time so I wrote some custom code to print out the progress as it creates those embeddings now this can take some time so I'll go grab some water you should go grab some water and we'll come back when it's done so that took almost 30 seconds for 100 chunks let's go get more water awesome now that it's done creating a beddings you can preview the results so here we see the chunk we see the document and we see the embedding it's a vector with 96 dimensions and the dimensions depend on the embedding model you use Embedding data translates text into numerical vectors that can be processed by models.\",\n",
       "  'id': '363d2807707831bfe78d3a0fd4aae170'},\n",
       " {'vid_id': 'cHrphSoRBX4',\n",
       "  'title': 'LLM Zoomcamp - LLM Orchestration 5.5 Export',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"the final stage in data preparation for your rag pipeline is to export the chunks and the embeddings into a vector database so let's go ahead and go into the export stage and we'll choose Vector database from here we can export to various databases where we can even write our own custom code but since we've used elastic search in previous modules we'll go ahead and choose the elastic search template here we'll put in our connection string since I'm running elastic search in a Docker container this is the URL for that or the connection string for that and I wrote some additional code to set this up most of the code actually is the same as what you've done in previous modules here we create the index with a few columns a few properties and now we can run this creting it's printing out some of the results now because this is exporting it it doesn't actually produce data or an output but here in this return statement of this block I output a sample of 10 just so that we can see some results so here we can see that we've exported 10 embeddings\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'After processing, data needs to be exported for storage so that it can be retrieved for better contextualization of user queries.',\n",
       "  'link': 'https://www.youtube.com/watch?v=cHrphSoRBX4',\n",
       "  'text_vector': \"LLM Zoomcamp - LLM Orchestration 5.5 Export the final stage in data preparation for your rag pipeline is to export the chunks and the embeddings into a vector database so let's go ahead and go into the export stage and we'll choose Vector database from here we can export to various databases where we can even write our own custom code but since we've used elastic search in previous modules we'll go ahead and choose the elastic search template here we'll put in our connection string since I'm running elastic search in a Docker container this is the URL for that or the connection string for that and I wrote some additional code to set this up most of the code actually is the same as what you've done in previous modules here we create the index with a few columns a few properties and now we can run this creting it's printing out some of the results now because this is exporting it it doesn't actually produce data or an output but here in this return statement of this block I output a sample of 10 just so that we can see some results so here we can see that we've exported 10 embeddings After processing, data needs to be exported for storage so that it can be retrieved for better contextualization of user queries.\",\n",
       "  'id': '9ba051a4d3380fef478331ac86277cc9'},\n",
       " {'vid_id': 'z5NqDcaBglY',\n",
       "  'title': 'LLM Zoomcamp - LLM Orchestration 5.6 Retrieval',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"now that we've exported our chunks and our embeddings to a vector database we have completed creating the data preparation for a rag pipeline we can optionally try out and query the database for the chunks that we just created this isn't required but we can try it out anyway so let's go into retrieval and let's add a template for elastic search since we stored our embeddings there let's go ahead and paste in the connection string that we used previously and we will modify the template just a bit and what we're mainly doing is using a hard coding a sample embedding great let's go ahead and run this and we can see this is what we sent this is all just logging that we added ourselves let's take a look at the output great let's go ahead and take a sneak peek at what was retrieved so here we can see the chunks that were retrieved based off of the sample embedding that we used\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'After exporting the chunks and embeddings, we can test the search query to retrieve relevant documents on sample queries.',\n",
       "  'link': 'https://www.youtube.com/watch?v=z5NqDcaBglY',\n",
       "  'text_vector': \"LLM Zoomcamp - LLM Orchestration 5.6 Retrieval now that we've exported our chunks and our embeddings to a vector database we have completed creating the data preparation for a rag pipeline we can optionally try out and query the database for the chunks that we just created this isn't required but we can try it out anyway so let's go into retrieval and let's add a template for elastic search since we stored our embeddings there let's go ahead and paste in the connection string that we used previously and we will modify the template just a bit and what we're mainly doing is using a hard coding a sample embedding great let's go ahead and run this and we can see this is what we sent this is all just logging that we added ourselves let's take a look at the output great let's go ahead and take a sneak peek at what was retrieved so here we can see the chunks that were retrieved based off of the sample embedding that we used After exporting the chunks and embeddings, we can test the search query to retrieve relevant documents on sample queries.\",\n",
       "  'id': 'e62a34c36498c242d8932f3d5891fd04'},\n",
       " {'vid_id': 'nuk7_soKMUA',\n",
       "  'title': 'LLM Zoomcamp - LLM Orchestration 5.7 Trigger',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"now that we finished creating our data preparation for rag let's create a schedule that will trigger the pipeline on a daily basis let's go back to our pipeline list and click on the pipeline you are working on then let create a new trigger choose schedule let's name this daily document Le refresh we can choose it to run daily let's set a timeout let's say it fails if it doesn't finish in an hour and we can skip the run if the previous one is still running that shouldn't happen but it's a good option and let's create an initial pipeline run and let's make the start date something in the past okay save and let's enable this now in a few seconds it'll start running great now we can watch it run if your pipeline fails it's because we have that test block for the retrieval and that we just added in to test it out we should remove that so let's go back back and open the pipeline go into inference go into retrieval and it was in here right click and just click remove from pipeline great now let's go back and then we can rerun this great now we wait till it's done\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'Automation is key to maintaining and updating your system. This section demonstrates how to schedule and trigger daily runs for your data pipelines, ensuring up-to-date and consistent data processing.',\n",
       "  'link': 'https://www.youtube.com/watch?v=nuk7_soKMUA',\n",
       "  'text_vector': \"LLM Zoomcamp - LLM Orchestration 5.7 Trigger now that we finished creating our data preparation for rag let's create a schedule that will trigger the pipeline on a daily basis let's go back to our pipeline list and click on the pipeline you are working on then let create a new trigger choose schedule let's name this daily document Le refresh we can choose it to run daily let's set a timeout let's say it fails if it doesn't finish in an hour and we can skip the run if the previous one is still running that shouldn't happen but it's a good option and let's create an initial pipeline run and let's make the start date something in the past okay save and let's enable this now in a few seconds it'll start running great now we can watch it run if your pipeline fails it's because we have that test block for the retrieval and that we just added in to test it out we should remove that so let's go back back and open the pipeline go into inference go into retrieval and it was in here right click and just click remove from pipeline great now let's go back and then we can rerun this great now we wait till it's done Automation is key to maintaining and updating your system. This section demonstrates how to schedule and trigger daily runs for your data pipelines, ensuring up-to-date and consistent data processing.\",\n",
       "  'id': 'efadb781b5679a6f89a6763dab3f3de1'},\n",
       " {'vid_id': 'Tq9Vbm_2z3o',\n",
       "  'title': 'LLM Zoomcamp 6.1 - Techniques to improve RAG pipeline',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hello everyone this is module six of the llm zoom cam CSE my name is Tim and uh in this video I'm going to tell you about tips and tricks for advanced rack systems so in the previous modules we covered a lot of topics related to LMS and rack systems starting from the basics like how to launch your own rack system uh using for example um simple search engine or maybe using some kind of database like elastic search and so on then we move to uh to the parts related to orchestrating and monitoring R pipelines and now I think it's a time to cover the topics that could help you to improve your R systems uh but before we start uh let me uh remind you about uh the whole rack concept so what we have here we have a user with it a question or query like uh is it still possible to join to our course or can I still join to the course or maybe some specific tool specific uh question like how can I install Docker on my laptop and so on so our user can uh directly interact with our language model so language model will produce the answer the result but the problem is here that our language model doesn't have enough uh context about for example about our course and uh this answer uh may not be so precise or so appropriate that we want for such purposes we need to provide some additional uh information or context for our llm model so we can get this context from some uh knowledge database where we can store all this uh necessary information and after that we can add this context to our users uh question or query and uh in this case language model will provide uh the result in more precise manner we can say so uh now before we go to the different techniques I want to mention also ke Concepts to be sure that uh we have the same understanding about the components of our rack system uh when we build uh a r system we usually have uh two stages we can say the first stage is a indexing stage where where we actually ingest our data uh when we index the data in our case let it be um the FAQ documents related to the three different courses like ml Zoom Camp data engineering Zoom camp and MLM Zoom Camp so these three courses is general and maybe uh specific questions so what we need to do uh is we need to take this uh documents uh part them and then split into the chunks of texts uh our case we can do that in a pretty straightforward U way because our documents are well structured I mean you know each FAQ document uh consist of different sections uh where the question and answers listed one by one so now we can take these chunks and turn them into Vector form and then we can uh we can store these vectors in our database or in our knowledge base and this is what we need uh to do to prepare our data for the rack system um so uh the second stage is a Q&A stage uh what we can do here we can embed uh our users question into a vector as well and um given this Vector of numbers we can now search in our Vector database to find uh the top K closes document uh we can say the most relevant in the embedding space compared to the question that U we got asked then we can take our initial question enrich it with the most relevant documents as a context and send this data to our LM model and this is how it works end to end now we can jump into different techniques focusing on the mostly on the retrieval p uh the first technique um is called small to big chunk retrial uh the problem here is how to choose the right chunk size because chunk size is also our parameter if the chunk size is too large uh for example uh equal to an entire section maybe or the entire paragraph then um when when we will embed those chunks we will probably have too much noise uh in the uh our embeddings and we will probably not be able to retrieve the relevant documents because the relevant data we can say uh gets lost in large amount of data of data information on the other hand if we choose a small chunk size we can miss the essential piece of data and our model could return an incomplete answer due to the lack of context so one of the ways to solve this problem is to use both start from the small Chun small chunks and then go to the big one this means that on the indexing stage we should use small chunks to put them in our storage to our database for example we can make embeddings of the single sentences but on the step when we uh provide the the context to our L model we could retrieve and combine several uh chunks related for example to the same topic that's how the first tip uh looks like moving on the second technique is uh called leveraging document metadata so some metadata describing those documents sing it can be things like you know uh title of the documents year o and so on and so forth it's some additional information um describing our documents in some sort of cases uh we can start from the simple documents name or pass to the documents or any kind of folder structure and on top of that what we can do is to make topic model from the classic NLP world iing and then we extract several topics related uh to the information discussed in in our documents and of course we can also do it with llms we can just ask a language model to extract the attributes characterizing our documents that was technique number two if you have a madata or if you don't have metadata then generate it and try to make use of it when you are providing the context to language model technique number three is a hybrid search so in a hybrid search we are combining two methods of doing the search it's a vector based search and the keyword based search in a vctor uh in a vector uh based search we make the embeddings of the source documents and then we find the documents that have the closest distance in the embedding space so this search is based on the semantic meaning keyword search is just a lexical search one to one match over the worlds we are just manually going through the through the text and trying to find words that uh that we are looking for so if we find this words this will be a match in some cases keybord based search may actually be um useful uh for example when our documents contain a lot of domain specific terms or maybe abbreviations um related to some topic uh so what we can do here we can combine both Vector search and keyboard uh key keyword based search in our uh search pipeline it means that given a user question we will actually run two search by plans in a parallel that was technique number three so uh what about technique number four it's called user query rewriting what the problem is here um as a user of the language models we are not always uh good at formulating our questions uh I mean in a very specific precise uh and concise manner some users um can be asking more informal questions and also include a lot of noise and uh a lot of words that are we can say not IR revent to the actual search so what we can do with this problem well uh we should reformulate user question in in a better structured uh search query and one of the ways you can do that um is to ask a separate language model to reformulate the initial user query uh that we can then take it turn to the vector form and retrieve the revent CHS according to this uh reformulated question or query that was technique number four so moving on and the last one the technique number five is document raning here the problem may may be when we retrieve the documents from our database uh using a bing similarity approach our chunks will be ranking let's say uh by cassine similarities between the vectors which is a good measure but might not always be the best measure to actually reflect how relevant the embedding uh of document uh is for answering this particular user uh this particular user questions it may be that we are only extracting top five documents to show them to the llm but actually the the document that contain the answer for our question was on the sixth place and we missed that because we ordered our documents by the cassin similarity criteria what we can do here uh basically the answer to that problem is trying to reun the chunks after we have already retrieved them one of the ways how we can U do that is uh again leveraging the language model instead of a single language call we actually have two calls to the language model the first one asking to rank our chunks of text and then step number two answering the question using the top chunks from the uh ranking list that we got from the previous tab so that were all five techniques that I wanted to cover in this video of course there are many other techniques out of this but these five are just we can say the most popular and here I give you just a brief overview of this techniques if you want to dive in this topic to explore it in more details please refer to this video recorded by Alexa and Nikita you can find it in the webinar playlist also I would like to add that in this module among all these techniques we will focus only on two of them I search and the writing part and uh that's all for this video see you soon\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'In this video, we discuss the most popular techniques to improve your RAG pipeline.',\n",
       "  'link': 'https://www.youtube.com/watch?v=Tq9Vbm_2z3o',\n",
       "  'text_vector': \"LLM Zoomcamp 6.1 - Techniques to improve RAG pipeline hello everyone this is module six of the llm zoom cam CSE my name is Tim and uh in this video I'm going to tell you about tips and tricks for advanced rack systems so in the previous modules we covered a lot of topics related to LMS and rack systems starting from the basics like how to launch your own rack system uh using for example um simple search engine or maybe using some kind of database like elastic search and so on then we move to uh to the parts related to orchestrating and monitoring R pipelines and now I think it's a time to cover the topics that could help you to improve your R systems uh but before we start uh let me uh remind you about uh the whole rack concept so what we have here we have a user with it a question or query like uh is it still possible to join to our course or can I still join to the course or maybe some specific tool specific uh question like how can I install Docker on my laptop and so on so our user can uh directly interact with our language model so language model will produce the answer the result but the problem is here that our language model doesn't have enough uh context about for example about our course and uh this answer uh may not be so precise or so appropriate that we want for such purposes we need to provide some additional uh information or context for our llm model so we can get this context from some uh knowledge database where we can store all this uh necessary information and after that we can add this context to our users uh question or query and uh in this case language model will provide uh the result in more precise manner we can say so uh now before we go to the different techniques I want to mention also ke Concepts to be sure that uh we have the same understanding about the components of our rack system uh when we build uh a r system we usually have uh two stages we can say the first stage is a indexing stage where where we actually ingest our data uh when we index the data in our case let it be um the FAQ documents related to the three different courses like ml Zoom Camp data engineering Zoom camp and MLM Zoom Camp so these three courses is general and maybe uh specific questions so what we need to do uh is we need to take this uh documents uh part them and then split into the chunks of texts uh our case we can do that in a pretty straightforward U way because our documents are well structured I mean you know each FAQ document uh consist of different sections uh where the question and answers listed one by one so now we can take these chunks and turn them into Vector form and then we can uh we can store these vectors in our database or in our knowledge base and this is what we need uh to do to prepare our data for the rack system um so uh the second stage is a Q&A stage uh what we can do here we can embed uh our users question into a vector as well and um given this Vector of numbers we can now search in our Vector database to find uh the top K closes document uh we can say the most relevant in the embedding space compared to the question that U we got asked then we can take our initial question enrich it with the most relevant documents as a context and send this data to our LM model and this is how it works end to end now we can jump into different techniques focusing on the mostly on the retrieval p uh the first technique um is called small to big chunk retrial uh the problem here is how to choose the right chunk size because chunk size is also our parameter if the chunk size is too large uh for example uh equal to an entire section maybe or the entire paragraph then um when when we will embed those chunks we will probably have too much noise uh in the uh our embeddings and we will probably not be able to retrieve the relevant documents because the relevant data we can say uh gets lost in large amount of data of data information on the other hand if we choose a small chunk size we can miss the essential piece of data and our model could return an incomplete answer due to the lack of context so one of the ways to solve this problem is to use both start from the small Chun small chunks and then go to the big one this means that on the indexing stage we should use small chunks to put them in our storage to our database for example we can make embeddings of the single sentences but on the step when we uh provide the the context to our L model we could retrieve and combine several uh chunks related for example to the same topic that's how the first tip uh looks like moving on the second technique is uh called leveraging document metadata so some metadata describing those documents sing it can be things like you know uh title of the documents year o and so on and so forth it's some additional information um describing our documents in some sort of cases uh we can start from the simple documents name or pass to the documents or any kind of folder structure and on top of that what we can do is to make topic model from the classic NLP world iing and then we extract several topics related uh to the information discussed in in our documents and of course we can also do it with llms we can just ask a language model to extract the attributes characterizing our documents that was technique number two if you have a madata or if you don't have metadata then generate it and try to make use of it when you are providing the context to language model technique number three is a hybrid search so in a hybrid search we are combining two methods of doing the search it's a vector based search and the keyword based search in a vctor uh in a vector uh based search we make the embeddings of the source documents and then we find the documents that have the closest distance in the embedding space so this search is based on the semantic meaning keyword search is just a lexical search one to one match over the worlds we are just manually going through the through the text and trying to find words that uh that we are looking for so if we find this words this will be a match in some cases keybord based search may actually be um useful uh for example when our documents contain a lot of domain specific terms or maybe abbreviations um related to some topic uh so what we can do here we can combine both Vector search and keyboard uh key keyword based search in our uh search pipeline it means that given a user question we will actually run two search by plans in a parallel that was technique number three so uh what about technique number four it's called user query rewriting what the problem is here um as a user of the language models we are not always uh good at formulating our questions uh I mean in a very specific precise uh and concise manner some users um can be asking more informal questions and also include a lot of noise and uh a lot of words that are we can say not IR revent to the actual search so what we can do with this problem well uh we should reformulate user question in in a better structured uh search query and one of the ways you can do that um is to ask a separate language model to reformulate the initial user query uh that we can then take it turn to the vector form and retrieve the revent CHS according to this uh reformulated question or query that was technique number four so moving on and the last one the technique number five is document raning here the problem may may be when we retrieve the documents from our database uh using a bing similarity approach our chunks will be ranking let's say uh by cassine similarities between the vectors which is a good measure but might not always be the best measure to actually reflect how relevant the embedding uh of document uh is for answering this particular user uh this particular user questions it may be that we are only extracting top five documents to show them to the llm but actually the the document that contain the answer for our question was on the sixth place and we missed that because we ordered our documents by the cassin similarity criteria what we can do here uh basically the answer to that problem is trying to reun the chunks after we have already retrieved them one of the ways how we can U do that is uh again leveraging the language model instead of a single language call we actually have two calls to the language model the first one asking to rank our chunks of text and then step number two answering the question using the top chunks from the uh ranking list that we got from the previous tab so that were all five techniques that I wanted to cover in this video of course there are many other techniques out of this but these five are just we can say the most popular and here I give you just a brief overview of this techniques if you want to dive in this topic to explore it in more details please refer to this video recorded by Alexa and Nikita you can find it in the webinar playlist also I would like to add that in this module among all these techniques we will focus only on two of them I search and the writing part and uh that's all for this video see you soon In this video, we discuss the most popular techniques to improve your RAG pipeline.\",\n",
       "  'id': '5f588fdcc76e5c03b1ae33253e076fc6'},\n",
       " {'vid_id': 'TQ_ck6Q9gSQ',\n",
       "  'title': 'LLM Zoomcamp 6.2 - Hybrid Search',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hello everyone this is video number two of the module six of LM Zoom camp in this module we discuss best practices different tricks and techniques to improve our R pipeline in the previous video I give you a brief overview of the five different techniques these five are just we can say the most popular and as I mentioned before we are not going to explore each Topic in details we will just focus only on two of these techniques aarch and ranking and this video is exactly about the H search H search is based on the combination of two methods Vector search and keyword search in Vector search we use vectorization to transform our initial text into embeddings or in other words into a bunch of numbers we make the edings of the source documents put them into our database and then we find the documents that have the closest distance to the user's query in the embedding space keyword search is more about one to one match over the words so the idea of hybrid search is to combine both approaches in one Pipeline and the question is how we can do that so we can do that in a pretty straightforward way basically each search here on theal part or dble stage produces results ranking by scores and we can take these scores and me them by using such simple equation in this equation Alpha determines U the weights assigned to each score with values typically ranging between 0 and one so in the case of alpha equals to one we get view Vector search otherwise this IP equals to zero so our equation transforms to the pure K and that's all about this Z now it's time to implement a hybd search what we have here so I have already launched a python in my code spaces environment I also started elastic search in the do container in a spaces also and prepared a old sketch so in this module we are going to use code from the notebooks from the module number three in the module three we cover topics related uh to the vector search uh vector. databases vector and sematic search and Ral part also also about evaluation matrics and so on and uh I just opened one of these notebooks to be precise this one evaluate Vector so in this notebook Alexi implemented a vector [Music] search and uh write all evolation functions and culate several metcs like heat rate and Mr and I just uh copy paste first lines of this code from The alexas Notebook to this new notebook of course I made some cosmetic changes for example I move all the Imports to the beginning of this notebook deleted some useless cells and so on but the whole pipeline stay the same we imported the necessary modules then we loaded our documents with questions answers and documents IDs then we loaded model uh that we that we use to build edings for our documents and finally set up elastic search settings and index our documents so in this blocks I guess this is the most important or most interesting part because in this blocks we have Vector search implementation by using KNN algorithm so let's try to experiment with this part now we want to change this Vector search in such a way as to turn it into hybrid search we know that we are using elastic search for storing our data for storing our documents that's why we can directly go to the Google or maybe to the official documentation and try to find information about hybrid search this is what I have already done and uh I found a couple of pages related to hybrid search impementation in elastic search so as you can see from this uh page to build our hrid search it will be enough just to take two uh parts I mean Vector search and our standard keyword search and put everything together in one in one place in this case the final score for each hit will be calculated the sum of the KNN and query scores we can also specify a boost par here to give a weight uh to each score in our sum so uh let's try to repeat this so uh here we have a search yeah and uh for implementing a hybrid search we need to copy paste a piece of code from another notebook in module 3 where Alex implemented just a simple uh keyw matching search so this not look called evaluate text and here we need just this part let's call it here search and here we have our default uh question like I just discovered the course can I still join it now we are going to vectorize our initial query users query now we are running can query ah yeah also we need to add uh of course we need to add a filter here and uh let me change this part my this key yeah get and there have C name let me check it cross yeah now I found this problem that was because we don't need these two lines of code Al we just need to combine Canon query and key query and now as you can see it works also what we we can do we can add this boost values to each part by default we can use uh 0.5 values and let me check play to move it inside want to match as you can see our scores in our search they changed that's all I think we implemented uh simple I resarch now I guess it's time to rewrite the whole pipeline let's call it um I research we line here I'm also my I'm going to take Cod Snippets from the Alexa snow look so what we need here of course we should load ground through dos um from this one so what we need we need also functions to culate our Matrix like great and Mr [Music] score so cells also we need the elastic search [Music] function but we are of course have to modify it and we need also this relation function and [Music] need this question can so now we are going to the right we will start from the elastic search Cann let's rename it like El search F and here we need to put course we need to put our modified [Music] pm and and also we need to add keyword very part yeah I also forget to mention one important thing I guess here we don't need to implement boosting for separate parts of our uh keyword search like we do in the first modules maybe you remember that we waited additionally U boosted addition of the question part just uh to make sure that our search algorithm will pay much more attention to this part but now I guess we don't need this that's why I deleted this boosting because uh now we are going to play with this boost variables just to to find the trade all between the vector search and um keyword search so let's try to add here our qu part G and also we need specify the size yeah it's the size of the theant answers that will return our search because K algorithm return five five elements here also our keyword search where return five elements uh it would be a 10 elements in some but we don't need to change the length of our answer that's why we need to specify the size uh of our query here and I think that's all next we need to modify this uh question Vector function let's call it just question h and uh yeah I forget just to change the list of params that our function elastic search hrid will take so we need here build the vector uh yeah also we need quy here because quy is needed for our keyword search and now we need also change the power punction here so it should be you stay the same Vector course and we need to change sare here would be question Y and last think we need to check our evolation function it seems like we don't need to make any changes in our evolation function now we can just call our evolation function to run the whole Pipeline and calculate our scores our Matrix heat rate and [Music] Mr also I would like to copy paste the score so that Alexa got by calling this function using simple Vector search I just put it heretic search sear and we need to pass to our Rel function ground TRS and our such function that hold question hybrid sear Canon is not defined yeah here we need also to change the name of our function purose yeah it will take some time to calculate our scores to run the whole pipeline what we can do while while we are waiting we of course can copy paste other functions uh to run it not only on questions but also on answers on documents answers and also as you remember we buil three types of embeddings on our questions and our answers and uh the third one the third one was U we combined just conate answers is questions and also build a bendings so uh for each of these cases we need to write the independent uh function so the second one will be this one let's name it text [Music] other pars we don't need to change them I guess and uh need to change the name of our search function yeah the first calculation was finished and as you can see we improved our scores the heat rate was increased by uh I think by 15% also we we improved our Mr score significantly that's great so now let's try to run our next function we calculate uh we calculate this course using KNN under uh can over the answers of our questions yeah there the last one our research um questions and answers what your name I [Music] so our second run also improved our Matrix and Mr but in a comparison with our po run where we implemented search over than only questions so I can say that we get better scores here looks like we get the same scores yeah but anyway they better than the scores that we got using just the simple Vector search and it's finished now we can compare our scores with the scores that Alexi got in his notebook Y and in the case number three where we used question answers uh embeddings we got the Matrix the metric Valu is slightly better than in previous cases and also better than lxa got in the modle three a pure Vector search so it seems that our har search has a benefits I'm pretty sure that's not the limit and we can further increase our metrix by by playing with this boost PM Bo values for the instance we can increase the weight value for k for km part for example and that will increase its impact on the final score but of course we need to carry on some additional experiments to prove this Theory so let's summarize uh H research uh looks like a linear combination of two parts um the vector search and key search um in this video we implemented fi search by using the functionality of the elastic search so then we um we borrowed the code from The alexas Notebook where he implemented the vector search and the vector search Pipeline and calculated the Matrix uh we uh slightly modify this code by um by adding keyword matching to the search and then we run all these three cases for the question bendings for the answer bendings and question with answer Bings and calculated the metrix in each case as as you can see we have managed to improve our metrics of course I will add all this uh notebooks and all these links to the GitHub repo um and I think it's it's enough for this video thank you very much see you soon\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'In this video, we discuss the hybrid search concept and then implement it using embedded Elasticsearch functionality.',\n",
       "  'link': 'https://www.youtube.com/watch?v=TQ_ck6Q9gSQ',\n",
       "  'text_vector': \"LLM Zoomcamp 6.2 - Hybrid Search hello everyone this is video number two of the module six of LM Zoom camp in this module we discuss best practices different tricks and techniques to improve our R pipeline in the previous video I give you a brief overview of the five different techniques these five are just we can say the most popular and as I mentioned before we are not going to explore each Topic in details we will just focus only on two of these techniques aarch and ranking and this video is exactly about the H search H search is based on the combination of two methods Vector search and keyword search in Vector search we use vectorization to transform our initial text into embeddings or in other words into a bunch of numbers we make the edings of the source documents put them into our database and then we find the documents that have the closest distance to the user's query in the embedding space keyword search is more about one to one match over the words so the idea of hybrid search is to combine both approaches in one Pipeline and the question is how we can do that so we can do that in a pretty straightforward way basically each search here on theal part or dble stage produces results ranking by scores and we can take these scores and me them by using such simple equation in this equation Alpha determines U the weights assigned to each score with values typically ranging between 0 and one so in the case of alpha equals to one we get view Vector search otherwise this IP equals to zero so our equation transforms to the pure K and that's all about this Z now it's time to implement a hybd search what we have here so I have already launched a python in my code spaces environment I also started elastic search in the do container in a spaces also and prepared a old sketch so in this module we are going to use code from the notebooks from the module number three in the module three we cover topics related uh to the vector search uh vector. databases vector and sematic search and Ral part also also about evaluation matrics and so on and uh I just opened one of these notebooks to be precise this one evaluate Vector so in this notebook Alexi implemented a vector [Music] search and uh write all evolation functions and culate several metcs like heat rate and Mr and I just uh copy paste first lines of this code from The alexas Notebook to this new notebook of course I made some cosmetic changes for example I move all the Imports to the beginning of this notebook deleted some useless cells and so on but the whole pipeline stay the same we imported the necessary modules then we loaded our documents with questions answers and documents IDs then we loaded model uh that we that we use to build edings for our documents and finally set up elastic search settings and index our documents so in this blocks I guess this is the most important or most interesting part because in this blocks we have Vector search implementation by using KNN algorithm so let's try to experiment with this part now we want to change this Vector search in such a way as to turn it into hybrid search we know that we are using elastic search for storing our data for storing our documents that's why we can directly go to the Google or maybe to the official documentation and try to find information about hybrid search this is what I have already done and uh I found a couple of pages related to hybrid search impementation in elastic search so as you can see from this uh page to build our hrid search it will be enough just to take two uh parts I mean Vector search and our standard keyword search and put everything together in one in one place in this case the final score for each hit will be calculated the sum of the KNN and query scores we can also specify a boost par here to give a weight uh to each score in our sum so uh let's try to repeat this so uh here we have a search yeah and uh for implementing a hybrid search we need to copy paste a piece of code from another notebook in module 3 where Alex implemented just a simple uh keyw matching search so this not look called evaluate text and here we need just this part let's call it here search and here we have our default uh question like I just discovered the course can I still join it now we are going to vectorize our initial query users query now we are running can query ah yeah also we need to add uh of course we need to add a filter here and uh let me change this part my this key yeah get and there have C name let me check it cross yeah now I found this problem that was because we don't need these two lines of code Al we just need to combine Canon query and key query and now as you can see it works also what we we can do we can add this boost values to each part by default we can use uh 0.5 values and let me check play to move it inside want to match as you can see our scores in our search they changed that's all I think we implemented uh simple I resarch now I guess it's time to rewrite the whole pipeline let's call it um I research we line here I'm also my I'm going to take Cod Snippets from the Alexa snow look so what we need here of course we should load ground through dos um from this one so what we need we need also functions to culate our Matrix like great and Mr [Music] score so cells also we need the elastic search [Music] function but we are of course have to modify it and we need also this relation function and [Music] need this question can so now we are going to the right we will start from the elastic search Cann let's rename it like El search F and here we need to put course we need to put our modified [Music] pm and and also we need to add keyword very part yeah I also forget to mention one important thing I guess here we don't need to implement boosting for separate parts of our uh keyword search like we do in the first modules maybe you remember that we waited additionally U boosted addition of the question part just uh to make sure that our search algorithm will pay much more attention to this part but now I guess we don't need this that's why I deleted this boosting because uh now we are going to play with this boost variables just to to find the trade all between the vector search and um keyword search so let's try to add here our qu part G and also we need specify the size yeah it's the size of the theant answers that will return our search because K algorithm return five five elements here also our keyword search where return five elements uh it would be a 10 elements in some but we don't need to change the length of our answer that's why we need to specify the size uh of our query here and I think that's all next we need to modify this uh question Vector function let's call it just question h and uh yeah I forget just to change the list of params that our function elastic search hrid will take so we need here build the vector uh yeah also we need quy here because quy is needed for our keyword search and now we need also change the power punction here so it should be you stay the same Vector course and we need to change sare here would be question Y and last think we need to check our evolation function it seems like we don't need to make any changes in our evolation function now we can just call our evolation function to run the whole Pipeline and calculate our scores our Matrix heat rate and [Music] Mr also I would like to copy paste the score so that Alexa got by calling this function using simple Vector search I just put it heretic search sear and we need to pass to our Rel function ground TRS and our such function that hold question hybrid sear Canon is not defined yeah here we need also to change the name of our function purose yeah it will take some time to calculate our scores to run the whole pipeline what we can do while while we are waiting we of course can copy paste other functions uh to run it not only on questions but also on answers on documents answers and also as you remember we buil three types of embeddings on our questions and our answers and uh the third one the third one was U we combined just conate answers is questions and also build a bendings so uh for each of these cases we need to write the independent uh function so the second one will be this one let's name it text [Music] other pars we don't need to change them I guess and uh need to change the name of our search function yeah the first calculation was finished and as you can see we improved our scores the heat rate was increased by uh I think by 15% also we we improved our Mr score significantly that's great so now let's try to run our next function we calculate uh we calculate this course using KNN under uh can over the answers of our questions yeah there the last one our research um questions and answers what your name I [Music] so our second run also improved our Matrix and Mr but in a comparison with our po run where we implemented search over than only questions so I can say that we get better scores here looks like we get the same scores yeah but anyway they better than the scores that we got using just the simple Vector search and it's finished now we can compare our scores with the scores that Alexi got in his notebook Y and in the case number three where we used question answers uh embeddings we got the Matrix the metric Valu is slightly better than in previous cases and also better than lxa got in the modle three a pure Vector search so it seems that our har search has a benefits I'm pretty sure that's not the limit and we can further increase our metrix by by playing with this boost PM Bo values for the instance we can increase the weight value for k for km part for example and that will increase its impact on the final score but of course we need to carry on some additional experiments to prove this Theory so let's summarize uh H research uh looks like a linear combination of two parts um the vector search and key search um in this video we implemented fi search by using the functionality of the elastic search so then we um we borrowed the code from The alexas Notebook where he implemented the vector search and the vector search Pipeline and calculated the Matrix uh we uh slightly modify this code by um by adding keyword matching to the search and then we run all these three cases for the question bendings for the answer bendings and question with answer Bings and calculated the metrix in each case as as you can see we have managed to improve our metrics of course I will add all this uh notebooks and all these links to the GitHub repo um and I think it's it's enough for this video thank you very much see you soon In this video, we discuss the hybrid search concept and then implement it using embedded Elasticsearch functionality.\",\n",
       "  'id': '3596ba2811fa80be4b0a302906174e6a'},\n",
       " {'vid_id': 'H4M55Ptc5cM',\n",
       "  'title': 'LLM Zoomcamp 6.3 - Document Reranking',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"welcome back this is module six of the LM Zoom camp in this module we are speaking about different best practices that can improve your rock pipeline in a previous video we covered a hybrid search we discussed how it works then implemented it using function analy of the elastic search and improved our metrics the heat rate and Mr so in this video we will speak about the next technique called documentary ranking first of all let's repeat what the problem we are dealing with here on the slide you can see our Pipeline with a hybrid search part that effectively combines various search techniques like vector and keyword searches both the vector and keyword search interact with the database to retrieve a relevant chks of data the relevant chunks from both searches are combined into a single set and then pass through a reranking process because we need the limited amount of chunks that we want to show them to our language model during this process each Chun is assigned a score based on its relevance with the user's query the chunks are sorted based on their course with the most rant chance appearing at the top so reranking is a process of pre-ordering a set of retrieved documents in order to improve such relevance its core principles involves calculating the relevant score between the user's question and the each document then returning a list of documents ordered by relevance from the highest to lowest typically the search process precedes rank as calculating rence scores between a query and millions of documents is highly inefficient therefore ranking is often positioned at the end of the search process making it ideal for managing and sourcing results from various search systems how we can Implement document ranking there are multiple techniques starting from the simple implementations like various her istics and uh rules and using specific models for ranking our chunks also one of the way one of the ways how we can do that is again leveraging the language model in this case we actually have two calls to the language model the first one asking to rank our chunks of texts and the second one answering the question using the top chunks for the from the ranking list that we got from the previous t and of course we should build our reranking process based on some relevance score on the slide you can see some common options that we can take like NCG main average Precision reciprocal rank fusion and so on you can Google some extra information about this metrix or you can just ask a charge PT and in this video we will use reciprocal rank Fusion or RF for short that combines the ranks of documents from multiple search methods you can ask why not just use the cin similarity here so the cassin similarity is a good measure for the calculating the distance between two embeddings but may not necessarily be the best measure to actually reflect how relevant this Chun to the initial user query at least because in our case we use a combination of the search techniques but not only a vector search well move on and what about about the ranking in the elastic search let's add rest to the documentation there are multiple ways how we can Implement R ranking in the elastic search by using building function anality or calling the external models and apis I think we will start with a simple approach in our search function we can specify we can specify a special param called rank where we can pass ourf algorithm for reranking here RF algorithm means the ranking method based on reciprocal rank fusion score that I mentioned on the previous slide you can find a lot of information about this method in the internet also in the elastic search database for example on this page you can see the formula that determines how to calculate the rank for each document so it seems like it couldn't be a big deal to implement this approach let's do it now I'm going to switch to our Jer notbook here we have the same environment and elastic search is running in the docket container in the background and uh now I'm going to open our previous uh notebook with a hybd search implementation as you remember how we can get started here first of all let's create a new section let's call it the ranking so next we can copy paste our H search function that we implemented on the previous step here is it and now we can just add a new [Music] parm rank our existing function I will add it here in search query and also let's rename our function can just add r to our function name and now I guess we're ready to call our function also we need here to copypaste question it's vector and the course name and function yeah one important thing that I forgot to remember that we need to use the elastic search of the appropriate version in the previous modules we uh launched elastic search in the do container elastic search uh with a version of I'm not mistaken yeah this one uh 8.4.3 so this version of the elastic search doesn't support our uh ranking algorithm I mean RF so we need to upgrade it we need to pull the docker image with a more recent version of elastic search it should be I'm not mistaken 8.90 yeah so you should launch this version of the elastic search to use this uh R ranking algorithm yeah now we can run ourselves here is our standard question and yeah here we expect to get an error as you can see according to the description of this error we don't have the appropriate license to use our reciprocal rank Fusion algorithm why we got this error so let's refer to to the documentation of the elastic search to be precise to the subcription plans so according to the subscription plans our free version of the elastic search doesn't support this reciprocal rank Fusion feature and that's the problem on the one hand we have functionality in elastic search that would help us to rank our results our documents uh we can say out of box but on the other hand this functionality isn't available in our free version of elastic search what can we do of course we can upgrade our version of the elastic search we can find maybe I don't know any PR version of the elastic search and run it but what uh if we will try to implement the RF functionality uh on our own so we we can just take and write our own function to calculate the ranking scores of course in uh this case we could rely on also on the documentation so here we have the formula how to calculate this R score we can just copy it and then Rew write our elastic search function that will take into account or this score on the stage of ranking our results R ranking of our documents yeah let's try to do it I just will clear this output now let's create new section called let it be uh yeah so I don't want to waste your time by Rew writing this code I will just copy paste the code that I prepared before and will explain some lines that is the most important I guess in this piece of code so what we have here so this function is a straightforward implementation of the our reciprocal rank Fusion function here we have just uh two parms the rank and the constant K if we compare our implementation it looks like this uh like this formula and uh the uh the left part and the r part is nothing more than our hybrid function uh in which I have made the several several changes let me explain what I did uh first of all I splited the hyd search into uh two parts uh as you can see here we independently make vector and keyword searches and each search returns its uh results with score after that we created the empty Target dictionary for the rence course uh then uh using the defin above RF function we uh calculated rence Force for the set of documents that were provided by the vector search then we repeat the same the same procedure for the documents that we got from the keyword search in in case of having the same uh do ID in the Target dictionary we just add the calculated their score to the existing values and then we just sorting our Target uh dictionary in in a descending order and finally we just return the top five documents with the highest values of this score yeah that's it now uh let's test our new modified function um I want just to copy paste the rest part of our pipeline so here we need the call of the function and also the evalation the call of the evaluation function uh let's rename this functions a little bit I believe that's all now we can try to run ourselves no yeah you need to Define our relation function of course yeah course we also need to import our ground Truth download our ground documents also our Matrix so we started our calculations and uh I guess we can also copy paste the results from our previous step when we implemented just a few hybrid Source without ranking yeah here we just using the embeddings for for the questions with with the answers with the text we are not going to to run our function for all types of embeddings I guess we can just take the last one because we get the best scores just on on this TR yeah of course it will take some time I just put my video on the pause and will return when it finishes so all the calculations took about 3 minutes as you can see we have improved each metric by by 2 or 3% it looks pretty good I think we can conclude that the document ranking has helped to improve our [Music] pipeline I think that we can stop here and wrap up in this video we discuss the concepts of the document ranking technique then we implement it we can say manually just by using the functionality of the elastic search and our make functions of course there are multiple other ways where we can build it especially using external tools or some other apis but it would be a topic of our next video so see you soon bye\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'In this video, we cover the topic related to the document reranking that were retrieved by the search algorithms on the previous step. We use Reciprocal rank fusion method to implement reranking in Elasticsearch.',\n",
       "  'link': 'https://www.youtube.com/watch?v=H4M55Ptc5cM',\n",
       "  'text_vector': \"LLM Zoomcamp 6.3 - Document Reranking welcome back this is module six of the LM Zoom camp in this module we are speaking about different best practices that can improve your rock pipeline in a previous video we covered a hybrid search we discussed how it works then implemented it using function analy of the elastic search and improved our metrics the heat rate and Mr so in this video we will speak about the next technique called documentary ranking first of all let's repeat what the problem we are dealing with here on the slide you can see our Pipeline with a hybrid search part that effectively combines various search techniques like vector and keyword searches both the vector and keyword search interact with the database to retrieve a relevant chks of data the relevant chunks from both searches are combined into a single set and then pass through a reranking process because we need the limited amount of chunks that we want to show them to our language model during this process each Chun is assigned a score based on its relevance with the user's query the chunks are sorted based on their course with the most rant chance appearing at the top so reranking is a process of pre-ordering a set of retrieved documents in order to improve such relevance its core principles involves calculating the relevant score between the user's question and the each document then returning a list of documents ordered by relevance from the highest to lowest typically the search process precedes rank as calculating rence scores between a query and millions of documents is highly inefficient therefore ranking is often positioned at the end of the search process making it ideal for managing and sourcing results from various search systems how we can Implement document ranking there are multiple techniques starting from the simple implementations like various her istics and uh rules and using specific models for ranking our chunks also one of the way one of the ways how we can do that is again leveraging the language model in this case we actually have two calls to the language model the first one asking to rank our chunks of texts and the second one answering the question using the top chunks for the from the ranking list that we got from the previous t and of course we should build our reranking process based on some relevance score on the slide you can see some common options that we can take like NCG main average Precision reciprocal rank fusion and so on you can Google some extra information about this metrix or you can just ask a charge PT and in this video we will use reciprocal rank Fusion or RF for short that combines the ranks of documents from multiple search methods you can ask why not just use the cin similarity here so the cassin similarity is a good measure for the calculating the distance between two embeddings but may not necessarily be the best measure to actually reflect how relevant this Chun to the initial user query at least because in our case we use a combination of the search techniques but not only a vector search well move on and what about about the ranking in the elastic search let's add rest to the documentation there are multiple ways how we can Implement R ranking in the elastic search by using building function anality or calling the external models and apis I think we will start with a simple approach in our search function we can specify we can specify a special param called rank where we can pass ourf algorithm for reranking here RF algorithm means the ranking method based on reciprocal rank fusion score that I mentioned on the previous slide you can find a lot of information about this method in the internet also in the elastic search database for example on this page you can see the formula that determines how to calculate the rank for each document so it seems like it couldn't be a big deal to implement this approach let's do it now I'm going to switch to our Jer notbook here we have the same environment and elastic search is running in the docket container in the background and uh now I'm going to open our previous uh notebook with a hybd search implementation as you remember how we can get started here first of all let's create a new section let's call it the ranking so next we can copy paste our H search function that we implemented on the previous step here is it and now we can just add a new [Music] parm rank our existing function I will add it here in search query and also let's rename our function can just add r to our function name and now I guess we're ready to call our function also we need here to copypaste question it's vector and the course name and function yeah one important thing that I forgot to remember that we need to use the elastic search of the appropriate version in the previous modules we uh launched elastic search in the do container elastic search uh with a version of I'm not mistaken yeah this one uh 8.4.3 so this version of the elastic search doesn't support our uh ranking algorithm I mean RF so we need to upgrade it we need to pull the docker image with a more recent version of elastic search it should be I'm not mistaken 8.90 yeah so you should launch this version of the elastic search to use this uh R ranking algorithm yeah now we can run ourselves here is our standard question and yeah here we expect to get an error as you can see according to the description of this error we don't have the appropriate license to use our reciprocal rank Fusion algorithm why we got this error so let's refer to to the documentation of the elastic search to be precise to the subcription plans so according to the subscription plans our free version of the elastic search doesn't support this reciprocal rank Fusion feature and that's the problem on the one hand we have functionality in elastic search that would help us to rank our results our documents uh we can say out of box but on the other hand this functionality isn't available in our free version of elastic search what can we do of course we can upgrade our version of the elastic search we can find maybe I don't know any PR version of the elastic search and run it but what uh if we will try to implement the RF functionality uh on our own so we we can just take and write our own function to calculate the ranking scores of course in uh this case we could rely on also on the documentation so here we have the formula how to calculate this R score we can just copy it and then Rew write our elastic search function that will take into account or this score on the stage of ranking our results R ranking of our documents yeah let's try to do it I just will clear this output now let's create new section called let it be uh yeah so I don't want to waste your time by Rew writing this code I will just copy paste the code that I prepared before and will explain some lines that is the most important I guess in this piece of code so what we have here so this function is a straightforward implementation of the our reciprocal rank Fusion function here we have just uh two parms the rank and the constant K if we compare our implementation it looks like this uh like this formula and uh the uh the left part and the r part is nothing more than our hybrid function uh in which I have made the several several changes let me explain what I did uh first of all I splited the hyd search into uh two parts uh as you can see here we independently make vector and keyword searches and each search returns its uh results with score after that we created the empty Target dictionary for the rence course uh then uh using the defin above RF function we uh calculated rence Force for the set of documents that were provided by the vector search then we repeat the same the same procedure for the documents that we got from the keyword search in in case of having the same uh do ID in the Target dictionary we just add the calculated their score to the existing values and then we just sorting our Target uh dictionary in in a descending order and finally we just return the top five documents with the highest values of this score yeah that's it now uh let's test our new modified function um I want just to copy paste the rest part of our pipeline so here we need the call of the function and also the evalation the call of the evaluation function uh let's rename this functions a little bit I believe that's all now we can try to run ourselves no yeah you need to Define our relation function of course yeah course we also need to import our ground Truth download our ground documents also our Matrix so we started our calculations and uh I guess we can also copy paste the results from our previous step when we implemented just a few hybrid Source without ranking yeah here we just using the embeddings for for the questions with with the answers with the text we are not going to to run our function for all types of embeddings I guess we can just take the last one because we get the best scores just on on this TR yeah of course it will take some time I just put my video on the pause and will return when it finishes so all the calculations took about 3 minutes as you can see we have improved each metric by by 2 or 3% it looks pretty good I think we can conclude that the document ranking has helped to improve our [Music] pipeline I think that we can stop here and wrap up in this video we discuss the concepts of the document ranking technique then we implement it we can say manually just by using the functionality of the elastic search and our make functions of course there are multiple other ways where we can build it especially using external tools or some other apis but it would be a topic of our next video so see you soon bye In this video, we cover the topic related to the document reranking that were retrieved by the search algorithms on the previous step. We use Reciprocal rank fusion method to implement reranking in Elasticsearch.\",\n",
       "  'id': 'ba8edf176ccccc07ab276dc10c7a391f'},\n",
       " {'vid_id': 'CRfg7tAsnUU',\n",
       "  'title': 'LLM Zoomcamp 6.4 - Hybrid search with LangChain',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"welcome back this is uh module six of the LM Zoom Camp so in this model we are speaking about different techniques how to improve our Rock pipeline in the previous videos uh we discussed such techniques as hybrid search and document ranking we implemented them just by using elastic search capabilities and in case of reranking we prepared our own ranking function based on the calculation of the reciprocal rank fusion score yeah that sounds great um besides in the previous modules of the LM Zoom camp we covered various topics related to LM orchestration uh recation also in one of the videos Alex demonstrated how to create a simple UI for our pipeline so in this way we use different tools and packages and uh spent a lot of time to build our end to end solution but what if we had a tool that would combine uh some of the steps and speed up the development process and decrease the whole uh time to Market so in such cases tools such as longchain come on a stage so what's L chain L chain is a tool or framing designed to simplify the development of applications powered by large language models as you can see it consists of different uh components and libraries some parts under the commercial use of course but um most of the libraries are open source longchain provides standard extendable interfaces and external Integrations for various components useful for building uh with LMS some components are built by a l chain um in other cases they rely on a third party Integrations sometimes they use a mix of solutions anyway longchain provides us out of the box functionality to create simple LM application build a chatboard or any kind of end to end rck pipeline so you can uh simply follow some of the some of these tutor else uh if you want yeah you're welcome so what we want to do in our case I think we will take the code from the previous videos where we implemented the hybrid search and documentary ranking and uh rewrite it using Lang chain framework and before we start I would like to notice that in this video we are not going to uh modify our preparation section um I mean the indexing stage as I mentioned in the first video we have two steps in in the Rock pipeline indexing stage and the retrieval stage on the indexing stage uh we pass our data F the database with it I mean we index uh documents in the elastic search and on the retrial stage we extract from the database the documents that the most um prevalent to to the user query and in our particular case we will not change this piece of code let's imagine um that we have already launched the elastic search in our U Docker container in the docker container and indexed our FAQ documents if you are interested in this topic U I mean how to index documents in the elastic search using the power of L chain I suggest you to refer to the documentation for example here on the elastic search tutorial Pages you can find uh the section about um chatboard implementation and one of this uh of the first steps here is the data ingestion uh this means how we can load spit and push our documents into elastic search using L chain packages and uh once again we don't want to change this part I mean this part uh and we we will proceed with the existing one therefore I have already created uh a new notebook and copy pasted uh the document indexing stage from our previous notebook let's call this section indexing stage yeah and this should be our retrieval part and uh we want to rewrite our this uh section I mean retal part using align chain and here is the good way to refer to the official documentation I think um as I mentioned before L chain consists of different components related to uh LMS and bedding models document loers Vector databases uh stores and so on and as we working with the elastic search as a key component in our pipeline for storing in and indexing our data it would be a good way just to find the component that is uh related to elastic search um where we can find it uh yeah and here is documentation that related to our elastic search component of course in order to use it we need to install uh the appropriate package and uh on this page we can find the explanation how to start to work with the uh elastic search so the key component I guess here the elastic SE Ser so object from the from the L chain but in our case I guess the most convenient way to interact with elastic search uh would be another component that called elastic search retriever so elastic search retriever is a wrapper under the elastic search uh client it's uh directly interacts with our elastic search client uh for example if we trying TR to implement some kind of search in our database it looks like uh we can say we call the search function from our base client and uh the most interesting thing I guess on this page is to how to implement the vector search yeah this part I just want to take this p of code just copy it and now we will try to rewrite it according to our query from the um previous notebook you just copy paste the queries for the keyword search and a for and now what we need we need to replace this part and this Vector search PA this example yeah and we also need to comment this line because as I mentioned before we are using elastic search uh with the pr subscription and our RF score metric isn't available by default in this free tril this fre trail version what we need else uh yeah so in this function I mean this uh example of HD search usage uh that we took from from a l chain we have a this embedding object so this embedding object is a model that we are using for building our Bings from our uh user query but uh as you remember in U in our previous notebook we just imported the model from the sentence Transformer package but this time we need to import this model or this object or class from the package of the L chain so uh well let's do it what else we need yeah of course we need to import this dict wrapper from the typing module and uh yeah of course we need to import our retrial yeah here is it um yeah the boosting of the question part and also make some changes so here we are building embedding inside our hybrid search function yeah and as you can see our hybrid search function returns both parts I mean keyword search dictionary and uh vectors search dictionary uh from the function and we pass this function into into the argument of our elastic saturat world to the body function and uh this is how it works also we need to pass the link or the URL to our elastic search as I told you us this under the put our standard uh elastic search client and also let's copy paste our test user query just to check it how it works and the course name [Music] yeah next build we also need to change it um here we are going to use our answers as a Content field so the answers from our documents from our FAQ I me and let's try to to use edding name is not defined I need to check the name oh yeah of course I just forgot to to download the model to assign it to this variable the name of power model here we want to use the same model as we did in our previous um in our previous notebook this one it's okay run it once again yeah and it works so this uh hybrid retriever that we used from L chain Library reduces the results the list of the answers the structure of the answers a little bit different from the our previous previous notebook from the results that we got in the previous notebook as you can see U blank chain produces its own uh document structure that contains uh the main part the main part that called page content if I'm not mistaken here lot of information yeah this page content this is the answers to our users query that we indexed in our elastic search database and uh the metadata contains some additional information about our records in the database like like the source section that can contains the section question names and uh course names also and also question IDs we can make a little bit simplify our output now let's save our results into separate variable we want to want to print the question let be need Source in this case and question field question let's say also first name and and the scores would be enough um yeah of course and here's our top results so if we want to output on the top cas I mean top five the most relevant documents from our database of course we need to pass um the size or we need to specify the size here there is it so well done now next step let's take the evaluation function the whole elastic search function from our previous notebook and uh let's try to rewrite the whole our pipeline just to calculate our scores like heat rate and RRM and this is our pipeline first of all we need to to read our ground for data file of course CU I want to call this section just H search this lines also so we need functions for calculating our Matrix our search hyrid function that uses the hybrid search function where we passing our query and this function also calls our elastic search Hy function and finally the evoluation meod and the only thing that we need to change here I think it would be our elastic search h function so these lines stay the same don't need to change our metrix functions so let's change this part uh so this is our elastic search client default client that calls the search function so we want to change this part by our new by our new function this our new part so let's leave this um method just inside our main elastic search fa function I think we don't need now the vector argument because we calculating embedding for our user query inside this function yeah and as a long chain using uh its own document format so we need to modify this part and just let's say we can take this function and our source is contained in the metadata check it now we can try to use our function just by passing one of the documents from our grand through data so let print the first element for data yeah we have question and the course of course so we need to pass to our function the following arguments field query and course I also need to change it name second argument is our commission the last one is the course name and uh yeah I Madeo yes seems like we get the correct [Music] results by the way you can also spfy on the fields [Music] here from the M that path yeah seems like it works the elastic search Hybrid function provided us the results we can check so our initial question is when does the course begin and here's the questions that he retri from our elastic search database that's the most relevant to our initial query so that means that we can now run the whole pipeline calculate our Matrix so let's do it so yeah let me check we need to add here question text Hy because we are going to use the embeddings of the not only the questions but also questions with answers uh we don't need this part because we are calculating embeding of our user query inside our hybrid query function we don't need this argument for course oh yeah now we can call our vation function finally while calculating our scores let's go to our previous notebook and find the results so we want to save these results and then compare them with the results that we will get after this calculation so it will take same time I just want to put the record this video on the on the pass and we will back soon so I am back and our calculations are completed and as you can see uh we' got the same results as last time in the previous notbook yeah because uh this markdown line I copy pasted from the our previous notebook and I guess that means that uh we did everything correct that's how it was supposed to be as we simply copy the existing working code and replace the Cod of of the original elastic search client to the um to the retrial component from the L chain but this component under the put interacts with the same elastic search client yeah so that's great and that's all oh for this video\",\n",
       "  'timecode_text': 'Full Transcript',\n",
       "  'description': 'In this video, we will get familiar with the LangChain framework and try to implement hybrid search using it.',\n",
       "  'link': 'https://www.youtube.com/watch?v=CRfg7tAsnUU',\n",
       "  'text_vector': \"LLM Zoomcamp 6.4 - Hybrid search with LangChain welcome back this is uh module six of the LM Zoom Camp so in this model we are speaking about different techniques how to improve our Rock pipeline in the previous videos uh we discussed such techniques as hybrid search and document ranking we implemented them just by using elastic search capabilities and in case of reranking we prepared our own ranking function based on the calculation of the reciprocal rank fusion score yeah that sounds great um besides in the previous modules of the LM Zoom camp we covered various topics related to LM orchestration uh recation also in one of the videos Alex demonstrated how to create a simple UI for our pipeline so in this way we use different tools and packages and uh spent a lot of time to build our end to end solution but what if we had a tool that would combine uh some of the steps and speed up the development process and decrease the whole uh time to Market so in such cases tools such as longchain come on a stage so what's L chain L chain is a tool or framing designed to simplify the development of applications powered by large language models as you can see it consists of different uh components and libraries some parts under the commercial use of course but um most of the libraries are open source longchain provides standard extendable interfaces and external Integrations for various components useful for building uh with LMS some components are built by a l chain um in other cases they rely on a third party Integrations sometimes they use a mix of solutions anyway longchain provides us out of the box functionality to create simple LM application build a chatboard or any kind of end to end rck pipeline so you can uh simply follow some of the some of these tutor else uh if you want yeah you're welcome so what we want to do in our case I think we will take the code from the previous videos where we implemented the hybrid search and documentary ranking and uh rewrite it using Lang chain framework and before we start I would like to notice that in this video we are not going to uh modify our preparation section um I mean the indexing stage as I mentioned in the first video we have two steps in in the Rock pipeline indexing stage and the retrieval stage on the indexing stage uh we pass our data F the database with it I mean we index uh documents in the elastic search and on the retrial stage we extract from the database the documents that the most um prevalent to to the user query and in our particular case we will not change this piece of code let's imagine um that we have already launched the elastic search in our U Docker container in the docker container and indexed our FAQ documents if you are interested in this topic U I mean how to index documents in the elastic search using the power of L chain I suggest you to refer to the documentation for example here on the elastic search tutorial Pages you can find uh the section about um chatboard implementation and one of this uh of the first steps here is the data ingestion uh this means how we can load spit and push our documents into elastic search using L chain packages and uh once again we don't want to change this part I mean this part uh and we we will proceed with the existing one therefore I have already created uh a new notebook and copy pasted uh the document indexing stage from our previous notebook let's call this section indexing stage yeah and this should be our retrieval part and uh we want to rewrite our this uh section I mean retal part using align chain and here is the good way to refer to the official documentation I think um as I mentioned before L chain consists of different components related to uh LMS and bedding models document loers Vector databases uh stores and so on and as we working with the elastic search as a key component in our pipeline for storing in and indexing our data it would be a good way just to find the component that is uh related to elastic search um where we can find it uh yeah and here is documentation that related to our elastic search component of course in order to use it we need to install uh the appropriate package and uh on this page we can find the explanation how to start to work with the uh elastic search so the key component I guess here the elastic SE Ser so object from the from the L chain but in our case I guess the most convenient way to interact with elastic search uh would be another component that called elastic search retriever so elastic search retriever is a wrapper under the elastic search uh client it's uh directly interacts with our elastic search client uh for example if we trying TR to implement some kind of search in our database it looks like uh we can say we call the search function from our base client and uh the most interesting thing I guess on this page is to how to implement the vector search yeah this part I just want to take this p of code just copy it and now we will try to rewrite it according to our query from the um previous notebook you just copy paste the queries for the keyword search and a for and now what we need we need to replace this part and this Vector search PA this example yeah and we also need to comment this line because as I mentioned before we are using elastic search uh with the pr subscription and our RF score metric isn't available by default in this free tril this fre trail version what we need else uh yeah so in this function I mean this uh example of HD search usage uh that we took from from a l chain we have a this embedding object so this embedding object is a model that we are using for building our Bings from our uh user query but uh as you remember in U in our previous notebook we just imported the model from the sentence Transformer package but this time we need to import this model or this object or class from the package of the L chain so uh well let's do it what else we need yeah of course we need to import this dict wrapper from the typing module and uh yeah of course we need to import our retrial yeah here is it um yeah the boosting of the question part and also make some changes so here we are building embedding inside our hybrid search function yeah and as you can see our hybrid search function returns both parts I mean keyword search dictionary and uh vectors search dictionary uh from the function and we pass this function into into the argument of our elastic saturat world to the body function and uh this is how it works also we need to pass the link or the URL to our elastic search as I told you us this under the put our standard uh elastic search client and also let's copy paste our test user query just to check it how it works and the course name [Music] yeah next build we also need to change it um here we are going to use our answers as a Content field so the answers from our documents from our FAQ I me and let's try to to use edding name is not defined I need to check the name oh yeah of course I just forgot to to download the model to assign it to this variable the name of power model here we want to use the same model as we did in our previous um in our previous notebook this one it's okay run it once again yeah and it works so this uh hybrid retriever that we used from L chain Library reduces the results the list of the answers the structure of the answers a little bit different from the our previous previous notebook from the results that we got in the previous notebook as you can see U blank chain produces its own uh document structure that contains uh the main part the main part that called page content if I'm not mistaken here lot of information yeah this page content this is the answers to our users query that we indexed in our elastic search database and uh the metadata contains some additional information about our records in the database like like the source section that can contains the section question names and uh course names also and also question IDs we can make a little bit simplify our output now let's save our results into separate variable we want to want to print the question let be need Source in this case and question field question let's say also first name and and the scores would be enough um yeah of course and here's our top results so if we want to output on the top cas I mean top five the most relevant documents from our database of course we need to pass um the size or we need to specify the size here there is it so well done now next step let's take the evaluation function the whole elastic search function from our previous notebook and uh let's try to rewrite the whole our pipeline just to calculate our scores like heat rate and RRM and this is our pipeline first of all we need to to read our ground for data file of course CU I want to call this section just H search this lines also so we need functions for calculating our Matrix our search hyrid function that uses the hybrid search function where we passing our query and this function also calls our elastic search Hy function and finally the evoluation meod and the only thing that we need to change here I think it would be our elastic search h function so these lines stay the same don't need to change our metrix functions so let's change this part uh so this is our elastic search client default client that calls the search function so we want to change this part by our new by our new function this our new part so let's leave this um method just inside our main elastic search fa function I think we don't need now the vector argument because we calculating embedding for our user query inside this function yeah and as a long chain using uh its own document format so we need to modify this part and just let's say we can take this function and our source is contained in the metadata check it now we can try to use our function just by passing one of the documents from our grand through data so let print the first element for data yeah we have question and the course of course so we need to pass to our function the following arguments field query and course I also need to change it name second argument is our commission the last one is the course name and uh yeah I Madeo yes seems like we get the correct [Music] results by the way you can also spfy on the fields [Music] here from the M that path yeah seems like it works the elastic search Hybrid function provided us the results we can check so our initial question is when does the course begin and here's the questions that he retri from our elastic search database that's the most relevant to our initial query so that means that we can now run the whole pipeline calculate our Matrix so let's do it so yeah let me check we need to add here question text Hy because we are going to use the embeddings of the not only the questions but also questions with answers uh we don't need this part because we are calculating embeding of our user query inside our hybrid query function we don't need this argument for course oh yeah now we can call our vation function finally while calculating our scores let's go to our previous notebook and find the results so we want to save these results and then compare them with the results that we will get after this calculation so it will take same time I just want to put the record this video on the on the pass and we will back soon so I am back and our calculations are completed and as you can see uh we' got the same results as last time in the previous notbook yeah because uh this markdown line I copy pasted from the our previous notebook and I guess that means that uh we did everything correct that's how it was supposed to be as we simply copy the existing working code and replace the Cod of of the original elastic search client to the um to the retrial component from the L chain but this component under the put interacts with the same elastic search client yeah so that's great and that's all oh for this video In this video, we will get familiar with the LangChain framework and try to implement hybrid search using it.\",\n",
       "  'id': 'bd902e4c82ab9094a4c0325df661d34c'},\n",
       " {'vid_id': 'E9O0Tg68PPg',\n",
       "  'title': 'LLM Zoomcamp 7.1 - Fitness Assistant project',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hi everyone in this series of videos I want to show you an end to end project according to this crateria so I'm looking right now at the project page from our llm Zoom camp and I want to build a project that should satisfy uh many if not all of these criteria so let's give it a try and as a data source I want to use um so this is my own database with x sizes so it's like half Russian half English mostly Russian so I don't want to attempt to use this exact data set so what I will do instead is I want to generate a similar data set using um llms and then we will use this data set later uh for for our project and it's going to be fun because I have not rehearsed it so I'm going to do this project live with you right now well live it's recorded but anyways um and let's see how it goes so I want to start with uh creating the data set right or maybe even before that uh I need to create a repo a GitHub repo for\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=E9O0Tg68PPg&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.1 - Fitness Assistant project hi everyone in this series of videos I want to show you an end to end project according to this crateria so I'm looking right now at the project page from our llm Zoom camp and I want to build a project that should satisfy uh many if not all of these criteria so let's give it a try and as a data source I want to use um so this is my own database with x sizes so it's like half Russian half English mostly Russian so I don't want to attempt to use this exact data set so what I will do instead is I want to generate a similar data set using um llms and then we will use this data set later uh for for our project and it's going to be fun because I have not rehearsed it so I'm going to do this project live with you right now well live it's recorded but anyways um and let's see how it goes so I want to start with uh creating the data set right or maybe even before that uh I need to create a repo a GitHub repo for Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'c02e16a8e6d11f37c5446e83bfd71c04'},\n",
       " {'vid_id': 'E9O0Tg68PPg',\n",
       "  'title': 'LLM Zoomcamp 7.1 - Fitness Assistant project',\n",
       "  'timecode': '01:15',\n",
       "  'text': \"that right so go to my GitHub account and then uh yeah I'll create a repo which I will call [Music] ex exercises Rock so yeah this will be so let's call it like that or maybe Fitness assistant I hope I spell it correctly this is your friendly Fitness ass assistant uh which is a rock application built as a part of LM Zoom Camp um something like that right so let's include R me get ignore will be Python and let's create repo so now I will I want to develop locally you can also use of course you can use um Cod spaces I I'm just more used to my local environment so I will now on it to uh my git folder come on so get Lo this so Fitness assist and I open my my visual studio code and I want to so uh as a model I want to use uh I'm I'm just trying to move the panel from Loom that this is the thing I'm I use for recording uh\",\n",
       "  'timecode_text': 'Creating a Github repository',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=E9O0Tg68PPg&t=75s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.1 - Fitness Assistant project that right so go to my GitHub account and then uh yeah I'll create a repo which I will call [Music] ex exercises Rock so yeah this will be so let's call it like that or maybe Fitness assistant I hope I spell it correctly this is your friendly Fitness ass assistant uh which is a rock application built as a part of LM Zoom Camp um something like that right so let's include R me get ignore will be Python and let's create repo so now I will I want to develop locally you can also use of course you can use um Cod spaces I I'm just more used to my local environment so I will now on it to uh my git folder come on so get Lo this so Fitness assist and I open my my visual studio code and I want to so uh as a model I want to use uh I'm I'm just trying to move the panel from Loom that this is the thing I'm I use for recording uh Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '9da97e1fe4e065489ddd40a607de6a7c'},\n",
       " {'vid_id': 'E9O0Tg68PPg',\n",
       "  'title': 'LLM Zoomcamp 7.1 - Fitness Assistant project',\n",
       "  'timecode': '03:20',\n",
       "  'text': \"so I think it's n RC um so here I want to use um GPT 40 mini and for that we need um uh we need to have the key right and I'll create a template template so this is NRC will not go to G ignore but the template the template will not be ignored so this I don't want to commit this because it will contain a key and now I go to open platform open AI and I let me log in and I want to create a key specifically for this project so I can create a new project Fitness assistant and probably somehow how do I actually do this um I can create an API key create new secret key or this Fitness assistant um def key so I'm going to create it right now because I will only share this video later and by the time I share this key will be deactivated so I'm not afraid of sharing it with you um so here what is the varable name we use for keys I don't remember so let me look it up in our so we put it um I don't remember what is the variable name I'll quickly look it uh in our LM Zoom Camp CU I will see that now it's loaded open the I ke cool uh so now I right expert this thing okay now in our template I'll your yeah right so our first step now we need to import a bunch of uh install a bunch of things I will use pen\",\n",
       "  'timecode_text': 'Configuring OpenAI and setting up the key',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=E9O0Tg68PPg&t=200s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.1 - Fitness Assistant project so I think it's n RC um so here I want to use um GPT 40 mini and for that we need um uh we need to have the key right and I'll create a template template so this is NRC will not go to G ignore but the template the template will not be ignored so this I don't want to commit this because it will contain a key and now I go to open platform open AI and I let me log in and I want to create a key specifically for this project so I can create a new project Fitness assistant and probably somehow how do I actually do this um I can create an API key create new secret key or this Fitness assistant um def key so I'm going to create it right now because I will only share this video later and by the time I share this key will be deactivated so I'm not afraid of sharing it with you um so here what is the varable name we use for keys I don't remember so let me look it up in our so we put it um I don't remember what is the variable name I'll quickly look it uh in our LM Zoom Camp CU I will see that now it's loaded open the I ke cool uh so now I right expert this thing okay now in our template I'll your yeah right so our first step now we need to import a bunch of uh install a bunch of things I will use pen Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'b34bcd1987b24f589de24fe1e6b3404b'},\n",
       " {'vid_id': 'E9O0Tg68PPg',\n",
       "  'title': 'LLM Zoomcamp 7.1 - Fitness Assistant project',\n",
       "  'timecode': '06:00',\n",
       "  'text': \"I already have it P and install um so we will use um open AI uh we will use I want to use I'll use mean search for search so I'll not use elastic search I want to keep it lightweight csy it learn csy it learn pandas um to mean search you needs pandas then stream let's instead of using stream lead use flask um so will create um an API uh so we already saw how to create uh stream lead application in uh in the course here we will use slightly a slightly different tool I'll use flask then we also need Jupiter but Jupiter more for development so right now let me just use let me just install this ones and then I'll install Jupiter as a uh development dependency so I'll put it on pause while it's in it's installing and then come back when it finishes okay it finished and now I'll do peep and install Dev for other things like I'll simply copy it from here we need to qdm notebook oops uh yeah I have this habit of using wait wait wait where it is of using contrl shift C for copying it'sall Def and we also need IP widgets prob so now what we have is this speit pile with package and DEP packages and then uh yeah I use um 3.12 Python and let us create a folder I'll call notebooks where we will do like the initial U uh let's say exploration um so while it's installing uh I showed you the data set I want to use but I also said that uh like this data set is um\",\n",
       "  'timecode_text': 'Installing dependencies with pipenv',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=E9O0Tg68PPg&t=360s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.1 - Fitness Assistant project I already have it P and install um so we will use um open AI uh we will use I want to use I'll use mean search for search so I'll not use elastic search I want to keep it lightweight csy it learn csy it learn pandas um to mean search you needs pandas then stream let's instead of using stream lead use flask um so will create um an API uh so we already saw how to create uh stream lead application in uh in the course here we will use slightly a slightly different tool I'll use flask then we also need Jupiter but Jupiter more for development so right now let me just use let me just install this ones and then I'll install Jupiter as a uh development dependency so I'll put it on pause while it's in it's installing and then come back when it finishes okay it finished and now I'll do peep and install Dev for other things like I'll simply copy it from here we need to qdm notebook oops uh yeah I have this habit of using wait wait wait where it is of using contrl shift C for copying it'sall Def and we also need IP widgets prob so now what we have is this speit pile with package and DEP packages and then uh yeah I use um 3.12 Python and let us create a folder I'll call notebooks where we will do like the initial U uh let's say exploration um so while it's installing uh I showed you the data set I want to use but I also said that uh like this data set is um Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '1389fa8edeab160311381ee4e6bf455c'},\n",
       " {'vid_id': 'E9O0Tg68PPg',\n",
       "  'title': 'LLM Zoomcamp 7.1 - Fitness Assistant project',\n",
       "  'timecode': '08:27',\n",
       "  'text': \"like first of all it's an ocean I need to somehow figure out how to export this then it's like half Russian half English if I look inside there is uh a lot of Russ so let us just generate a similar data set with uh an llm so I'm not sure which one I'll use um for that um so Cho choose a or b randomly so a will be Char PT B will be clot let's see okay than spot so let's use chpd and uh I want to create a data set with all this um so now let me just take a screenshot of that and I want to generate a similar data set to this one um I need the following columns one right um English name or just accessor name his name then um yeah type of it uh what I have cardio warm up stretching strength cardio warm up Mobility uh and then strength did I spell it correctly I did not the next um type of equipment so what I need like resistance band resistance band uh dump Bell uh kettle bells uh bar well um what else and so on right um then next one is uh um body part I have just three categories upper body lower core lower body like it's basically arms and chest and back upper back core lower body like legs so yeah I was just wondering so on what I mean meant by so on like what exactly is there apart from legs but anyways then uh uh type push or pull or sometimes we have um like um static it's like plank when you just hold it and yeah actually hold hold or stretch hold stretch and uh must s groups activated as a comma separated list uh okay so something like that uh we will probably refine it um I need a list of I don't know how many I have let's say first let's generate let's generate 10 to see that the format is correct and then once we have by like once we figure out the format we can ask it to generate I don't know 500 different exercises so let's see what it has for us okay strength Mobility is plank Mobility but whatever um okay to me it actually looks great so uh now let's\",\n",
       "  'timecode_text': 'Creating the dataset',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=E9O0Tg68PPg&t=507s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.1 - Fitness Assistant project like first of all it's an ocean I need to somehow figure out how to export this then it's like half Russian half English if I look inside there is uh a lot of Russ so let us just generate a similar data set with uh an llm so I'm not sure which one I'll use um for that um so Cho choose a or b randomly so a will be Char PT B will be clot let's see okay than spot so let's use chpd and uh I want to create a data set with all this um so now let me just take a screenshot of that and I want to generate a similar data set to this one um I need the following columns one right um English name or just accessor name his name then um yeah type of it uh what I have cardio warm up stretching strength cardio warm up Mobility uh and then strength did I spell it correctly I did not the next um type of equipment so what I need like resistance band resistance band uh dump Bell uh kettle bells uh bar well um what else and so on right um then next one is uh um body part I have just three categories upper body lower core lower body like it's basically arms and chest and back upper back core lower body like legs so yeah I was just wondering so on what I mean meant by so on like what exactly is there apart from legs but anyways then uh uh type push or pull or sometimes we have um like um static it's like plank when you just hold it and yeah actually hold hold or stretch hold stretch and uh must s groups activated as a comma separated list uh okay so something like that uh we will probably refine it um I need a list of I don't know how many I have let's say first let's generate let's generate 10 to see that the format is correct and then once we have by like once we figure out the format we can ask it to generate I don't know 500 different exercises so let's see what it has for us okay strength Mobility is plank Mobility but whatever um okay to me it actually looks great so uh now let's Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '9699b7744cfed79aa73ab2dbfc54bdbb'},\n",
       " {'vid_id': 'E9O0Tg68PPg',\n",
       "  'title': 'LLM Zoomcamp 7.1 - Fitness Assistant project',\n",
       "  'timecode': '13:19',\n",
       "  'text': \"follow this format and generate a CSV file with exer sizes so and wondering what it's analyzing I don't know what it's analyzing ah okay so now it is just generating some uh python code to provide to create a CSV that's cool so I'll just put it on pause and then come back when it finishes um so I see that it um produced an error CU like it was too large I guess so what I want to do now is I just want to stop it and say instead of um writing a python pile for producing CSV um create a CSV CSV file directly in the output or produce so I wanted to just start generating it right here in the output don't use Python right it here uh maybe it's uh cuz like it needs to be a properly parsable CSV maybe that's why it's trying trying to uh use uh python right here use uh for use and for separating um what comes okay so I think it should be Parable great now produce wait I think we forgot one important thing like how exactly the exercise should be done right wait we forgot one thing instructions to perform the exercise so regenerate regenerate regenerate the 10 sizes okay that's good so this I think should be possible we can quickly test it when it finishes uh I think it finished installing so now I'll do pep and run Jupiter notebook and then copy this put it here file CSV and then let's try to parse it import PD PD CV data and then separator is this one yeah so it can parse it so now I will ask it to generate 500 generate I hope it doesn't use Python now okay like it says maybe it's just too much but it's fine like cuz we can just keep using and using and using it and then perhaps just remove um duplicates right so if there are duplicates so I'll put it on pause and then come back when it stops yeah it finished and then it says like okay you can just do it yourself of course I don't want uh please can please generate next 100 exer sizes and then perhaps we can just ask it to generate 100 and then 100 and then 100 um and then perhaps we will need to remove duplicates I don't know actually like it's quite good so like it continued from G so let's see I I think we can just remove uh duplicates later when it finishes so putting this on pause again uh before first I'll copy it here and then now uh yeah note that it included the header so we don't need the header right now I'm going to put it on pause and then come back then it finishes so note that it stopped generating and then I will just click continue generating and now it should just continue so I'm putting this on pause again so it finished generating it of course it's uh we should still check how like um sometimes there might be mistakes like maybe it's say it's a push type of exercise but it's actually it's a pull one but like for our our purposes it's fine so we don't need to forget to remove the the heater so here remove the heater and then we have maybe let me just generate um let's generate more so and then probably it should be enough or us and then we'll do a bit of cleaning I think this should be enough for us for now like I don't know if it's really 100 or not but yeah so yeah 210 like it said should be sufficient dat uh um how to unclean I want to to clean it now so now I read the data what of course so and now I want to drop duplicates just to see if we actually have any uh data frame yeah drop duplicates uh subset we will\",\n",
       "  'timecode_text': 'Generating a CSV file',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=E9O0Tg68PPg&t=799s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.1 - Fitness Assistant project follow this format and generate a CSV file with exer sizes so and wondering what it's analyzing I don't know what it's analyzing ah okay so now it is just generating some uh python code to provide to create a CSV that's cool so I'll just put it on pause and then come back when it finishes um so I see that it um produced an error CU like it was too large I guess so what I want to do now is I just want to stop it and say instead of um writing a python pile for producing CSV um create a CSV CSV file directly in the output or produce so I wanted to just start generating it right here in the output don't use Python right it here uh maybe it's uh cuz like it needs to be a properly parsable CSV maybe that's why it's trying trying to uh use uh python right here use uh for use and for separating um what comes okay so I think it should be Parable great now produce wait I think we forgot one important thing like how exactly the exercise should be done right wait we forgot one thing instructions to perform the exercise so regenerate regenerate regenerate the 10 sizes okay that's good so this I think should be possible we can quickly test it when it finishes uh I think it finished installing so now I'll do pep and run Jupiter notebook and then copy this put it here file CSV and then let's try to parse it import PD PD CV data and then separator is this one yeah so it can parse it so now I will ask it to generate 500 generate I hope it doesn't use Python now okay like it says maybe it's just too much but it's fine like cuz we can just keep using and using and using it and then perhaps just remove um duplicates right so if there are duplicates so I'll put it on pause and then come back when it stops yeah it finished and then it says like okay you can just do it yourself of course I don't want uh please can please generate next 100 exer sizes and then perhaps we can just ask it to generate 100 and then 100 and then 100 um and then perhaps we will need to remove duplicates I don't know actually like it's quite good so like it continued from G so let's see I I think we can just remove uh duplicates later when it finishes so putting this on pause again uh before first I'll copy it here and then now uh yeah note that it included the header so we don't need the header right now I'm going to put it on pause and then come back then it finishes so note that it stopped generating and then I will just click continue generating and now it should just continue so I'm putting this on pause again so it finished generating it of course it's uh we should still check how like um sometimes there might be mistakes like maybe it's say it's a push type of exercise but it's actually it's a pull one but like for our our purposes it's fine so we don't need to forget to remove the the heater so here remove the heater and then we have maybe let me just generate um let's generate more so and then probably it should be enough or us and then we'll do a bit of cleaning I think this should be enough for us for now like I don't know if it's really 100 or not but yeah so yeah 210 like it said should be sufficient dat uh um how to unclean I want to to clean it now so now I read the data what of course so and now I want to drop duplicates just to see if we actually have any uh data frame yeah drop duplicates uh subset we will Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'da9ec3f84643113dbedf1bacef4d45bd'},\n",
       " {'vid_id': 'E9O0Tg68PPg',\n",
       "  'title': 'LLM Zoomcamp 7.1 - Fitness Assistant project',\n",
       "  'timecode': '21:27',\n",
       "  'text': \"just use exercise name so actually there were some duplicates you see now we have fewer rows than before so and then now we will save it to data frame just data this see right so this will be our clean data uh I don't want to have index so this is our data set that we are going to use for the project Okay cool so now um clean the data set I will just do data frame um BD be\",\n",
       "  'timecode_text': 'Cleaning the dataset',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=E9O0Tg68PPg&t=1287s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.1 - Fitness Assistant project just use exercise name so actually there were some duplicates you see now we have fewer rows than before so and then now we will save it to data frame just data this see right so this will be our clean data uh I don't want to have index so this is our data set that we are going to use for the project Okay cool so now um clean the data set I will just do data frame um BD be Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '361993a7f329dd36c3ffcb033889c998'},\n",
       " {'vid_id': 'E9O0Tg68PPg',\n",
       "  'title': 'LLM Zoomcamp 7.1 - Fitness Assistant project',\n",
       "  'timecode': '22:25',\n",
       "  'text': \"CSV and we now will Index this with uh mean search so let me open I'll just use this thing our very first notebook and I'll just copy things from there so let's use um main search and um we already have this so I'll just turn it into a dictionary and and probably first columns so I want to uh make them lowercase right and I want uh replace uh how do I replace is there H it's replace yeah so I want to replace uh replace um SP spaces with underscores so this will be our names for cones um I think I should have written this before um oops where where did it go I should have written this uh I should have done this before writing it to clean um data index false so I don't need to do it and now if I look at our data frame so this how it looks like okay cool um so now again columns I need the list of columns CU we want to now index them with mean search so actually I don't know if we need any keyword Fields probably all of them will be just text Fields yeah I need to import Min search actually I don't think I tested Min search with zero key keyword Fields I think it should work yeah does index search um so query could be let's say um give me um La exercises exercises for um what um how do you call it hamstrings hamstring okay this will be our query right we need to of course index everything so I'll turn I'll create documents now will be two dictionary Orient records so now we have our documents right and index fit now we can do search and then let's uh number results 10 yeah I think 10 is okay right so we see lower body leg curl hamstrings hamstrings hamstrings then like press okay single leg deadlift okay is there usual deadlift I'm wondering CU there are could be like many hamstrings exercises and then um yeah maybe it will not not always return the most effective ones but let's um set this aside for now we can see that it works and now we can keep continue implementing our uh rack FL so let me now actually I don't know if I activated um dear\",\n",
       "  'timecode_text': 'Indexing the data with minsearch',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=E9O0Tg68PPg&t=1345s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.1 - Fitness Assistant project CSV and we now will Index this with uh mean search so let me open I'll just use this thing our very first notebook and I'll just copy things from there so let's use um main search and um we already have this so I'll just turn it into a dictionary and and probably first columns so I want to uh make them lowercase right and I want uh replace uh how do I replace is there H it's replace yeah so I want to replace uh replace um SP spaces with underscores so this will be our names for cones um I think I should have written this before um oops where where did it go I should have written this uh I should have done this before writing it to clean um data index false so I don't need to do it and now if I look at our data frame so this how it looks like okay cool um so now again columns I need the list of columns CU we want to now index them with mean search so actually I don't know if we need any keyword Fields probably all of them will be just text Fields yeah I need to import Min search actually I don't think I tested Min search with zero key keyword Fields I think it should work yeah does index search um so query could be let's say um give me um La exercises exercises for um what um how do you call it hamstrings hamstring okay this will be our query right we need to of course index everything so I'll turn I'll create documents now will be two dictionary Orient records so now we have our documents right and index fit now we can do search and then let's uh number results 10 yeah I think 10 is okay right so we see lower body leg curl hamstrings hamstrings hamstrings then like press okay single leg deadlift okay is there usual deadlift I'm wondering CU there are could be like many hamstrings exercises and then um yeah maybe it will not not always return the most effective ones but let's um set this aside for now we can see that it works and now we can keep continue implementing our uh rack FL so let me now actually I don't know if I activated um dear Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '60127264308512e845a5433ee584df1d'},\n",
       " {'vid_id': 'E9O0Tg68PPg',\n",
       "  'title': 'LLM Zoomcamp 7.1 - Fitness Assistant project',\n",
       "  'timecode': '27:10',\n",
       "  'text': \"en yeah so I need to restart it so I don't want to restart it so what I'll do right now is I'll just import OS and then OS environment environment like ideally I would restart just restart the notebook and then do de end follow and then it would pick the uh this key from the environment of variables but I feel lazy right now so what I will do right now is just copy the key put it here and then run it so for now I'll remove this so I don't accidentally committed of course uh but yeah so right now what happened is we just sent the query directly to uh GPT 40 and then yeah it gave some answer but of course we want to have D\",\n",
       "  'timecode_text': 'Configuring OpenAI',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=E9O0Tg68PPg&t=1630s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.1 - Fitness Assistant project en yeah so I need to restart it so I don't want to restart it so what I'll do right now is I'll just import OS and then OS environment environment like ideally I would restart just restart the notebook and then do de end follow and then it would pick the uh this key from the environment of variables but I feel lazy right now so what I will do right now is just copy the key put it here and then run it so for now I'll remove this so I don't accidentally committed of course uh but yeah so right now what happened is we just sent the query directly to uh GPT 40 and then yeah it gave some answer but of course we want to have D Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'af80794e54679fd6f4534ac0fcb656a2'},\n",
       " {'vid_id': 'E9O0Tg68PPg',\n",
       "  'title': 'LLM Zoomcamp 7.1 - Fitness Assistant project',\n",
       "  'timecode': '28:20',\n",
       "  'text': \"FL so I will now copy this entire Rock full remove this because we need the client but the rest um the search for now we will not do any boosting um let's say 10 build prompt so now we also need to update the prompt right um I will so let's keep this one here right and now let's spend some time working on the prompts so I'll put prompts template here outside of the function and then um can have entry template yeah we can have entry template so entry template we have documents here so I want to have like the entire thing here right so entry template could be um I'll probably do this in Visual Studio code CU I can do it faster so first I will do something something like this then I press crl D multiple times to uh select this thing then remove it and then what we want to do is I select it and then put it inside this thing so then it becomes a template kind of thing strip okay so this would be like an entry template so we will iterate over search results and then include um entry template format doc right so now we'll have the context um probably plus n n so that's right then prom template format uh question query uh okay and of course we need to uh update the prompt template let's say you are a fitness uh instructor answer the question based on the conent from our xer size database only use the fact from the context when answering the question yeah let's use that one so here so for now let's just test this one if this works build prompt search where then search results uh prompt build prompt query search results and let's see what we have in the prompt looks okay so now we can actually put this into an llm go into an llm let's use minimum model and then we have the rock thing right Ro okay so um what else can we ask um I want some po exercise that also help my back okay so I guess it works um I think so like there is probably room for improvement but we can see that yeah it works we can tweak The Prompt we can improve our database of course um but from what I can see it works quite fine so for now I'm\",\n",
       "  'timecode_text': 'Implementing the RAG flow',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=E9O0Tg68PPg&t=1700s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.1 - Fitness Assistant project FL so I will now copy this entire Rock full remove this because we need the client but the rest um the search for now we will not do any boosting um let's say 10 build prompt so now we also need to update the prompt right um I will so let's keep this one here right and now let's spend some time working on the prompts so I'll put prompts template here outside of the function and then um can have entry template yeah we can have entry template so entry template we have documents here so I want to have like the entire thing here right so entry template could be um I'll probably do this in Visual Studio code CU I can do it faster so first I will do something something like this then I press crl D multiple times to uh select this thing then remove it and then what we want to do is I select it and then put it inside this thing so then it becomes a template kind of thing strip okay so this would be like an entry template so we will iterate over search results and then include um entry template format doc right so now we'll have the context um probably plus n n so that's right then prom template format uh question query uh okay and of course we need to uh update the prompt template let's say you are a fitness uh instructor answer the question based on the conent from our xer size database only use the fact from the context when answering the question yeah let's use that one so here so for now let's just test this one if this works build prompt search where then search results uh prompt build prompt query search results and let's see what we have in the prompt looks okay so now we can actually put this into an llm go into an llm let's use minimum model and then we have the rock thing right Ro okay so um what else can we ask um I want some po exercise that also help my back okay so I guess it works um I think so like there is probably room for improvement but we can see that yeah it works we can tweak The Prompt we can improve our database of course um but from what I can see it works quite fine so for now I'm Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '2309b14ddf96ccd8e9e10f8557436c8b'},\n",
       " {'vid_id': 'E9O0Tg68PPg',\n",
       "  'title': 'LLM Zoomcamp 7.1 - Fitness Assistant project',\n",
       "  'timecode': '33:50',\n",
       "  'text': \"going to stop the video because otherwise it will be too long and then in the next one we will will continue with actually like we want to remember we want to tick all the boxes here in the evalation criteria for now I just implemented RL but of course like I only test it in jupyter Notebook we will need to move it somewhere else so I think it makes sense to now focus on yeah we can focus on evaluating this thing and then build the interface later I think I need to move the interface down here so then it makes it easier for you to follow yeah so the the criteria criteria are still work in progress but this is what we are going to do next so we implemented track flow now we're going to evaluate it and see if we need to tweak anything in our prompt or in our retrieval so see you soon\",\n",
       "  'timecode_text': 'Conclusions',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=E9O0Tg68PPg&t=2030s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.1 - Fitness Assistant project going to stop the video because otherwise it will be too long and then in the next one we will will continue with actually like we want to remember we want to tick all the boxes here in the evalation criteria for now I just implemented RL but of course like I only test it in jupyter Notebook we will need to move it somewhere else so I think it makes sense to now focus on yeah we can focus on evaluating this thing and then build the interface later I think I need to move the interface down here so then it makes it easier for you to follow yeah so the the criteria criteria are still work in progress but this is what we are going to do next so we implemented track flow now we're going to evaluate it and see if we need to tweak anything in our prompt or in our retrieval so see you soon Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'e3e06455d66b6022c3de6545bd80d362'},\n",
       " {'vid_id': '6ulnHtJPCWY',\n",
       "  'title': 'LLM Zoomcamp 7.2 - Evaluating retrieval',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hey welcome back so in this series of videos we are doing an end to end project and we use as an example a fitness assistant so we have in the previous video we prepared a data set using Char PT with different exercises type of equipment you need and so on and and how to perform the exercise and here in this uh video we will continue doing whatever we need to do in order to complete the project and um so what we covered so far was rlow so in this video I want to cover problem description and retrieval evalation right and rack evolation maybe to right and um yeah so let's start with the problem description so uh where is\",\n",
       "  'timecode_text': 'Introduction and project overview',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=6ulnHtJPCWY&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.2 - Evaluating retrieval hey welcome back so in this series of videos we are doing an end to end project and we use as an example a fitness assistant so we have in the previous video we prepared a data set using Char PT with different exercises type of equipment you need and so on and and how to perform the exercise and here in this uh video we will continue doing whatever we need to do in order to complete the project and um so what we covered so far was rlow so in this video I want to cover problem description and retrieval evalation right and rack evolation maybe to right and um yeah so let's start with the problem description so uh where is Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '25994209d1dc84c9aa306cdc1a1d8537'},\n",
       " {'vid_id': '6ulnHtJPCWY',\n",
       "  'title': 'LLM Zoomcamp 7.2 - Evaluating retrieval',\n",
       "  'timecode': '00:55',\n",
       "  'text': \"so I have the read me file here so we need to improve it uh there are already a few things uh we can uh mention here first of all running it uh um we use P file so we can already mention that now we use pen for managing uh dependent C and python 312 uh make sure you have P installed uh install p uh and then um we will add more instructions here as we do installing theend uh P install right so so far this what we covered and then also Jupiter notebook juper notebook for experiments speep and run and maybe it's a good idea to also CD to notebook right uh now we also need to write a description like what exactly we are trying to solve and uh here I want to use chat GPT of course so I'll create a new chat and right now I'm going to put it on pause write a prompt and then uh read The Prompt and the results and then we'll just we'll copy paste it okay so this is the prompt I wrote I'm writing documentation for my course project then I I pasted the criteria that um we have in the course and I say that I need your help and then the project I'm working is AR application for fitness and gym I called Fitness assistant then next I show the data set right and then say how many records are there and then we can chat with the assistant so I describe what we can do with this and then I ask to write a read me file that describes a problem the data and what we do and I ask to use markdown code block for output so this is the output I copy it and of course we will need to edit it and by the way I also moved data to the data directory um so here we have it I will remove that um I somehow don't like it this is a bit verbos so let's make it less even lesser boss yeah so I like this more even though I didn't really read it but I see that it's um yeah it's smaller um El well it doesn't really help learn proper form so more manageable maybe well we don't a to make it more excess okay so I think I like that it's uh clear concise um and then we have the data set description the data set using this project contains with various ex sizes data set was generating uspd and contains I like that um project overview Fitness assistant is a rock application I don't think we need that long thing uh for assisting users with their fitness routines I like that uh the features the main use cases I would say cases include exercise selection um exercise replacement ex instruction this is kind of not needed getting started and conclusions are not [Music] needed so let's see how it looks like problem description data set project overview M okay I think we can make it less con less uh verbose more concise example ask uh I recommend exercises exercise based on uh on type activity targeted muscle or will equipment uh replace an exercise uh uh with a suitable alternative uh and uh perform a specific exercise um um make it easy to get information uh without uh let say sifting through complex through manuals or websites I like it more so now it's more concise it's also kind of very easy to spot uh content generated by chpt CU always Le look like that first uh B um name of the bullet point and then um just the like the clarification so let's make it look like a bit less uh generated even though it's generated uh yeah we know that but um like the moment I see that there's a bullet point and there's bolt uh name and then like clarification then it's clearly chargeability content okay we use pen like doing like making it um look like code you have people have installed installing dependencies running jupter notebook for experiments okay for now should be enough we will keep adding more stuff here um maybe we'll add a picture here but later okay so what do we have next so problem description um yeah we will of course include more here rck flow covered retrieval evalation so let's do that now so I want to um also this is something we need to mention in rbe also check the uh tips and best practices uh give a project me make it easier for evaluators to evaluate each of the criteria by mentioning them in your read me and including screenshots so this is exactly what we're going to do we're going to uh have a section of evaluation now and we have two things here retrieval and track wall and then also monitoring monitoring what else do we have inje continue ation reability I think like this is kind of self- explained if you see Docker compos there then there's clearly conization you can look at that but of course you can also mention that so that the reviewers can just go through the project and immediately see that um so I'm not sure this will be the final order maybe inje will put uh here um okay so coming back to evaluation so we will need to right now perform retrieval evaluation and also tune it a little bit cuz like when you look at the criteria um like if we just evaluate the approach we already have we will get only one point we want of course two points so we will also play with our mean search engine uh we of course can compare it with some other search engine but also we can play bit with weights uh but for now what we need to do is generate a data set so let me go to\",\n",
       "  'timecode_text': 'Creating the first version of README',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=6ulnHtJPCWY&t=55s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.2 - Evaluating retrieval so I have the read me file here so we need to improve it uh there are already a few things uh we can uh mention here first of all running it uh um we use P file so we can already mention that now we use pen for managing uh dependent C and python 312 uh make sure you have P installed uh install p uh and then um we will add more instructions here as we do installing theend uh P install right so so far this what we covered and then also Jupiter notebook juper notebook for experiments speep and run and maybe it's a good idea to also CD to notebook right uh now we also need to write a description like what exactly we are trying to solve and uh here I want to use chat GPT of course so I'll create a new chat and right now I'm going to put it on pause write a prompt and then uh read The Prompt and the results and then we'll just we'll copy paste it okay so this is the prompt I wrote I'm writing documentation for my course project then I I pasted the criteria that um we have in the course and I say that I need your help and then the project I'm working is AR application for fitness and gym I called Fitness assistant then next I show the data set right and then say how many records are there and then we can chat with the assistant so I describe what we can do with this and then I ask to write a read me file that describes a problem the data and what we do and I ask to use markdown code block for output so this is the output I copy it and of course we will need to edit it and by the way I also moved data to the data directory um so here we have it I will remove that um I somehow don't like it this is a bit verbos so let's make it less even lesser boss yeah so I like this more even though I didn't really read it but I see that it's um yeah it's smaller um El well it doesn't really help learn proper form so more manageable maybe well we don't a to make it more excess okay so I think I like that it's uh clear concise um and then we have the data set description the data set using this project contains with various ex sizes data set was generating uspd and contains I like that um project overview Fitness assistant is a rock application I don't think we need that long thing uh for assisting users with their fitness routines I like that uh the features the main use cases I would say cases include exercise selection um exercise replacement ex instruction this is kind of not needed getting started and conclusions are not [Music] needed so let's see how it looks like problem description data set project overview M okay I think we can make it less con less uh verbose more concise example ask uh I recommend exercises exercise based on uh on type activity targeted muscle or will equipment uh replace an exercise uh uh with a suitable alternative uh and uh perform a specific exercise um um make it easy to get information uh without uh let say sifting through complex through manuals or websites I like it more so now it's more concise it's also kind of very easy to spot uh content generated by chpt CU always Le look like that first uh B um name of the bullet point and then um just the like the clarification so let's make it look like a bit less uh generated even though it's generated uh yeah we know that but um like the moment I see that there's a bullet point and there's bolt uh name and then like clarification then it's clearly chargeability content okay we use pen like doing like making it um look like code you have people have installed installing dependencies running jupter notebook for experiments okay for now should be enough we will keep adding more stuff here um maybe we'll add a picture here but later okay so what do we have next so problem description um yeah we will of course include more here rck flow covered retrieval evalation so let's do that now so I want to um also this is something we need to mention in rbe also check the uh tips and best practices uh give a project me make it easier for evaluators to evaluate each of the criteria by mentioning them in your read me and including screenshots so this is exactly what we're going to do we're going to uh have a section of evaluation now and we have two things here retrieval and track wall and then also monitoring monitoring what else do we have inje continue ation reability I think like this is kind of self- explained if you see Docker compos there then there's clearly conization you can look at that but of course you can also mention that so that the reviewers can just go through the project and immediately see that um so I'm not sure this will be the final order maybe inje will put uh here um okay so coming back to evaluation so we will need to right now perform retrieval evaluation and also tune it a little bit cuz like when you look at the criteria um like if we just evaluate the approach we already have we will get only one point we want of course two points so we will also play with our mean search engine uh we of course can compare it with some other search engine but also we can play bit with weights uh but for now what we need to do is generate a data set so let me go to Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '76acdcce9bc1a2dbbf3b9a8669b67fbe'},\n",
       " {'vid_id': '6ulnHtJPCWY',\n",
       "  'title': 'LLM Zoomcamp 7.2 - Evaluating retrieval',\n",
       "  'timecode': '10:29',\n",
       "  'text': \"our lsom camp and we did evaluation in the vector search model in the future editions uh cohorts I think what we will do is we will have a separate model on evaluation and a separate model on monitoring right now it kind of spr across multiple models I don't like that and also like there should be a larger model on Vector search so if you're watching this in the future right now this uh your the structure you have might look differently from what what I have in the video but anyways so here we have evation and um ground through data actually I remember when I generated the data set I uh removed the index from there I think it could be a good idea to actually have the index back so what I'll do is I'll just create column ID and I think I want to append this column so let me just call it [Music] ID I'm trying to remember how do I pend the coln at colum at the beginning ah I think it's insert right uh insert right insert uh location zero column ID value will be data frame index then right now this is what we have right so right now we will have ID for each of the exercises but also like on the other hand maybe we don't even need to have the ID cuz each exercise name is supposed to be unique so this is our ID so yeah I'll keep the ID just in case uh see data data this IND false I know for some reasons it stopped highlighting this index don't know why so now we have this ID and maybe it should actually be idea I'll remove the I want to make it lower case so it's consistent with the rest okay uh and maybe AD should be a keyboard key so now when I search um loots wait then we get ready back too cool okay and now I want to organize it a little bit so I'll that I'll wait the thing here yeah so first here we have the inje we will create a separate script for that later um so here we ingest it in our uh database and here we have the rock ball and open AI let's put it here okay so this is our rack flow here we test it let me just test it to make sure that it's still working um some core exercises cor calistenics exercises let's see how the answer changes dead buug Okay I I think it's different now right because there was something with machine so yeah now what we want to f is uh evil evaluation and we can also say that um for the code for evaluating um the system you can check uh the so it's notebook Rock test notebook right so here we will just give the summary but if the reviewers are interested in the actual um uh evaluation they can go and check the code there um so this is evaluation but we also need to generate some data I think for that maybe I will just create um duplicate for that rename valuation data\",\n",
       "  'timecode_text': 'Dataset preparation',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=6ulnHtJPCWY&t=629s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.2 - Evaluating retrieval our lsom camp and we did evaluation in the vector search model in the future editions uh cohorts I think what we will do is we will have a separate model on evaluation and a separate model on monitoring right now it kind of spr across multiple models I don't like that and also like there should be a larger model on Vector search so if you're watching this in the future right now this uh your the structure you have might look differently from what what I have in the video but anyways so here we have evation and um ground through data actually I remember when I generated the data set I uh removed the index from there I think it could be a good idea to actually have the index back so what I'll do is I'll just create column ID and I think I want to append this column so let me just call it [Music] ID I'm trying to remember how do I pend the coln at colum at the beginning ah I think it's insert right uh insert right insert uh location zero column ID value will be data frame index then right now this is what we have right so right now we will have ID for each of the exercises but also like on the other hand maybe we don't even need to have the ID cuz each exercise name is supposed to be unique so this is our ID so yeah I'll keep the ID just in case uh see data data this IND false I know for some reasons it stopped highlighting this index don't know why so now we have this ID and maybe it should actually be idea I'll remove the I want to make it lower case so it's consistent with the rest okay uh and maybe AD should be a keyboard key so now when I search um loots wait then we get ready back too cool okay and now I want to organize it a little bit so I'll that I'll wait the thing here yeah so first here we have the inje we will create a separate script for that later um so here we ingest it in our uh database and here we have the rock ball and open AI let's put it here okay so this is our rack flow here we test it let me just test it to make sure that it's still working um some core exercises cor calistenics exercises let's see how the answer changes dead buug Okay I I think it's different now right because there was something with machine so yeah now what we want to f is uh evil evaluation and we can also say that um for the code for evaluating um the system you can check uh the so it's notebook Rock test notebook right so here we will just give the summary but if the reviewers are interested in the actual um uh evaluation they can go and check the code there um so this is evaluation but we also need to generate some data I think for that maybe I will just create um duplicate for that rename valuation data Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '687b624833cd2217790f5d6197e4d3c1'},\n",
       " {'vid_id': '6ulnHtJPCWY',\n",
       "  'title': 'LLM Zoomcamp 7.2 - Evaluating retrieval',\n",
       "  'timecode': '16:03',\n",
       "  'text': \"generation so we don't need all these things for that well maybe we need that part so I should not have removed the open AI part cuz we will actually H use that for generating the data so oops what did I do um can I undo this I don't know what I have done I somehow accidentally deleted everything but okay I'll just copy it CU I think this is actually all we need doents yeah cool works okay so now let me open this ground through thing um and yeah so this is the part where we generate IDs we have the IDS PR template let's get the prom template you emulate a user of our fitness assistant application um formulate five questions this user might ask based on a provided exercise the record should contain the answer to the question and the question should be completely not short if possible use asp words as let's just use use as fewer words as possible from the record so actually like like the reason we added this we did this in the videos is because we also wanted to test um Vector search right so right now um I think I'm just going to stick with mean search for Simplicity of course in your uh project you should evaluate different approaches uh I am going to just stick to M search um so h let's keep it the record and then uh here for record let me just copy this thing provide output in personable Json Parable Json without using Cod loocks question for questions okay so what is wrong yeah right um okay let's see promt template format documents zero yeah so let's try to send it to open ey so I will where's so remove this add this and prom LM oh go answers iner [Music] Jason Jason what answers yeah I yeah it should be questions not answers and I see that it did not stick to the format but let's see maybe it will stick it yeah this is this it's not right so maybe we would just explicitly tell it to to do it questions yeah I think this how we screen it okay what is the exercise name and its type of activity what muscle groups are okay this is not what we actually want so uh based on Pro exercise make the questions specific to this exercise most GRS are primary activated when doing a push-up what do how do I properly lower my body during the push-up what equipment do I need to perform push-ups can push-ups Target the upper body effectively okay um maybe let's check question I'm just wondering if they actually use the um yeah it actually did use push up and Clan push up spider push up clan push press yeah so many push-ups yeah I think this is a good answer okay uh what is the correct starting point uh cool so right now we need to perform the same thing for all our um\",\n",
       "  'timecode_text': 'Generating the ground truth dataset:',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=6ulnHtJPCWY&t=963s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.2 - Evaluating retrieval generation so we don't need all these things for that well maybe we need that part so I should not have removed the open AI part cuz we will actually H use that for generating the data so oops what did I do um can I undo this I don't know what I have done I somehow accidentally deleted everything but okay I'll just copy it CU I think this is actually all we need doents yeah cool works okay so now let me open this ground through thing um and yeah so this is the part where we generate IDs we have the IDS PR template let's get the prom template you emulate a user of our fitness assistant application um formulate five questions this user might ask based on a provided exercise the record should contain the answer to the question and the question should be completely not short if possible use asp words as let's just use use as fewer words as possible from the record so actually like like the reason we added this we did this in the videos is because we also wanted to test um Vector search right so right now um I think I'm just going to stick with mean search for Simplicity of course in your uh project you should evaluate different approaches uh I am going to just stick to M search um so h let's keep it the record and then uh here for record let me just copy this thing provide output in personable Json Parable Json without using Cod loocks question for questions okay so what is wrong yeah right um okay let's see promt template format documents zero yeah so let's try to send it to open ey so I will where's so remove this add this and prom LM oh go answers iner [Music] Jason Jason what answers yeah I yeah it should be questions not answers and I see that it did not stick to the format but let's see maybe it will stick it yeah this is this it's not right so maybe we would just explicitly tell it to to do it questions yeah I think this how we screen it okay what is the exercise name and its type of activity what muscle groups are okay this is not what we actually want so uh based on Pro exercise make the questions specific to this exercise most GRS are primary activated when doing a push-up what do how do I properly lower my body during the push-up what equipment do I need to perform push-ups can push-ups Target the upper body effectively okay um maybe let's check question I'm just wondering if they actually use the um yeah it actually did use push up and Clan push up spider push up clan push press yeah so many push-ups yeah I think this is a good answer okay uh what is the correct starting point uh cool so right now we need to perform the same thing for all our um Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '922f93ba14e8f0006e5594507e3b0873'},\n",
       " {'vid_id': '6ulnHtJPCWY',\n",
       "  'title': 'LLM Zoomcamp 7.2 - Evaluating retrieval',\n",
       "  'timecode': '22:34',\n",
       "  'text': \"records and for that um yeah this generate question think promt I'll use the Mini model um and yeah let's use TQ Dam results yeah I'll now copy that uh dock ID I'll put the results here separately so in case I need to continue um we have the ID question generate questions SES questions uh and then here they are in questions right uh it's possible that maybe it doesn't stick to the format so ideally we can try to uh add try catch perhaps but let's not do it right now so let's see what actually happens yeah I forgot to pass it right uh questions roll so now let's add Jason Lots questions so let's do it one more time okay so it's working putting this on pause and coming back when it finishes no while I was having a banana it finished so I did not have to rerun it so like it was always producing a valid uh Parable Json in this format which is great so now I can see results um yeah so good questions actually we can test it oops Okay cool so this is our results and now we need to save them right so I will well it's already pass uh doc index I don't need to do any of that what I can just do is this final results dock ID equation first results results I have this find bad Q yeah I don't even need to do that because doc adid oh no doc dock ad is not there so indeed Q doc ad final results let's see what we have here um maybe it should be like that do ID I like it more and now let's be to P the frame um results cols uh ID question or do call query I don't remember question course document so I'll just call it a question cool so now maybe it should be results so now I'll just save it to CSV again we go to data gr truth retrieval CSV index BS right and let's just quickly take a look at it okay so this is how it looks like cool now we can actually use that for Evola and retrieval and I will take note of the uhing\",\n",
       "  'timecode_text': 'Generating questions for all the records',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=6ulnHtJPCWY&t=1354s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.2 - Evaluating retrieval records and for that um yeah this generate question think promt I'll use the Mini model um and yeah let's use TQ Dam results yeah I'll now copy that uh dock ID I'll put the results here separately so in case I need to continue um we have the ID question generate questions SES questions uh and then here they are in questions right uh it's possible that maybe it doesn't stick to the format so ideally we can try to uh add try catch perhaps but let's not do it right now so let's see what actually happens yeah I forgot to pass it right uh questions roll so now let's add Jason Lots questions so let's do it one more time okay so it's working putting this on pause and coming back when it finishes no while I was having a banana it finished so I did not have to rerun it so like it was always producing a valid uh Parable Json in this format which is great so now I can see results um yeah so good questions actually we can test it oops Okay cool so this is our results and now we need to save them right so I will well it's already pass uh doc index I don't need to do any of that what I can just do is this final results dock ID equation first results results I have this find bad Q yeah I don't even need to do that because doc adid oh no doc dock ad is not there so indeed Q doc ad final results let's see what we have here um maybe it should be like that do ID I like it more and now let's be to P the frame um results cols uh ID question or do call query I don't remember question course document so I'll just call it a question cool so now maybe it should be results so now I'll just save it to CSV again we go to data gr truth retrieval CSV index BS right and let's just quickly take a look at it okay so this is how it looks like cool now we can actually use that for Evola and retrieval and I will take note of the uhing Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '5192cf06986952c44fc26126cb0a35db'},\n",
       " {'vid_id': '6ulnHtJPCWY',\n",
       "  'title': 'LLM Zoomcamp 7.2 - Evaluating retrieval',\n",
       "  'timecode': '27:24',\n",
       "  'text': \"um I'll call data frame questions P 3v right and now what we can do is we can check if this document was retrieved or not and where is it ah this is our ground roof generation uh evaluation text right so this is what we want to do so now I will not use elastic search um uh yeah where is I have this heat rate mrr let me copy it so this one I am just going to close we don't need it anymore TR mrr um and this one mean search search and then we have another thing is8 right so this one is our ground Ruth right this is how we call it here ground truth and data frame questions to dictionary Z right I should use Orient records ID zero question what is the starting position for D pushup okay um key trate mrr mean search query so I'll use this search right so this is what we used for search so we don't have any boost we have 10 results and for course yeah yeah we don't really have the course here just query right Meer search uh doc ID it's just [Music] ID um okay so that should work and now let's run it l ground truth search we we use mean search search Q question tqdm to import tqm rather fast that's good so now we should quickly have the results and here we have the heat rate which is pretty awesome but like also because number of results is 10 it's kind of probably difficult to miss the document we need uh but let's uh use that retrieval the basic approach using mean search without any boting gave the following Matrix he trade this one Mr this one we probably don't need can just use this one stre and mrr I'll use capital letters 82% and now we can try to improve it and for that I actually the idea I have is uh use data to optimize it um so use this data set to try to find the best parameters and I will go to our mlop\",\n",
       "  'timecode_text': 'Evaluating retrieval: baseline',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=6ulnHtJPCWY&t=1644s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.2 - Evaluating retrieval um I'll call data frame questions P 3v right and now what we can do is we can check if this document was retrieved or not and where is it ah this is our ground roof generation uh evaluation text right so this is what we want to do so now I will not use elastic search um uh yeah where is I have this heat rate mrr let me copy it so this one I am just going to close we don't need it anymore TR mrr um and this one mean search search and then we have another thing is8 right so this one is our ground Ruth right this is how we call it here ground truth and data frame questions to dictionary Z right I should use Orient records ID zero question what is the starting position for D pushup okay um key trate mrr mean search query so I'll use this search right so this is what we used for search so we don't have any boost we have 10 results and for course yeah yeah we don't really have the course here just query right Meer search uh doc ID it's just [Music] ID um okay so that should work and now let's run it l ground truth search we we use mean search search Q question tqdm to import tqm rather fast that's good so now we should quickly have the results and here we have the heat rate which is pretty awesome but like also because number of results is 10 it's kind of probably difficult to miss the document we need uh but let's uh use that retrieval the basic approach using mean search without any boting gave the following Matrix he trade this one Mr this one we probably don't need can just use this one stre and mrr I'll use capital letters 82% and now we can try to improve it and for that I actually the idea I have is uh use data to optimize it um so use this data set to try to find the best parameters and I will go to our mlop Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '62d164a37ff28f240ee5e18897d31719'},\n",
       " {'vid_id': '6ulnHtJPCWY',\n",
       "  'title': 'LLM Zoomcamp 7.2 - Evaluating retrieval',\n",
       "  'timecode': '32:18',\n",
       "  'text': \"scores and here we have the experiment tracking um module where we use a special tool for up optimizing parameters and I think it's duration prediction so here we use um special tool uh called uh where is it what was the name hyper opt yeah for tuning XG boost but we can also use hyper opt for tuning Our parameters so I will now install it assistant install def hyper and it's that it's because we only do it here in the jupter notebook so when we actually want to create the interface we want to maybe publish it to the cloud we don't need all these things right so we don't need hyper op we don't need jupter notebooks now um so yeah that's why it's adaptive then while it's installing I'll already start finding the best parameters I'll already start putting some of these things and um yeah of course also one thing to note is uh right now we have these uh questions and this is our entire ground through data set so in practice we actually need to split it into uh multiple Parts like training and testing or testing and evaluation or training and evaluation we call it train validation and test yeah so what I'll do is I'll take first 100 as uh thef validation and the rest as test because I don't want to overfit so I'll just tune my parameters on the first uh 100 and then we'll do evaluation on the rest but actually like since we already did the evaluation of this thing on the entire data set we should also do the evalation here on the entire data set so but um let's see how it goes um so I'll finding the best premes right so did install it so module not\",\n",
       "  'timecode_text': 'Tuning the boosting params',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=6ulnHtJPCWY&t=1938s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.2 - Evaluating retrieval scores and here we have the experiment tracking um module where we use a special tool for up optimizing parameters and I think it's duration prediction so here we use um special tool uh called uh where is it what was the name hyper opt yeah for tuning XG boost but we can also use hyper opt for tuning Our parameters so I will now install it assistant install def hyper and it's that it's because we only do it here in the jupter notebook so when we actually want to create the interface we want to maybe publish it to the cloud we don't need all these things right so we don't need hyper op we don't need jupter notebooks now um so yeah that's why it's adaptive then while it's installing I'll already start finding the best parameters I'll already start putting some of these things and um yeah of course also one thing to note is uh right now we have these uh questions and this is our entire ground through data set so in practice we actually need to split it into uh multiple Parts like training and testing or testing and evaluation or training and evaluation we call it train validation and test yeah so what I'll do is I'll take first 100 as uh thef validation and the rest as test because I don't want to overfit so I'll just tune my parameters on the first uh 100 and then we'll do evaluation on the rest but actually like since we already did the evaluation of this thing on the entire data set we should also do the evalation here on the entire data set so but um let's see how it goes um so I'll finding the best premes right so did install it so module not Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '6f6859ea5c86d9f21af132f50e2cf14a'},\n",
       " {'vid_id': '6ulnHtJPCWY',\n",
       "  'title': 'LLM Zoomcamp 7.2 - Evaluating retrieval',\n",
       "  'timecode': '35:05',\n",
       "  'text': \"found perhaps I need to restart it so let me just reexecute the whole thing so I will not uh execute this one works yeah I don't know what's the problem I'm trying to stall um hyper on Windows and python 312 and this is the error message I get let's see hper hole okay maybe this is what we should do oh so let's see if this one works I'll put it on pause while it's installing it um let's see okay I guess we need to install it separately putting this on pause again okay I it seems like I cannot install it on uh Python 3 12 no pity but whatever like it should not stop us for actually doing this so what I want to do is\",\n",
       "  'timecode_text': 'Attempting to install hyperopt',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=6ulnHtJPCWY&t=2105s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.2 - Evaluating retrieval found perhaps I need to restart it so let me just reexecute the whole thing so I will not uh execute this one works yeah I don't know what's the problem I'm trying to stall um hyper on Windows and python 312 and this is the error message I get let's see hper hole okay maybe this is what we should do oh so let's see if this one works I'll put it on pause while it's installing it um let's see okay I guess we need to install it separately putting this on pause again okay I it seems like I cannot install it on uh Python 3 12 no pity but whatever like it should not stop us for actually doing this so what I want to do is Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '2c6337130be0242b39874997f9824fcb'},\n",
       " {'vid_id': '6ulnHtJPCWY',\n",
       "  'title': 'LLM Zoomcamp 7.2 - Evaluating retrieval',\n",
       "  'timecode': '37:05',\n",
       "  'text': \"Implement a simple alternative to that I want to let me just finish typing my prompt and then I'll show you what I do but we can just like we have a simple alternative to search in main search we can have a simple alternative for T parameters let me finish the prompt and you'll see what I have in mind so what I wrote is I cannot install it so let's Implement a simple alternative so I just want to randomly select some parameters with certain range within a certain range execute the function I have and then store the results and then at the end just return the results that I want um and yeah let's just try to see if this code that it gave us Works was it that f okay cool um so here this is uh objective function number iterations um so let's do that but for 10 iterations so what we are going to do our objective functions will be taking the validation I'll call GT validation well to dict or records and then executing that uh but before that um so I want to where is mean search search I want to use oops stop stop stop stop yeah click the wrong stop button it actually stopped recording the video anyways um so what I wanted to do actually is this one right right that should be reasonably fast and uh here the what we can also pass here to this function I'll put it here is uh boost right if Boost is none then it's empty but this is what we want to optimize right um so um param ranges will be uh what do we actually have here so for each of the document we want to have uh want to have different ranges right so exercise name so we want to now optimize boosting parameters so between zero and three let's say and for for all of them we want to try different boting parameters between zero and three to see which one is actually uh like the most important one at least for this 100 and yeah so how does our function work parameters current parameters here we pick a parameter objective function so this our objective function right so we will just pass parameters like that so we will take this as a dictionary and pass it objective function so let's Implement our objective def Che if params then will be our search function Q then return so I'm implementing this Lambda now uh pars right so this is our objective and now results evalate search function function return let's optimize for uh mrr mrr so this is our objective so this our param ranges this our objective uh here query boost so these are boost params cool and now let's just try to to run it we remove these things simple optimize evaluate um what do we have here par ranges objective function number of iterations so paramet ranges would be this one objective number of iterations 10 let's see what's the best results and if we can actually improve Mr at all I think we're doing it for 10 iterations um what why it's 05 are we actually maximizing it or minimizing it yeah so we are actually maximizing it so instead of do doing the best one we found the worst one see let's do it again so remember with no optimization it was uh 82 what is I think we need to debug a little bit it should be minus infinity yeah right that's why it didn't uh do anything yeah we want to maximize not minimize or we could have returned just minus Mr here and not change the code it's slightly better so we can just go with this right so exercise name is uh the most important type of activity is this important um yeah or we can also experiment uh instead of uh giving it uh like instead of integer we can maybe have like fractions not fractions but like um lot what is and let's do it for 20 diretion I'll put it on pause so you don't need to wait now it finished and now it's actually better right so we managed to improve it a little bit so let's see how well it performs on the entire data set so I'll create mean search improved how does it actually look like me search improved and then this is are the post parameters okay we perhaps don't need to have this long flots cool so we have the Boost now results and let's evaluate the that one more time so let's see what is the result okay this is the result we can see that on the entire data set the value is larger so I will copy this now and so the improved version with better Boos um give rate did not really change well maybe it's like just in one so if I look at this I I don't remember actually what was the number here so maybe I shouldn't do this but Mr improved and if we were optimizing for heat rate maybe uh yeah maybe we would have moved it too um but mrr actually makes sure that the relevant ones are higher in the results which is something we could have achieved also with uh uh query R ranking um but for now yeah let's just stick with that and we can also show the best boosting parameters okay so I think for now it's enough so we evaluated the retrieval uh we generated dat well we actually did a lot in this video so we um wrote started writing our readme file we generated the ground prooof data set we evaluated our retrieval and we also tried different approaches and found the best set of parameters for our retrieval and this video is quite long so let's stop for now and in the next video we will evalate our Rock so see you soon\",\n",
       "  'timecode_text': 'Random search for best boosting parameters',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=6ulnHtJPCWY&t=2225s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.2 - Evaluating retrieval Implement a simple alternative to that I want to let me just finish typing my prompt and then I'll show you what I do but we can just like we have a simple alternative to search in main search we can have a simple alternative for T parameters let me finish the prompt and you'll see what I have in mind so what I wrote is I cannot install it so let's Implement a simple alternative so I just want to randomly select some parameters with certain range within a certain range execute the function I have and then store the results and then at the end just return the results that I want um and yeah let's just try to see if this code that it gave us Works was it that f okay cool um so here this is uh objective function number iterations um so let's do that but for 10 iterations so what we are going to do our objective functions will be taking the validation I'll call GT validation well to dict or records and then executing that uh but before that um so I want to where is mean search search I want to use oops stop stop stop stop yeah click the wrong stop button it actually stopped recording the video anyways um so what I wanted to do actually is this one right right that should be reasonably fast and uh here the what we can also pass here to this function I'll put it here is uh boost right if Boost is none then it's empty but this is what we want to optimize right um so um param ranges will be uh what do we actually have here so for each of the document we want to have uh want to have different ranges right so exercise name so we want to now optimize boosting parameters so between zero and three let's say and for for all of them we want to try different boting parameters between zero and three to see which one is actually uh like the most important one at least for this 100 and yeah so how does our function work parameters current parameters here we pick a parameter objective function so this our objective function right so we will just pass parameters like that so we will take this as a dictionary and pass it objective function so let's Implement our objective def Che if params then will be our search function Q then return so I'm implementing this Lambda now uh pars right so this is our objective and now results evalate search function function return let's optimize for uh mrr mrr so this is our objective so this our param ranges this our objective uh here query boost so these are boost params cool and now let's just try to to run it we remove these things simple optimize evaluate um what do we have here par ranges objective function number of iterations so paramet ranges would be this one objective number of iterations 10 let's see what's the best results and if we can actually improve Mr at all I think we're doing it for 10 iterations um what why it's 05 are we actually maximizing it or minimizing it yeah so we are actually maximizing it so instead of do doing the best one we found the worst one see let's do it again so remember with no optimization it was uh 82 what is I think we need to debug a little bit it should be minus infinity yeah right that's why it didn't uh do anything yeah we want to maximize not minimize or we could have returned just minus Mr here and not change the code it's slightly better so we can just go with this right so exercise name is uh the most important type of activity is this important um yeah or we can also experiment uh instead of uh giving it uh like instead of integer we can maybe have like fractions not fractions but like um lot what is and let's do it for 20 diretion I'll put it on pause so you don't need to wait now it finished and now it's actually better right so we managed to improve it a little bit so let's see how well it performs on the entire data set so I'll create mean search improved how does it actually look like me search improved and then this is are the post parameters okay we perhaps don't need to have this long flots cool so we have the Boost now results and let's evaluate the that one more time so let's see what is the result okay this is the result we can see that on the entire data set the value is larger so I will copy this now and so the improved version with better Boos um give rate did not really change well maybe it's like just in one so if I look at this I I don't remember actually what was the number here so maybe I shouldn't do this but Mr improved and if we were optimizing for heat rate maybe uh yeah maybe we would have moved it too um but mrr actually makes sure that the relevant ones are higher in the results which is something we could have achieved also with uh uh query R ranking um but for now yeah let's just stick with that and we can also show the best boosting parameters okay so I think for now it's enough so we evaluated the retrieval uh we generated dat well we actually did a lot in this video so we um wrote started writing our readme file we generated the ground prooof data set we evaluated our retrieval and we also tried different approaches and found the best set of parameters for our retrieval and this video is quite long so let's stop for now and in the next video we will evalate our Rock so see you soon Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '1c897ea59bf8798c53d066027ccff678'},\n",
       " {'vid_id': 'lxpW2mR7dGk',\n",
       "  'title': 'LLM Zoomcamp 7.3 - RAG evaluation',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hey welcome back in this series of videos we're doing an nend to end project similar to what we did in the course but a different one so which is about building a fitness assistant so so far we have covered uh these three first three criterias so now I want to talk I want to cover this reculation which will be like in the previous cases mostly copy and paste from the course materials um so let me now go to monitoring and I want to find this offline regulation notebook and copy paste\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=lxpW2mR7dGk&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.3 - RAG evaluation hey welcome back in this series of videos we're doing an nend to end project similar to what we did in the course but a different one so which is about building a fitness assistant so so far we have covered uh these three first three criterias so now I want to talk I want to cover this reculation which will be like in the previous cases mostly copy and paste from the course materials um so let me now go to monitoring and I want to find this offline regulation notebook and copy paste Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '4c72dd0e3f453c000793e28947780915'},\n",
       " {'vid_id': 'lxpW2mR7dGk',\n",
       "  'title': 'LLM Zoomcamp 7.3 - RAG evaluation',\n",
       "  'timecode': '00:51',\n",
       "  'text': \"things from there so I already have the ground roof data set let me actually call It Rock evaluation and what I want to have here uh yeah I'll just continue using the same uh approach as previously I just want to know how well it works right so this is theflow okay sign similarity metric so I just want to I don't know if I should use cosign similarity um I'll just use llm as a judge so for S similarity uh it's a good metric I should implement it too uh but now I need to install an extra Library I don't want to do it right now so what I'll do instead is um and actually like I'm not really sure if I can do cosign similarity here cuz I don't have a reference uh yeah I don't have a reference answer here so probably the only thing we can do right now is as a judge of course when we start running the um when we start running the application and users start giving up votes and down VES we can use up votes as a reference uh answers right but we don't have this data yet so the only thing we can do is actually using this llm as a\",\n",
       "  'timecode_text': 'RAG evaluation code',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=lxpW2mR7dGk&t=51s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.3 - RAG evaluation things from there so I already have the ground roof data set let me actually call It Rock evaluation and what I want to have here uh yeah I'll just continue using the same uh approach as previously I just want to know how well it works right so this is theflow okay sign similarity metric so I just want to I don't know if I should use cosign similarity um I'll just use llm as a judge so for S similarity uh it's a good metric I should implement it too uh but now I need to install an extra Library I don't want to do it right now so what I'll do instead is um and actually like I'm not really sure if I can do cosign similarity here cuz I don't have a reference uh yeah I don't have a reference answer here so probably the only thing we can do right now is as a judge of course when we start running the um when we start running the application and users start giving up votes and down VES we can use up votes as a reference uh answers right but we don't have this data yet so the only thing we can do is actually using this llm as a Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'a3b0ef9864f35e3242a73ec9a89b5c9a'},\n",
       " {'vid_id': 'lxpW2mR7dGk',\n",
       "  'title': 'LLM Zoomcamp 7.3 - RAG evaluation',\n",
       "  'timecode': '02:33',\n",
       "  'text': \"judge and I will now so here we don't have the original answer right we only have the question and the generated answer so we need to take this thing uh you're an expert relator for a rock system your task is to analyze the relevance of the generator answer to the given question based on the relevance of the generate answer you will classify it as yeah okay looks uh good so now uh I won't do it for a sample I can just do it for the entire thing we only have um let's see 1,000 like with GPT 4 or mini it's not only fast but also pretty cheap so I can just do it for the entire thing um but also taking a sample is fine right so let's see how a prompt might look like for um so the record would be ground truth [Music] zero and yeah answer LM of course yeah we need to also give an llm answer question we have the question we don't have to answer right um question will be record question and then the [Music] answer answer LM will be Rock so what I'll do is I'll here the question then we send it to Rock Let's see what the answer the starting position for doing pushup is to okay so question is question answer LM is answer LM so let's see what the PR is so this is the prompt and again we use LM for this prompt and then it says it's a relevant answer okay so now we will use judge to actually evaluate all our answers and we should have here this thing and this is actually not complete let me at Ground Ruth and I will just do this thing and we will need to actually I want to append U so I want to append U the uh record from the ground roof um so it will be record ID right then um I want to append let me just append the entire record then append the answer from the LM and append the evaluation this should be prompt to so let me see if I forget anything uh I only forgot parsing and let me do it like that so record ID if ID in evaluations then we don't redo it and I want to do it just in case if something is wrong right so for example the Jason is not possible then I just hit uh shift enter one more time and then I redo the thing and then aeration ID is um let me make it a little bit more interesting so I'll just have ID here which is ID question which is question then answer LM LM and then evaluation which will be evoluation okay so I think it looks correct so we iterate over the ground proof data set um yeah let's let's just see yeah so here I did not copy this thing properly so let me just do that okay let's run this okay it's working um yeah let me quickly stop it because I want to make sure that it's actually good so let's see what we have here what is the starting position yeah I actually wanted to also do Jon a what s right so we want to make sure that they are par okay okay let me import this okay let me see if it works let's look atation Elation relevance relevant explanation okay looks okay so let me just regenerate or I don't need to regenerate it for records for which it's already calculated so we can just keep these records okay now yeah now I'm am going to just put it on post and come back when it finishes and while it's still relating I thought maybe I could already start writing some things here uh where do we WR about\",\n",
       "  'timecode_text': 'Implementing LLM as a Judge',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=lxpW2mR7dGk&t=153s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.3 - RAG evaluation judge and I will now so here we don't have the original answer right we only have the question and the generated answer so we need to take this thing uh you're an expert relator for a rock system your task is to analyze the relevance of the generator answer to the given question based on the relevance of the generate answer you will classify it as yeah okay looks uh good so now uh I won't do it for a sample I can just do it for the entire thing we only have um let's see 1,000 like with GPT 4 or mini it's not only fast but also pretty cheap so I can just do it for the entire thing um but also taking a sample is fine right so let's see how a prompt might look like for um so the record would be ground truth [Music] zero and yeah answer LM of course yeah we need to also give an llm answer question we have the question we don't have to answer right um question will be record question and then the [Music] answer answer LM will be Rock so what I'll do is I'll here the question then we send it to Rock Let's see what the answer the starting position for doing pushup is to okay so question is question answer LM is answer LM so let's see what the PR is so this is the prompt and again we use LM for this prompt and then it says it's a relevant answer okay so now we will use judge to actually evaluate all our answers and we should have here this thing and this is actually not complete let me at Ground Ruth and I will just do this thing and we will need to actually I want to append U so I want to append U the uh record from the ground roof um so it will be record ID right then um I want to append let me just append the entire record then append the answer from the LM and append the evaluation this should be prompt to so let me see if I forget anything uh I only forgot parsing and let me do it like that so record ID if ID in evaluations then we don't redo it and I want to do it just in case if something is wrong right so for example the Jason is not possible then I just hit uh shift enter one more time and then I redo the thing and then aeration ID is um let me make it a little bit more interesting so I'll just have ID here which is ID question which is question then answer LM LM and then evaluation which will be evoluation okay so I think it looks correct so we iterate over the ground proof data set um yeah let's let's just see yeah so here I did not copy this thing properly so let me just do that okay let's run this okay it's working um yeah let me quickly stop it because I want to make sure that it's actually good so let's see what we have here what is the starting position yeah I actually wanted to also do Jon a what s right so we want to make sure that they are par okay okay let me import this okay let me see if it works let's look atation Elation relevance relevant explanation okay looks okay so let me just regenerate or I don't need to regenerate it for records for which it's already calculated so we can just keep these records okay now yeah now I'm am going to just put it on post and come back when it finishes and while it's still relating I thought maybe I could already start writing some things here uh where do we WR about Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '80023759dc7149ff40dfdd21e4fd7cec'},\n",
       " {'vid_id': 'lxpW2mR7dGk',\n",
       "  'title': 'LLM Zoomcamp 7.3 - RAG evaluation',\n",
       "  'timecode': '09:53',\n",
       "  'text': \"operation rlow we used the LM is a judge metric uh to evaluate the quality of all rock among um how many records uh we had uh X relevant why partly relevant and but maybe I I'll just write it yeah we'll see now just a little bit more and I I don't know if I want to like ideally we should evalate multiple approaches to get the um get two points um so for example it could be using a different model or using a different prompt or both right uh we can of course do that like just replace it GPT 40 mini with GPT 3.5 to score two points instead of one but I don't really want to do this to be\",\n",
       "  'timecode_text': 'Updating README',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=lxpW2mR7dGk&t=593s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.3 - RAG evaluation operation rlow we used the LM is a judge metric uh to evaluate the quality of all rock among um how many records uh we had uh X relevant why partly relevant and but maybe I I'll just write it yeah we'll see now just a little bit more and I I don't know if I want to like ideally we should evalate multiple approaches to get the um get two points um so for example it could be using a different model or using a different prompt or both right uh we can of course do that like just replace it GPT 40 mini with GPT 3.5 to score two points instead of one but I don't really want to do this to be Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'e9fabd9afbf8d2cfb94deeda2422434a'},\n",
       " {'vid_id': 'lxpW2mR7dGk',\n",
       "  'title': 'LLM Zoomcamp 7.3 - RAG evaluation',\n",
       "  'timecode': '11:40',\n",
       "  'text': \"honest uh so let me just uh let me just focus on finishing this one and I think if we just end up with one point instead of two it's not the end of the world um so uh for some reasons it's like let me do it this way um um because it is a yeah it's a dictionary that's why it was kind of cner but I just simply transposed it now we can delete um ID or should we delete ID yeah I just realized that uh making ID as a key was a bad idea uh cuz it overwrote uh like for one record we had what for one document we had five answers so sadly I just wasted\",\n",
       "  'timecode_text': 'Computing the results',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=lxpW2mR7dGk&t=700s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.3 - RAG evaluation honest uh so let me just uh let me just focus on finishing this one and I think if we just end up with one point instead of two it's not the end of the world um so uh for some reasons it's like let me do it this way um um because it is a yeah it's a dictionary that's why it was kind of cner but I just simply transposed it now we can delete um ID or should we delete ID yeah I just realized that uh making ID as a key was a bad idea uh cuz it overwrote uh like for one record we had what for one document we had five answers so sadly I just wasted Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '4ae1dc7cff504440802682f503fb0724'},\n",
       " {'vid_id': 'lxpW2mR7dGk',\n",
       "  'title': 'LLM Zoomcamp 7.3 - RAG evaluation',\n",
       "  'timecode': '12:50',\n",
       "  'text': \"uh like one fth of the evoluation so let me do that I'll take a sample like we did previously what was that data frame not a while frame question yeah that one I'll take a sample 200 and then maybe uh let me put random State one so it's reproducible if some and it will be to dict Orient records this our sample and I'll remove this thing when I was trying to be smart and instead like the CH pt40 seems to be doing fine with um like um creating Jason that is Parable so I will want to do this stuff and instead I'll just use append and then we will append um alant tole so pent record I'll pend the answer from the llm and I'll append the evaluation so we'll just parse it and I'll add here and because it's only 200 it should be faster and then while it will be doing that\",\n",
       "  'timecode_text': 'Fixing errors, computing the metric on a sample',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=lxpW2mR7dGk&t=770s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.3 - RAG evaluation uh like one fth of the evoluation so let me do that I'll take a sample like we did previously what was that data frame not a while frame question yeah that one I'll take a sample 200 and then maybe uh let me put random State one so it's reproducible if some and it will be to dict Orient records this our sample and I'll remove this thing when I was trying to be smart and instead like the CH pt40 seems to be doing fine with um like um creating Jason that is Parable so I will want to do this stuff and instead I'll just use append and then we will append um alant tole so pent record I'll pend the answer from the llm and I'll append the evaluation so we'll just parse it and I'll add here and because it's only 200 it should be faster and then while it will be doing that Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '2f6a19ca1246f87f0cc6abd558f8fe8e'},\n",
       " {'vid_id': 'lxpW2mR7dGk',\n",
       "  'title': 'LLM Zoomcamp 7.3 - RAG evaluation',\n",
       "  'timecode': '15:00',\n",
       "  'text': \"no stop should be something okay good and while it's doing that so let us uh change it slightly where is our LM function yeah so let's add let's do the same thing as we did previously model will be model um so by default it's dpt4 O mini but uh what do we actually have apart from that um wait this is not a playground yeah so what do we have hbt oh mini um bt4 turbo jb4 latest um yeah maybe let's see how worse uh gbt 40 Mini compared to gbt 40 right so this is something we can do too and I can simply now do relation GP T4 o and then open this here for rock I think we also need to add model bt4 o and here yeah for evaluation let's use GPT 40 mini uh okay okay which means I need to add this to our Rock flow too just add it like that and [Music] here uh L model equals model okay and while it's waiting I will all ready yeah we can do it later right now when it finishes I want to just take some time so then this is meaning with GPT 40 it will take even more time I guess okay let me put it on pause and then I'll come back when it finishes uh so it finished let's now turn this\",\n",
       "  'timecode_text': 'Evaluating gpt-4o (code)',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=lxpW2mR7dGk&t=900s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.3 - RAG evaluation no stop should be something okay good and while it's doing that so let us uh change it slightly where is our LM function yeah so let's add let's do the same thing as we did previously model will be model um so by default it's dpt4 O mini but uh what do we actually have apart from that um wait this is not a playground yeah so what do we have hbt oh mini um bt4 turbo jb4 latest um yeah maybe let's see how worse uh gbt 40 Mini compared to gbt 40 right so this is something we can do too and I can simply now do relation GP T4 o and then open this here for rock I think we also need to add model bt4 o and here yeah for evaluation let's use GPT 40 mini uh okay okay which means I need to add this to our Rock flow too just add it like that and [Music] here uh L model equals model okay and while it's waiting I will all ready yeah we can do it later right now when it finishes I want to just take some time so then this is meaning with GPT 40 it will take even more time I guess okay let me put it on pause and then I'll come back when it finishes uh so it finished let's now turn this Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '9c6afa055155c97100708c6cca2a832b'},\n",
       " {'vid_id': 'lxpW2mR7dGk',\n",
       "  'title': 'LLM Zoomcamp 7.3 - RAG evaluation',\n",
       "  'timecode': '17:56',\n",
       "  'text': \"into uh something we can evaluate right um so for us actually okay we need to specify columns now because this was it Apple um was it record then answer then evaluation and now we can expand it so we can just um use ID or record a by how would I write it no lamb. d d ID okay for all so now we have the ID the same way we can um get question question would be question and now uh relevance is evaluation relevance and finally again evolation explanation and we have this and now I can just delete uh record and delete uh evolation and we have data let's do relevance count vales or value counts okay so we have this many relevant partly relevant and nonrelevant three three which is not that many right I'm just curious to see what are the non-relevant ones I'm sorry about the provide context uh I don't know why it's not relevant but yeah okay non relevant non correct I guess anyways um let's not think too much about that so it's not that many um we can also normalize it to see that it's 83% 83% relevant and 15% partly relevant on three which is like 1.5% right non relevant okay uh we also test that uh GPT 40 record records we had yeah GPT or GPT for o mini we also test it and now we can run the same thing but for GPT um for all I will not do it right now so I'll I might do it and then put some uh values there\",\n",
       "  'timecode_text': 'Results for gpt-4o-mini',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=lxpW2mR7dGk&t=1076s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.3 - RAG evaluation into uh something we can evaluate right um so for us actually okay we need to specify columns now because this was it Apple um was it record then answer then evaluation and now we can expand it so we can just um use ID or record a by how would I write it no lamb. d d ID okay for all so now we have the ID the same way we can um get question question would be question and now uh relevance is evaluation relevance and finally again evolation explanation and we have this and now I can just delete uh record and delete uh evolation and we have data let's do relevance count vales or value counts okay so we have this many relevant partly relevant and nonrelevant three three which is not that many right I'm just curious to see what are the non-relevant ones I'm sorry about the provide context uh I don't know why it's not relevant but yeah okay non relevant non correct I guess anyways um let's not think too much about that so it's not that many um we can also normalize it to see that it's 83% 83% relevant and 15% partly relevant on three which is like 1.5% right non relevant okay uh we also test that uh GPT 40 record records we had yeah GPT or GPT for o mini we also test it and now we can run the same thing but for GPT um for all I will not do it right now so I'll I might do it and then put some uh values there Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'e2bba73318f113b86c7f61abce2edee7'},\n",
       " {'vid_id': 'lxpW2mR7dGk',\n",
       "  'title': 'LLM Zoomcamp 7.3 - RAG evaluation',\n",
       "  'timecode': '22:52',\n",
       "  'text': \"but I think the idea is clear here right what we do and then we report this and at the end I will go with GPT 40 mini regardless of the results I see here I'm sure this might be even better but yeah like I don't want to pay 20 times more for this okay so that's all for this one and next we should start covering the interface right so this would this should be relatively straightforward so yeah yeah let's see if it will be and see you soon\",\n",
       "  'timecode_text': 'Evaluating gpt-4o',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=lxpW2mR7dGk&t=1372s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.3 - RAG evaluation but I think the idea is clear here right what we do and then we report this and at the end I will go with GPT 40 mini regardless of the results I see here I'm sure this might be even better but yeah like I don't want to pay 20 times more for this okay so that's all for this one and next we should start covering the interface right so this would this should be relatively straightforward so yeah yeah let's see if it will be and see you soon Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'eed796a08221a72ef6129bf2e625acff'},\n",
       " {'vid_id': 'vMHve2EyA5M',\n",
       "  'title': 'LLM Zoomcamp 7.4 - Interface and ingestion pipeline',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hi everyone welcome back in this series of videos we are implementing an NN project uh this project is a fitness assistant uh we what we have done was already well quite a lot so we generated the data set we implemented track flow we wrote the problem problem description and dation retal Elation and track evalation now I want to tackle these two parts interface and ingestion pipeline because we don't use a real database our inje pipeline will be simplified so I'll go to an automated ingestion with a python script which will happen at the same time uh when the interface starts right so if you use real database then your inje pipeline and interface would be separate things uh and um will be a separate script that does the ingestion and then a separate uh application that does the interface could be stream lead or Jango or or fast API so I want to tackle here both of them and at the beginning I thought we already covered stream lead let's use flask streamlet or any UI is uh usually better at least in the sense that it's more visual uh but since I decided to go with flask we can do that too it will be just more common line less visual um but then nobody um nothing prevents us from building an UI on top of the API so let's do that what I'll do now is I'll go to uh this uh notebook that we wrote already um that we created already I still don't know why I don't have syntax highlight for some reasons it doesn't work for me but anyways so this small part is the inje\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=vMHve2EyA5M&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.4 - Interface and ingestion pipeline hi everyone welcome back in this series of videos we are implementing an NN project uh this project is a fitness assistant uh we what we have done was already well quite a lot so we generated the data set we implemented track flow we wrote the problem problem description and dation retal Elation and track evalation now I want to tackle these two parts interface and ingestion pipeline because we don't use a real database our inje pipeline will be simplified so I'll go to an automated ingestion with a python script which will happen at the same time uh when the interface starts right so if you use real database then your inje pipeline and interface would be separate things uh and um will be a separate script that does the ingestion and then a separate uh application that does the interface could be stream lead or Jango or or fast API so I want to tackle here both of them and at the beginning I thought we already covered stream lead let's use flask streamlet or any UI is uh usually better at least in the sense that it's more visual uh but since I decided to go with flask we can do that too it will be just more common line less visual um but then nobody um nothing prevents us from building an UI on top of the API so let's do that what I'll do now is I'll go to uh this uh notebook that we wrote already um that we created already I still don't know why I don't have syntax highlight for some reasons it doesn't work for me but anyways so this small part is the inje Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'fe17a49f29732cf33648bc2344a7bcad'},\n",
       " {'vid_id': 'vMHve2EyA5M',\n",
       "  'title': 'LLM Zoomcamp 7.4 - Interface and ingestion pipeline',\n",
       "  'timecode': '02:02',\n",
       "  'text': \"and uh this part is our Rock FL right so now I need to expert it save it as a python file and then create a bunch of scripts so I'll I'll actually use common line right now for that so um I'll go to notebook and do um so first of all I want to create um I'll create uh a package for that I'll call it Fitness assistant so this will contain all the code right now I will convert and front I'll convert The Notebook Jupiter NB convert to script I think uh Rock test I hope I get it right I don't remember if I should write to script or to python um yeah it works so now I will just take this script and move here and also we need me search I'll put here too so and now\",\n",
       "  'timecode_text': 'Saving the notebook as a script',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=vMHve2EyA5M&t=122s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.4 - Interface and ingestion pipeline and uh this part is our Rock FL right so now I need to expert it save it as a python file and then create a bunch of scripts so I'll I'll actually use common line right now for that so um I'll go to notebook and do um so first of all I want to create um I'll create uh a package for that I'll call it Fitness assistant so this will contain all the code right now I will convert and front I'll convert The Notebook Jupiter NB convert to script I think uh Rock test I hope I get it right I don't remember if I should write to script or to python um yeah it works so now I will just take this script and move here and also we need me search I'll put here too so and now Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'bfe8005afbc69cba05c9dde63ab8d136'},\n",
       " {'vid_id': 'vMHve2EyA5M',\n",
       "  'title': 'LLM Zoomcamp 7.4 - Interface and ingestion pipeline',\n",
       "  'timecode': '03:32',\n",
       "  'text': \"um I'll create two files uh first one in just pi and second one is um appp now so these names are arbitrary you don't have to use the same names um so this will be our inje script so copy it and just and notice that we actually need uh need the data set so when we start the script we need to make sure that the data set is always with us because this is um how our injection works so when we start the application it loads the entire script in memory so it means that our data set needs to be with us all the time um so I'm cleaning a little bit and I don't need that CL and then I'll just call it he okay um lot data uh and like this stuff will go here or loot index and there will be return index um let's use path data path which by default will be this one okay so this should do uh let me format it a little bit okay cool so this will be our injection script of of course like if we use a proper database it will be more complex and um I think what we did uh in the course llm um I think here in app so we have our generate data uh no not that one um TB so I think that was kind of our ingestion script in the sense that um wait where did we actually create um rep yeah so here um we have this uh thing that puts the data in elastic SE sets up the database and all that stuff so that was our sort of inje script which we run prior to running our application and like when we do it in Mage this is also what we run prior to running application we create the index we ingest all the data um so we can use Mage I'll just use or prefect or like whatever uh orchestrator you want here I'll just rely on a simple python script um and I just wanted to add a note that right now this will give two points um uh or a simple python script but in the future editions we actually might revisit um this and um give more points when you use a special tool okay so so far so good let's um continue so this is the inj script and then I have the app uh so this stuff will go to the app uh let me actually call It Rock so this will be the Rock part uh so from U import justest and index and just load index okay so it will be rather simple I don't want to complicate it we can of course use um like make it object oriented create some classes I don't want to do it here right now um I think it's okay like that okay so this is our rack um we create the index and then I'll create app which will be a flask application um right now I am going to use um an llm again to write FL code because it's faster um let me actually create a new chart I have the hen um let me I'll put it on pause write a prompt and then show you what I wrote okay so this is the prompt eral this is the prompt eral I have the r function\",\n",
       "  'timecode_text': 'Reorganizing the code across multiple Python files',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=vMHve2EyA5M&t=212s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.4 - Interface and ingestion pipeline um I'll create two files uh first one in just pi and second one is um appp now so these names are arbitrary you don't have to use the same names um so this will be our inje script so copy it and just and notice that we actually need uh need the data set so when we start the script we need to make sure that the data set is always with us because this is um how our injection works so when we start the application it loads the entire script in memory so it means that our data set needs to be with us all the time um so I'm cleaning a little bit and I don't need that CL and then I'll just call it he okay um lot data uh and like this stuff will go here or loot index and there will be return index um let's use path data path which by default will be this one okay so this should do uh let me format it a little bit okay cool so this will be our injection script of of course like if we use a proper database it will be more complex and um I think what we did uh in the course llm um I think here in app so we have our generate data uh no not that one um TB so I think that was kind of our ingestion script in the sense that um wait where did we actually create um rep yeah so here um we have this uh thing that puts the data in elastic SE sets up the database and all that stuff so that was our sort of inje script which we run prior to running our application and like when we do it in Mage this is also what we run prior to running application we create the index we ingest all the data um so we can use Mage I'll just use or prefect or like whatever uh orchestrator you want here I'll just rely on a simple python script um and I just wanted to add a note that right now this will give two points um uh or a simple python script but in the future editions we actually might revisit um this and um give more points when you use a special tool okay so so far so good let's um continue so this is the inj script and then I have the app uh so this stuff will go to the app uh let me actually call It Rock so this will be the Rock part uh so from U import justest and index and just load index okay so it will be rather simple I don't want to complicate it we can of course use um like make it object oriented create some classes I don't want to do it here right now um I think it's okay like that okay so this is our rack um we create the index and then I'll create app which will be a flask application um right now I am going to use um an llm again to write FL code because it's faster um let me actually create a new chart I have the hen um let me I'll put it on pause write a prompt and then show you what I wrote okay so this is the prompt eral this is the prompt eral I have the r function Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '19b61a7ea791cc989c9110a1ef8f0256'},\n",
       " {'vid_id': 'vMHve2EyA5M',\n",
       "  'title': 'LLM Zoomcamp 7.4 - Interface and ingestion pipeline',\n",
       "  'timecode': '09:12',\n",
       "  'text': \"let me just execute it and I'll continue reading I have the rack function in R.P script which I want to invoke from app.py app.py is a flask application which has two post end points so the first one will just invoke the ra rack function the second one I want to already think about monitoring in monitoring we want to collect user feedback so I already want to create um I want to create an endpoint that accepts this user feedback uh right now we will not do anything um with this just say okay everything is good uh but later we will store we will keep this in a database um okay so this is our application let me where is the copy button so this is the name from rocker Rock I don't like these comments cuz they're kind of useless well at least I I don't see much like I don't see them as helpful um okay something like this conversation ID answer I'll quote I'll quote answer and then I'll callot result the this thing yeah and then I'll I don't think I actually need to jsonify it um I don't think I need to jsonify it let's see later if I need uh it should by default disi it but okay let's make it because I think like when we do jsonify it also adds um content uh header saying that this is Json position this will be the result\",\n",
       "  'timecode_text': 'Creating the Flask application',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=vMHve2EyA5M&t=552s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.4 - Interface and ingestion pipeline let me just execute it and I'll continue reading I have the rack function in R.P script which I want to invoke from app.py app.py is a flask application which has two post end points so the first one will just invoke the ra rack function the second one I want to already think about monitoring in monitoring we want to collect user feedback so I already want to create um I want to create an endpoint that accepts this user feedback uh right now we will not do anything um with this just say okay everything is good uh but later we will store we will keep this in a database um okay so this is our application let me where is the copy button so this is the name from rocker Rock I don't like these comments cuz they're kind of useless well at least I I don't see much like I don't see them as helpful um okay something like this conversation ID answer I'll quote I'll quote answer and then I'll callot result the this thing yeah and then I'll I don't think I actually need to jsonify it um I don't think I need to jsonify it let's see later if I need uh it should by default disi it but okay let's make it because I think like when we do jsonify it also adds um content uh header saying that this is Json position this will be the result Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'ba17fece705d52a56e9fd57ecb17e0ef'},\n",
       " {'vid_id': 'vMHve2EyA5M',\n",
       "  'title': 'LLM Zoomcamp 7.4 - Interface and ingestion pipeline',\n",
       "  'timecode': '12:05',\n",
       "  'text': \"okay so now let us run it uh how I actually run a flask application maybe it's it wrote how to run it people install flask I already have ah we just run python app uh I go to Fitness assistant do python app and of course I need to prefix it with beep and front and what we can already do is um\",\n",
       "  'timecode_text': 'Running the application',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=vMHve2EyA5M&t=725s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.4 - Interface and ingestion pipeline okay so now let us run it uh how I actually run a flask application maybe it's it wrote how to run it people install flask I already have ah we just run python app uh I go to Fitness assistant do python app and of course I need to prefix it with beep and front and what we can already do is um Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '103e4a26ebe3dfcae3cfdae3f930d564'},\n",
       " {'vid_id': 'vMHve2EyA5M',\n",
       "  'title': 'LLM Zoomcamp 7.4 - Interface and ingestion pipeline',\n",
       "  'timecode': '12:43',\n",
       "  'text': \"put this to the r me file uh to running running the application and since we actually now I realized that since we used flask and flask wasn't covered in the course we should write uh like uh a few lines about uh interface like what exactly is flask and we use flk for um serving the application application as API I'll probably expand later on this but I think we should add more detail uh details here about flask um um we will when we show how to actually use it no I don't um right now we want to test it so it's running um let's uh create a like right now I want to test it with um curl test it with curl uh sent a query what did we do here this one for example so let me copy it here we install installing dependencies is here here running the [Music] application and the rest I'll just call it uh um uh testing it so we have um URL which is this one um I'll I'll just use this P then oops your question then um I want to put it at the end cont type and then we have the data right data so this is what we sent can make it a little bit Yeah format it so here we have data and let's see if it works create a um so I want to create a new terminal where I'll s the request from so now I copy it okay I think we need to use oops sorry we need to use Coes here okay so it worked quite fast um yes lot consider strength training activity uh maybe to make it even better what we can do is also include the question so then we can see what the question is uh maybe let me make it a bit easier um question so our question was in this one so then I just put question here I think I need to do something like this let's see cool so we have the question we have the answer uh and then we have our conversation ID which we can now send to um sending feedback uh ID this is the ID okay now let's send the feedback with this ID and make the script similar to this one edit a little bit Yeah it made it even more complex but uh I think we can just use this one feedback data um and then take this one and the URL is is feedback right and data is feedback data cool so for now it's okay um we have our inje um script we have our um interface I want to spend a little bit uh of time to describe um flask background maybe so here um I will say that we use flask maybe Technologies Technologies mean search uh for searching and uh let me find search me search for text uh text search uh then what do we use uh open AI uh s and llm then flask is the uh API interface interace see yeah background uh or more onk and I want to ask CLA to write U um to write a little bit about flask and what it is and how to use it so I'll put it on pause WR a prompt and then we'll talk about this so this is what I wrote um I use flask as a part of course project but since we didn't cover flask in the course I need to provide sufficient background for people to understand what it is write a few sentences about it in markdown format um so let's copy it and put it here flk um all right we use flask uh for like again I'll put it on pause edit it and then read what I have at the end and so this is what I ended up writing here we provide background on some tech note used in the course and links for third reading and then I just include a few sentences like we use Plus for creating the API for our application um and then for example uh we can send in our case we can send the question to http Local Host 500 uh was it questions or I don't remember question I think this should be sufficient right so like for example if you use a database that is not covered in the course you can just write hey like for example if you use quadrant quadrant is a database um for like vector database and then you just briefly describe what it is and then give a link for more information by the way let's check that this link is actually valid cool it is yeah okay so right now we tested it with curl and by the way you of course need to install uh curl uh if you don't have it alternative could be uh doing it with requests and then I can quickly show it to you so I'll do assistant P install def requests and by the way I need to also specify that when you install it you also need to install Dev dependencies um yeah um to also show that it works we can include um examples the answer could the answer will look like this Jon or uh you will see something like the four looping in the response and here after sending it you receive uh the acknowledgement okay um I'm just thinking if this should be like that uh I think it's kind of clear um what it is uh cuz yeah maybe let's uh make it explicit I'll think how to organize this later maybe you'll when you check the final version you'll see how exactly I put it here cuz like it feels like kind of lonely just one sentence for the entire section um but we want the reviewers to make it we want to make it easier for reviewers to actually review that in dets flask for interface so we need to be explicit about that so I'll think how to organize it better okay and for ingestion also um we need to write that the ingestion script is in um uh Fitness assistant inest and it's run as on the start up the app in uh in app right oh it's actually in rck uh maybe for interface we can say refer to um running the application for detail okay section for more detail okay um yeah maybe later if we let's say decide to include stream lead we can also put a screenshot here um for now I think that should be enough um I'll think of what kind of screenshots we can add later um yeah and yeah I already installed request right so this is one other thing I wanted to show you so I'll create a\",\n",
       "  'timecode_text': 'Updating README and testing the application with curl',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=vMHve2EyA5M&t=763s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.4 - Interface and ingestion pipeline put this to the r me file uh to running running the application and since we actually now I realized that since we used flask and flask wasn't covered in the course we should write uh like uh a few lines about uh interface like what exactly is flask and we use flk for um serving the application application as API I'll probably expand later on this but I think we should add more detail uh details here about flask um um we will when we show how to actually use it no I don't um right now we want to test it so it's running um let's uh create a like right now I want to test it with um curl test it with curl uh sent a query what did we do here this one for example so let me copy it here we install installing dependencies is here here running the [Music] application and the rest I'll just call it uh um uh testing it so we have um URL which is this one um I'll I'll just use this P then oops your question then um I want to put it at the end cont type and then we have the data right data so this is what we sent can make it a little bit Yeah format it so here we have data and let's see if it works create a um so I want to create a new terminal where I'll s the request from so now I copy it okay I think we need to use oops sorry we need to use Coes here okay so it worked quite fast um yes lot consider strength training activity uh maybe to make it even better what we can do is also include the question so then we can see what the question is uh maybe let me make it a bit easier um question so our question was in this one so then I just put question here I think I need to do something like this let's see cool so we have the question we have the answer uh and then we have our conversation ID which we can now send to um sending feedback uh ID this is the ID okay now let's send the feedback with this ID and make the script similar to this one edit a little bit Yeah it made it even more complex but uh I think we can just use this one feedback data um and then take this one and the URL is is feedback right and data is feedback data cool so for now it's okay um we have our inje um script we have our um interface I want to spend a little bit uh of time to describe um flask background maybe so here um I will say that we use flask maybe Technologies Technologies mean search uh for searching and uh let me find search me search for text uh text search uh then what do we use uh open AI uh s and llm then flask is the uh API interface interace see yeah background uh or more onk and I want to ask CLA to write U um to write a little bit about flask and what it is and how to use it so I'll put it on pause WR a prompt and then we'll talk about this so this is what I wrote um I use flask as a part of course project but since we didn't cover flask in the course I need to provide sufficient background for people to understand what it is write a few sentences about it in markdown format um so let's copy it and put it here flk um all right we use flask uh for like again I'll put it on pause edit it and then read what I have at the end and so this is what I ended up writing here we provide background on some tech note used in the course and links for third reading and then I just include a few sentences like we use Plus for creating the API for our application um and then for example uh we can send in our case we can send the question to http Local Host 500 uh was it questions or I don't remember question I think this should be sufficient right so like for example if you use a database that is not covered in the course you can just write hey like for example if you use quadrant quadrant is a database um for like vector database and then you just briefly describe what it is and then give a link for more information by the way let's check that this link is actually valid cool it is yeah okay so right now we tested it with curl and by the way you of course need to install uh curl uh if you don't have it alternative could be uh doing it with requests and then I can quickly show it to you so I'll do assistant P install def requests and by the way I need to also specify that when you install it you also need to install Dev dependencies um yeah um to also show that it works we can include um examples the answer could the answer will look like this Jon or uh you will see something like the four looping in the response and here after sending it you receive uh the acknowledgement okay um I'm just thinking if this should be like that uh I think it's kind of clear um what it is uh cuz yeah maybe let's uh make it explicit I'll think how to organize this later maybe you'll when you check the final version you'll see how exactly I put it here cuz like it feels like kind of lonely just one sentence for the entire section um but we want the reviewers to make it we want to make it easier for reviewers to actually review that in dets flask for interface so we need to be explicit about that so I'll think how to organize it better okay and for ingestion also um we need to write that the ingestion script is in um uh Fitness assistant inest and it's run as on the start up the app in uh in app right oh it's actually in rck uh maybe for interface we can say refer to um running the application for detail okay section for more detail okay um yeah maybe later if we let's say decide to include stream lead we can also put a screenshot here um for now I think that should be enough um I'll think of what kind of screenshots we can add later um yeah and yeah I already installed request right so this is one other thing I wanted to show you so I'll create a Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '7ea46b6fb811d9f550867f094d7dc6aa'},\n",
       " {'vid_id': 'vMHve2EyA5M',\n",
       "  'title': 'LLM Zoomcamp 7.4 - Interface and ingestion pipeline',\n",
       "  'timecode': '28:00',\n",
       "  'text': \"file called test.py uh it will be import requests requests post Json um data we will take the data from here so this our question so data will be question question then this will be our data and URL is this one predict Jon and then we print response so let me format it and now I will execute the test okay something is wrong print response content so let's see why it's not um parsing it correctly yeah so we it's not predict it's question right okay now it works so we can uh command this one and just keep this one okay so we can say in the me um alternatively you can use the test. P testing it and then pen FR python testy okay I think that's enough for this one um has been a long video um so what we covered here was um interface and inje pipeline um so we took our code from The jupyter Notebook um created a few scripts then uh we spend some time describing what flask is um we don't need to describe what mean search is because uh we covered that in the course I think it wouldn't hurt to also mention that um here I want to expand on ingestion saying that because it's in memory we need to run it on Startup I don't want to do it now to save you sometime and then I also need to think a little bit um so then at the end you can just see the final result and yeah so we covered that and we need to cover monitoring continuation producibility and best practices so we will cover these two in the next one cuz I think it makes sense to cover them at the same time um because we will run grafana postgress and other things as containers so it makes sense for us to just uh to the actual app as a container too at the same time um so that's it um and actually like as go as far as reproducibility goes we try to make sure it's reproducible from the very beginning so we kind of get it already maybe we'll just check that everything is reproducible by looking out out by looking at our readme file and then making sure that we did not forget anything okay so yeah see you soon\",\n",
       "  'timecode_text': 'Testing the application with requests',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=vMHve2EyA5M&t=1680s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.4 - Interface and ingestion pipeline file called test.py uh it will be import requests requests post Json um data we will take the data from here so this our question so data will be question question then this will be our data and URL is this one predict Jon and then we print response so let me format it and now I will execute the test okay something is wrong print response content so let's see why it's not um parsing it correctly yeah so we it's not predict it's question right okay now it works so we can uh command this one and just keep this one okay so we can say in the me um alternatively you can use the test. P testing it and then pen FR python testy okay I think that's enough for this one um has been a long video um so what we covered here was um interface and inje pipeline um so we took our code from The jupyter Notebook um created a few scripts then uh we spend some time describing what flask is um we don't need to describe what mean search is because uh we covered that in the course I think it wouldn't hurt to also mention that um here I want to expand on ingestion saying that because it's in memory we need to run it on Startup I don't want to do it now to save you sometime and then I also need to think a little bit um so then at the end you can just see the final result and yeah so we covered that and we need to cover monitoring continuation producibility and best practices so we will cover these two in the next one cuz I think it makes sense to cover them at the same time um because we will run grafana postgress and other things as containers so it makes sense for us to just uh to the actual app as a container too at the same time um so that's it um and actually like as go as far as reproducibility goes we try to make sure it's reproducible from the very beginning so we kind of get it already maybe we'll just check that everything is reproducible by looking out out by looking at our readme file and then making sure that we did not forget anything okay so yeah see you soon Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '94d901e92dca0e083fd0d1bb4ffac889'},\n",
       " {'vid_id': 'nQda9etJWW8',\n",
       "  'title': 'LLM Zoomcamp 7.5 - Monitoring and containerization',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hi everyone welcome back in this series of videos we are doing an end to end project for the llm zomom course and uh we in this video we want to cover no we want to cover actually monitoring and con containerization we have what we have done so far is we covered this and I think this is already sufficient to pass the course uh but um right now we will cover these things and as I mentioned previously reproducibility is kind of cover it already cuz all we do make sure um that results are reproducible and we'll just need to also probably do a video on best practices but for now we'll cover just these two before we start I want to\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=nQda9etJWW8&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.5 - Monitoring and containerization hi everyone welcome back in this series of videos we are doing an end to end project for the llm zomom course and uh we in this video we want to cover no we want to cover actually monitoring and con containerization we have what we have done so far is we covered this and I think this is already sufficient to pass the course uh but um right now we will cover these things and as I mentioned previously reproducibility is kind of cover it already cuz all we do make sure um that results are reproducible and we'll just need to also probably do a video on best practices but for now we'll cover just these two before we start I want to Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'fecc21274502ea018af781c352529aa4'},\n",
       " {'vid_id': 'nQda9etJWW8',\n",
       "  'title': 'LLM Zoomcamp 7.5 - Monitoring and containerization',\n",
       "  'timecode': '00:55',\n",
       "  'text': \"quickly talk about the changes I made to read me so I just rearranged things slightly um so what I did so I created the section code where I described that the code is in this uh folder then listed the files then I'll add description for each file and then in this section uh code uh we have um like how we run uh how we run it uh inje script um yeah what else here I added a section on experiments where I described that all the experiments uh we have notebooks where we keep the experiments and this is how we start the notebook these are the notebooks we have and then I put the evaluation there um it's a slight change uh then as I said at the end the read me file will look different anyways um but maybe this way it's a bit easier for you to keep track of the changes so this is the version we have so far yeah so I want to add a few pictures later but um yeah this is the structure we have so far we have data which we keep here this is the code and these are our experiments okay so let's start with\",\n",
       "  'timecode_text': 'Changes in README',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=nQda9etJWW8&t=55s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.5 - Monitoring and containerization quickly talk about the changes I made to read me so I just rearranged things slightly um so what I did so I created the section code where I described that the code is in this uh folder then listed the files then I'll add description for each file and then in this section uh code uh we have um like how we run uh how we run it uh inje script um yeah what else here I added a section on experiments where I described that all the experiments uh we have notebooks where we keep the experiments and this is how we start the notebook these are the notebooks we have and then I put the evaluation there um it's a slight change uh then as I said at the end the read me file will look different anyways um but maybe this way it's a bit easier for you to keep track of the changes so this is the version we have so far yeah so I want to add a few pictures later but um yeah this is the structure we have so far we have data which we keep here this is the code and these are our experiments okay so let's start with Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '9e3a627315888e9cfda8564cc54a284b'},\n",
       " {'vid_id': 'nQda9etJWW8',\n",
       "  'title': 'LLM Zoomcamp 7.5 - Monitoring and containerization',\n",
       "  'timecode': '02:30',\n",
       "  'text': \"monitoring and containerization we cover them at the same time and I will start by going to this monitoring thing and yeah we have app and we have Docker compost and I want to copy the docker compost file here Y and what do we need from here elastic search we don't need Alama we don't need we need um pogress and we need our app so for our app we need to create a Docker file from um I'll use uh python 3.2 slim uh and then I'll add more stuff later but this will be our um Docker file right container name I don't actually know why we need container name so let's remove that um this is something we will adjust to uh for now I'll just comment this and Port uh let app port, 5,000 it depends on pogress right don't have things grafana we keep grafana of course two and I'll create do file two what do we actually have in the do end file I'll copy it we don't need these things ort and this was uh 5,000 right okay um [Music] so cool I will remove these things for now okay so this is what we have so\",\n",
       "  'timecode_text': 'Creating Dockerfile and docker-compose.yaml',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=nQda9etJWW8&t=150s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.5 - Monitoring and containerization monitoring and containerization we cover them at the same time and I will start by going to this monitoring thing and yeah we have app and we have Docker compost and I want to copy the docker compost file here Y and what do we need from here elastic search we don't need Alama we don't need we need um pogress and we need our app so for our app we need to create a Docker file from um I'll use uh python 3.2 slim uh and then I'll add more stuff later but this will be our um Docker file right container name I don't actually know why we need container name so let's remove that um this is something we will adjust to uh for now I'll just comment this and Port uh let app port, 5,000 it depends on pogress right don't have things grafana we keep grafana of course two and I'll create do file two what do we actually have in the do end file I'll copy it we don't need these things ort and this was uh 5,000 right okay um [Music] so cool I will remove these things for now okay so this is what we have so Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '04f2b8bc8c93388f742d2be9332bf802'},\n",
       " {'vid_id': 'nQda9etJWW8',\n",
       "  'title': 'LLM Zoomcamp 7.5 - Monitoring and containerization',\n",
       "  'timecode': '05:09',\n",
       "  'text': \"far and now I want to add we want to add monitoring and for that we have this prep script and BB script so let me copy prep or DB prep here uh we don't need this this we don't need this we don't need this so this is elastic search stuff actually we don't need that much from here so you would need this if you had elastic search but from what I see the only thing we need here is initializing the database and yeah I guess the only reason that we need this is we have this uh main thing so then we can easily initialize the database so then I can actually put it here and and here we'll create db. Pi do PI with these things all right uh okay so I don't think we need to um change anything here conversation ID feedback time stamp yeah I think we can just use it as uh as is without changing anything okay right we also need to [Music] install install psyo pg2 binary I hope I brought it correctly this is for the PG connection po G connection so now when let me actually in a different I'll stop it I want to stop it it doesn't stop stopped so I want to actually um what's happening stop I want to do Docker compose up and\",\n",
       "  'timecode_text': 'Code for monitoring',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=nQda9etJWW8&t=309s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.5 - Monitoring and containerization far and now I want to add we want to add monitoring and for that we have this prep script and BB script so let me copy prep or DB prep here uh we don't need this this we don't need this we don't need this so this is elastic search stuff actually we don't need that much from here so you would need this if you had elastic search but from what I see the only thing we need here is initializing the database and yeah I guess the only reason that we need this is we have this uh main thing so then we can easily initialize the database so then I can actually put it here and and here we'll create db. Pi do PI with these things all right uh okay so I don't think we need to um change anything here conversation ID feedback time stamp yeah I think we can just use it as uh as is without changing anything okay right we also need to [Music] install install psyo pg2 binary I hope I brought it correctly this is for the PG connection po G connection so now when let me actually in a different I'll stop it I want to stop it it doesn't stop stopped so I want to actually um what's happening stop I want to do Docker compose up and Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '24f4499774f10fd968daf2e0315288cf'},\n",
       " {'vid_id': 'nQda9etJWW8',\n",
       "  'title': 'LLM Zoomcamp 7.5 - Monitoring and containerization',\n",
       "  'timecode': '08:09',\n",
       "  'text': \"we need to start putting these things in the r me too cuz so far we described how to start it without doer compose um it with docker Rush compos up uh I can mention that the easiest way to run this application is with Docker um yeah if you don't use do and want to run locally we need to manually inare the environment environment and install all the depend dependencies um we're running the application locally uh do this okay and actually I'll call it running locally and then we have stting dependencies we have running the application and then uh using the application running with Docker yeah also I was thinking that maybe I should put the data set uh below after project overview here yeah run it locally run it with loer and then uh using the application one first you need to start the application either with Docker com compose or locally uh when it's running let's test it okay\",\n",
       "  'timecode_text': 'Updating README',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=nQda9etJWW8&t=489s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.5 - Monitoring and containerization we need to start putting these things in the r me too cuz so far we described how to start it without doer compose um it with docker Rush compos up uh I can mention that the easiest way to run this application is with Docker um yeah if you don't use do and want to run locally we need to manually inare the environment environment and install all the depend dependencies um we're running the application locally uh do this okay and actually I'll call it running locally and then we have stting dependencies we have running the application and then uh using the application running with Docker yeah also I was thinking that maybe I should put the data set uh below after project overview here yeah run it locally run it with loer and then uh using the application one first you need to start the application either with Docker com compose or locally uh when it's running let's test it okay Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'ade3ad4c4644a4f649425ffba4e37931'},\n",
       " {'vid_id': 'nQda9etJWW8',\n",
       "  'title': 'LLM Zoomcamp 7.5 - Monitoring and containerization',\n",
       "  'timecode': '10:56',\n",
       "  'text': \"so it what okay I probably have something already running so let me do Docker compose um Docker kill so I'm running some M stuff PS okay now nothing is running let's start one more time Docker so it says that there's already a thing called grafana Docker PSA so just to Docker around I think Q was gray still running uh let's yeah it cannot we need to kill it first and then then so now we don't have anything now let's try one more time hopefully now it should run and yeah like I don't think we should call we should have the gra name many things are happening here um okay and now we run it let me open another thing and I go to fitness assistant and I want to do p and\",\n",
       "  'timecode_text': 'Running docker-compose up (+debugging)',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=nQda9etJWW8&t=656s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.5 - Monitoring and containerization so it what okay I probably have something already running so let me do Docker compose um Docker kill so I'm running some M stuff PS okay now nothing is running let's start one more time Docker so it says that there's already a thing called grafana Docker PSA so just to Docker around I think Q was gray still running uh let's yeah it cannot we need to kill it first and then then so now we don't have anything now let's try one more time hopefully now it should run and yeah like I don't think we should call we should have the gra name many things are happening here um okay and now we run it let me open another thing and I go to fitness assistant and I want to do p and Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '068237c34f6f9a5ab89eaaf019e6ccf6'},\n",
       " {'vid_id': 'nQda9etJWW8',\n",
       "  'title': 'LLM Zoomcamp 7.5 - Monitoring and containerization',\n",
       "  'timecode': '13:02',\n",
       "  'text': \"run DB prep I probably need to it will not what ah right I forgot to add python yeah and n install is it python. N I think so luckily when you review a project or when you need to run the project you don't need to install all these Library separately you just do p install minus minus Def and you have all these dependencies and then uh I already want to start filling in the me where we okay so right now when we start we need to um prepare the database right eping the application uh before we can use the up we need to iniz the database we can do it by running the DP by script so going me make it a little bit fency uh script CD Fitness assistant p and from python DP prep. and we also need to set up some environmental variables cuz it will now not be able to connect to yeah so it will not be able to connect to the database right now yeah CU right now our database is running locally so we need what we need to do is to overwrite this so yeah this one should actually be committed I just noticed that it was gray so yeah I removed it from get ignore um okay so now we need to change postgress host and where is the me File Expert post host loal post or actually instead of exporting it we want to Define it only for this round so I'll do something like this then are the what um yeah let me so the reason I'm doing this is uh it will set it to Local Host but probably it doesn't get still need to do export it doesn't get propagated to the actual script or do 10 is overwriting it I don't know actually check what we have here yeah so we load it here and then get in postgress host and this is what I have here yeah maybe maybe I should not use this thing yeah let's see H yeah I I'll put it on pause and try to figure out I I don't know why it's not picking up okay so the reason is actually because we run with P somehow it doesn't see the environmental variables exported before so what I do what I will do is p\",\n",
       "  'timecode_text': 'Initializing the database (+debugging)',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=nQda9etJWW8&t=782s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.5 - Monitoring and containerization run DB prep I probably need to it will not what ah right I forgot to add python yeah and n install is it python. N I think so luckily when you review a project or when you need to run the project you don't need to install all these Library separately you just do p install minus minus Def and you have all these dependencies and then uh I already want to start filling in the me where we okay so right now when we start we need to um prepare the database right eping the application uh before we can use the up we need to iniz the database we can do it by running the DP by script so going me make it a little bit fency uh script CD Fitness assistant p and from python DP prep. and we also need to set up some environmental variables cuz it will now not be able to connect to yeah so it will not be able to connect to the database right now yeah CU right now our database is running locally so we need what we need to do is to overwrite this so yeah this one should actually be committed I just noticed that it was gray so yeah I removed it from get ignore um okay so now we need to change postgress host and where is the me File Expert post host loal post or actually instead of exporting it we want to Define it only for this round so I'll do something like this then are the what um yeah let me so the reason I'm doing this is uh it will set it to Local Host but probably it doesn't get still need to do export it doesn't get propagated to the actual script or do 10 is overwriting it I don't know actually check what we have here yeah so we load it here and then get in postgress host and this is what I have here yeah maybe maybe I should not use this thing yeah let's see H yeah I I'll put it on pause and try to figure out I I don't know why it's not picking up okay so the reason is actually because we run with P somehow it doesn't see the environmental variables exported before so what I do what I will do is p Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'd54a8ddbc7af6bc780f88a3fcbfbadc1'},\n",
       " {'vid_id': 'nQda9etJWW8',\n",
       "  'title': 'LLM Zoomcamp 7.5 - Monitoring and containerization',\n",
       "  'timecode': '18:24',\n",
       "  'text': \"shell and I need to update the here be shell so we'll do something like that so when we do pan shell we enter the virtual environment created by pan so yeah I do sport by the way I need to go to assistant and then now run this yeah this time it worked um so now we have we initialized the database so all the data is there of course we can do it even more automated in a more automated way so when we start the application for the first time we can check if the data if the tables are not there we initialize them but yeah I don't want to make it more complex then it should be um cool so um yeah I need to check how to actually propagate the environmental variables through uh pen but not now so now it's working and we initialize the database let's Now work on our Docker compost uh and um yeah run it with doer\",\n",
       "  'timecode_text': 'Initializing the database',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=nQda9etJWW8&t=1104s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.5 - Monitoring and containerization shell and I need to update the here be shell so we'll do something like that so when we do pan shell we enter the virtual environment created by pan so yeah I do sport by the way I need to go to assistant and then now run this yeah this time it worked um so now we have we initialized the database so all the data is there of course we can do it even more automated in a more automated way so when we start the application for the first time we can check if the data if the tables are not there we initialize them but yeah I don't want to make it more complex then it should be um cool so um yeah I need to check how to actually propagate the environmental variables through uh pen but not now so now it's working and we initialize the database let's Now work on our Docker compost uh and um yeah run it with doer Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '702bd0e05799baa6e0e409aa506961cb'},\n",
       " {'vid_id': 'nQda9etJWW8',\n",
       "  'title': 'LLM Zoomcamp 7.5 - Monitoring and containerization',\n",
       "  'timecode': '19:53',\n",
       "  'text': \"compul so first of all I want to change the location for the data so I'll do I'll create an environmental variable data path and it'll be always get no like that the default value will be this one and I don't remember if when we do rock we actually yeah we just use the default value right or maybe actually we should configure it here but yeah I I will not spend much time on thinking about that right now okay data path and it's now in our Docker file what I want to copy is first the data this data CSV to data data CSV then we copy um we copy the pipn PIP file and so first of all I want to say that work directory is SL up then we copy data copy peep pep file and P file lock to the current directory uh now we yeah run P install even before that um and now run yeah I like these suggestions from G GitHub compiled and then I copy Fitness assistant to the current directory all these files and yeah we don't need PE and anymore and ideally instead of using flask uh it's actually appp instead of using flask for serving the application we should install something like G unicorn um yeah let's actually do it right now G unicorn so G unicorn is a special thing um that allows to serve flask applications and other uh vsgi applications so we shouldn't think too much about that right now like it's outside of the scope for this and I don't think we should mention this in the read me too cuz uh when you run locally you just use uh flask but when you run it with Docker under the hood it will use unicorn so the common line should be maybe entry point not common line CMD G un horn yeah help me I don't remember how to do this okay let me just ask how do I serve BK with G unor yeah well this is what I need and I think it was five or 5,000 and it's app.py and then inside we have app so it's app app okay I hope Works expose right so I think um that looks complete so we should also provide some environmental variables but we will do this later okay right now I'll just do Docker build minus t uh Fitness assistant and I need a l so we will not need when you run this you will not need to do this that's why I don't add this instructions so with instructions we just do Docker compos up and then it works right now I'm just testing that these things work so right now it's just going through the docker file uh and installing all the things yeah I'll put it on pause and then come back when it finishes okay it's ready so let's test it for that I'll do Docker run minus a uhm then the name of the thing is Fitness assistant then we need to send uh to set some environmental variables like for example open AI FBI key which will be set to this um and then we open port by th000 right or maybe it's actually I could think to also keep track of these things um okay did I install the Unicorn let's do it one more time H weird so we have two unicorn but for some reasons it's not picking it up I will just ask this thing my pile oops and this is how I oh my locker file let's see what's is missing but why do I need to do uh ah right I need to do system this way so the reason I want to add system is it skips creating the virtual environment and it will install directly as if I I just used peep so I'll put it on pause while it's installing this and then we'll test it again okay done now let's test it so this is how it should be okay let's try one more time rebuilding this thing now should be pretty fast what's happening I have no idea what this thing is actually like CMD okay let's do CMD to be honest I don't remember what's the difference between CD and ENT point maybe I should check it yeah seems to be working now we can what ah right so and the other environmental variable we need to send to set is e what was that just data path uh data data CC was it something like that yeah so let's test this one and now we're kind of tested so now I'll just execute the test script that we have assistant python test yeah it still works cool so now we have the docker thing uh I'll just just in case so we have this with Docker Docker compose if you need to change something in the docker file and test it quickly you can use the following points and we will need to update it because later we will also add like pogress here so we'll need to pass this information to uh which we don't need to worry about when we use doer compost okay so this thing is working now we need to add um now we\",\n",
       "  'timecode_text': 'Containerization of the application',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=nQda9etJWW8&t=1193s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.5 - Monitoring and containerization compul so first of all I want to change the location for the data so I'll do I'll create an environmental variable data path and it'll be always get no like that the default value will be this one and I don't remember if when we do rock we actually yeah we just use the default value right or maybe actually we should configure it here but yeah I I will not spend much time on thinking about that right now okay data path and it's now in our Docker file what I want to copy is first the data this data CSV to data data CSV then we copy um we copy the pipn PIP file and so first of all I want to say that work directory is SL up then we copy data copy peep pep file and P file lock to the current directory uh now we yeah run P install even before that um and now run yeah I like these suggestions from G GitHub compiled and then I copy Fitness assistant to the current directory all these files and yeah we don't need PE and anymore and ideally instead of using flask uh it's actually appp instead of using flask for serving the application we should install something like G unicorn um yeah let's actually do it right now G unicorn so G unicorn is a special thing um that allows to serve flask applications and other uh vsgi applications so we shouldn't think too much about that right now like it's outside of the scope for this and I don't think we should mention this in the read me too cuz uh when you run locally you just use uh flask but when you run it with Docker under the hood it will use unicorn so the common line should be maybe entry point not common line CMD G un horn yeah help me I don't remember how to do this okay let me just ask how do I serve BK with G unor yeah well this is what I need and I think it was five or 5,000 and it's app.py and then inside we have app so it's app app okay I hope Works expose right so I think um that looks complete so we should also provide some environmental variables but we will do this later okay right now I'll just do Docker build minus t uh Fitness assistant and I need a l so we will not need when you run this you will not need to do this that's why I don't add this instructions so with instructions we just do Docker compos up and then it works right now I'm just testing that these things work so right now it's just going through the docker file uh and installing all the things yeah I'll put it on pause and then come back when it finishes okay it's ready so let's test it for that I'll do Docker run minus a uhm then the name of the thing is Fitness assistant then we need to send uh to set some environmental variables like for example open AI FBI key which will be set to this um and then we open port by th000 right or maybe it's actually I could think to also keep track of these things um okay did I install the Unicorn let's do it one more time H weird so we have two unicorn but for some reasons it's not picking it up I will just ask this thing my pile oops and this is how I oh my locker file let's see what's is missing but why do I need to do uh ah right I need to do system this way so the reason I want to add system is it skips creating the virtual environment and it will install directly as if I I just used peep so I'll put it on pause while it's installing this and then we'll test it again okay done now let's test it so this is how it should be okay let's try one more time rebuilding this thing now should be pretty fast what's happening I have no idea what this thing is actually like CMD okay let's do CMD to be honest I don't remember what's the difference between CD and ENT point maybe I should check it yeah seems to be working now we can what ah right so and the other environmental variable we need to send to set is e what was that just data path uh data data CC was it something like that yeah so let's test this one and now we're kind of tested so now I'll just execute the test script that we have assistant python test yeah it still works cool so now we have the docker thing uh I'll just just in case so we have this with Docker Docker compose if you need to change something in the docker file and test it quickly you can use the following points and we will need to update it because later we will also add like pogress here so we'll need to pass this information to uh which we don't need to worry about when we use doer compost okay so this thing is working now we need to add um now we Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'd266d46f8d207fb241a46bfb9dcfebf2'},\n",
       " {'vid_id': 'nQda9etJWW8',\n",
       "  'title': 'LLM Zoomcamp 7.5 - Monitoring and containerization',\n",
       "  'timecode': '31:42',\n",
       "  'text': \"need to add login here and save all the information to the that to the database so we do import TV and right now I don't want to do it in Docker so what I'll do is I will just run and shell then I'll export so I think I need to update it and shell um so I think running the application I'll put it yeah I'm now confused as where where to put what so let's put prepareing application here and running it locally here okay P install F uh running the application P shell and then I want to do this this exactly I'll need to improve the me file cuz like uh we install pen here but use it here so then it's uh kind kind of strange um and probably I need to explain what p face also in me for now I will do this so I run people shelf expert and then round app and this is the reason why we use the Unicorn this is the warning this warning is the reason why we use it um so not to have this morning uh hopefully it still works cool um yeah so what we want to do now is take this and add database and for that I'll go to our uh code here and see how we used it here so where did we log it I think we this get answer this is where we LED it it's in Rock right or assistant yeah assistant yeah this code is very complex uh get answer build prompt um and somewhere we lock this I don't remember where so what I'm looking for is the piece of code where we invoke this thing safe conversation but maybe what I can do is just yeah use it DB save conversation and these are the things we want to so conversation ID is this one question is this one what is answer data oh wow many things um yeah so for that we need um to update our uh Rock function right answer data and course yeah course we don't need the course yeah so here the thing is that we still have the course so I'll just put none um ideally I should have removed this maybe let's actually do that yeah we don't have courses anywhere here anymore right and I need to run um python DB in it again is it how is it called python python DB prep so then it deletes the whole database and creates a new one without the course\",\n",
       "  'timecode_text': 'Adding database logging',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=nQda9etJWW8&t=1902s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.5 - Monitoring and containerization need to add login here and save all the information to the that to the database so we do import TV and right now I don't want to do it in Docker so what I'll do is I will just run and shell then I'll export so I think I need to update it and shell um so I think running the application I'll put it yeah I'm now confused as where where to put what so let's put prepareing application here and running it locally here okay P install F uh running the application P shell and then I want to do this this exactly I'll need to improve the me file cuz like uh we install pen here but use it here so then it's uh kind kind of strange um and probably I need to explain what p face also in me for now I will do this so I run people shelf expert and then round app and this is the reason why we use the Unicorn this is the warning this warning is the reason why we use it um so not to have this morning uh hopefully it still works cool um yeah so what we want to do now is take this and add database and for that I'll go to our uh code here and see how we used it here so where did we log it I think we this get answer this is where we LED it it's in Rock right or assistant yeah assistant yeah this code is very complex uh get answer build prompt um and somewhere we lock this I don't remember where so what I'm looking for is the piece of code where we invoke this thing safe conversation but maybe what I can do is just yeah use it DB save conversation and these are the things we want to so conversation ID is this one question is this one what is answer data oh wow many things um yeah so for that we need um to update our uh Rock function right answer data and course yeah course we don't need the course yeah so here the thing is that we still have the course so I'll just put none um ideally I should have removed this maybe let's actually do that yeah we don't have courses anywhere here anymore right and I need to run um python DB in it again is it how is it called python python DB prep so then it deletes the whole database and creates a new one without the course Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '8c6ff9609750cc7ad3b9836e9a8f4f99'},\n",
       " {'vid_id': 'nQda9etJWW8',\n",
       "  'title': 'LLM Zoomcamp 7.5 - Monitoring and containerization',\n",
       "  'timecode': '37:30',\n",
       "  'text': \"so this is what we need so the answer data it needs to have these things I'll just put it like that so now we will need to actually change it answer data answer and now let's update our Rock function to include answer data answer data answer will be the answer model used model OKAY response time Let's uh add this import time time import time yeah okay so from time I'll call it t0 time and then T1 is end time and two T1 minus t0 so this is the time and seconds it took uh to answer response time then we have um relevance relevance exclanation from tokens completion tokens tokens I cost and that's it right so we will need to uh let me just uh keep it like that for now just to test that this thing still works and doesn't fall apart and then we will gradually feel the rest okay okay so what's really want so we just want to make sure that these things appear in our database right okay so does it still work 36 data okay so we just need three conversation question answer data yeah so we passed all these things uh let's see that this thing actually works now so I will run the test and let's see what is happening uh yeah there's probably some sort of Stack Trace yeah let's see what is there I don't know uh so probably when we remove the course something things happen so DB line 65 yeah this one so I will just ask Claud to fix it cuz probably uh I have too many like this uh percent signs I have an error here or maybe actually a safe conversation should happen in rock I don't well let's keep it in an app yeah this is what I thought so the number of placeholders in your SQL query doesn't match the number of values you're trying to insert let's let's fix it and it will now take this thing be what did I actually copy up to time stop right yeah I'm just trying to figure out what I need to copy so first is the query and next is this thing okay so I think what happened is just it deleted one of these things I think I also did deleted it what is that yeah okay now it works and it should save the data to uh postgress um well ideally I should go and check but I think it works right so we will see later when we set up grafana that it actually works um okay so this one works now we just need to implement safe feedback DP safe DB safe feedback okay I will now execute this thing ID will be this and let me execute it this way what is wrong uh yeah maybe URL let's put here for completeness cool so now it works we can save uh the logs and we can save the feedback uh using this okay so what else we need to do here so it kind of works more or less we just need to make sure that these things that we return are good and for that we need to to also Implement that right so I'll go back to the zoom camp and where is the app assistant I think this is where we Implement all the things right so evate relevance uh so first of all let's go to llm and also so answer tokens yeah go to the llm and we have the answer we have tokens and this will be the response answer tokens or token stats it's better and right now let's find where we use LM in just one place this token stats so token starts uh here is it right from tokens completion tokens total tokens okay token STS completion tokens and total tokens um okay now we have uh prom tokens and uh completion tokens for evaluation right so we want to also evaluate the results and the response time it's the the\",\n",
       "  'timecode_text': 'Capturing extra information from LLM calls + debuggning',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=nQda9etJWW8&t=2250s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.5 - Monitoring and containerization so this is what we need so the answer data it needs to have these things I'll just put it like that so now we will need to actually change it answer data answer and now let's update our Rock function to include answer data answer data answer will be the answer model used model OKAY response time Let's uh add this import time time import time yeah okay so from time I'll call it t0 time and then T1 is end time and two T1 minus t0 so this is the time and seconds it took uh to answer response time then we have um relevance relevance exclanation from tokens completion tokens tokens I cost and that's it right so we will need to uh let me just uh keep it like that for now just to test that this thing still works and doesn't fall apart and then we will gradually feel the rest okay okay so what's really want so we just want to make sure that these things appear in our database right okay so does it still work 36 data okay so we just need three conversation question answer data yeah so we passed all these things uh let's see that this thing actually works now so I will run the test and let's see what is happening uh yeah there's probably some sort of Stack Trace yeah let's see what is there I don't know uh so probably when we remove the course something things happen so DB line 65 yeah this one so I will just ask Claud to fix it cuz probably uh I have too many like this uh percent signs I have an error here or maybe actually a safe conversation should happen in rock I don't well let's keep it in an app yeah this is what I thought so the number of placeholders in your SQL query doesn't match the number of values you're trying to insert let's let's fix it and it will now take this thing be what did I actually copy up to time stop right yeah I'm just trying to figure out what I need to copy so first is the query and next is this thing okay so I think what happened is just it deleted one of these things I think I also did deleted it what is that yeah okay now it works and it should save the data to uh postgress um well ideally I should go and check but I think it works right so we will see later when we set up grafana that it actually works um okay so this one works now we just need to implement safe feedback DP safe DB safe feedback okay I will now execute this thing ID will be this and let me execute it this way what is wrong uh yeah maybe URL let's put here for completeness cool so now it works we can save uh the logs and we can save the feedback uh using this okay so what else we need to do here so it kind of works more or less we just need to make sure that these things that we return are good and for that we need to to also Implement that right so I'll go back to the zoom camp and where is the app assistant I think this is where we Implement all the things right so evate relevance uh so first of all let's go to llm and also so answer tokens yeah go to the llm and we have the answer we have tokens and this will be the response answer tokens or token stats it's better and right now let's find where we use LM in just one place this token stats so token starts uh here is it right from tokens completion tokens total tokens okay token STS completion tokens and total tokens um okay now we have uh prom tokens and uh completion tokens for evaluation right so we want to also evaluate the results and the response time it's the the Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '49b73a7d01ec5bb5e0d95f9d51309a06'},\n",
       " {'vid_id': 'nQda9etJWW8',\n",
       "  'title': 'LLM Zoomcamp 7.5 - Monitoring and containerization',\n",
       "  'timecode': '48:00',\n",
       "  'text': \"entire thing right so I want to include everything here also evaluation so how do we evaluate evaluate relevance I will now take the prompt from our our prompt not the one from the course we have it here in this thing you need to scroll here you go so this is the prompt I will just copy it and paste it here I want to put it outside so it kind of doesn't look messy uh prompt evaluation prompt template so here evolation prom template uh so it will be GPT model GPT for o mini for revolation I need to import Json okay evolation tokens then we cool and now we just need to let make do it this way J all and so why do we need to return so many things then this one is already a dictionary uh how does it look like so it should be result okay like that and Jon well yeah I like this way it's just two parameters not three [Music] um here evalate relevance question and answer and the results will be relevant and then real starts um it is relevance right and or actually we need to make sure that these things uh are there um yeah I don't know like probably it will fail if relevance is not there when it fails to stick to the format we need I'll probably use get and then default value will be unknown get explanation default value will be failed to par solation okay so now I just need to put this thing here this here and this here and then add a prefix I'll starts token starts and then finally the\",\n",
       "  'timecode_text': 'Adding LLM evaluation',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=nQda9etJWW8&t=2880s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.5 - Monitoring and containerization entire thing right so I want to include everything here also evaluation so how do we evaluate evaluate relevance I will now take the prompt from our our prompt not the one from the course we have it here in this thing you need to scroll here you go so this is the prompt I will just copy it and paste it here I want to put it outside so it kind of doesn't look messy uh prompt evaluation prompt template so here evolation prom template uh so it will be GPT model GPT for o mini for revolation I need to import Json okay evolation tokens then we cool and now we just need to let make do it this way J all and so why do we need to return so many things then this one is already a dictionary uh how does it look like so it should be result okay like that and Jon well yeah I like this way it's just two parameters not three [Music] um here evalate relevance question and answer and the results will be relevant and then real starts um it is relevance right and or actually we need to make sure that these things uh are there um yeah I don't know like probably it will fail if relevance is not there when it fails to stick to the format we need I'll probably use get and then default value will be unknown get explanation default value will be failed to par solation okay so now I just need to put this thing here this here and this here and then add a prefix I'll starts token starts and then finally the Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '08516a5c16872704edf9414618e852b2'},\n",
       " {'vid_id': 'nQda9etJWW8',\n",
       "  'title': 'LLM Zoomcamp 7.5 - Monitoring and containerization',\n",
       "  'timecode': '52:15',\n",
       "  'text': \"cost but that we have a formula right where is it open cost um if model to mini so let me Google G pt4 or mini cost this one where is the Mini one ah here can I do it for 1,000 tokens how like there is somewhere at toggle I think no ah here so input tokens output tokens and this one is output tokens right okay so we have our cost else uh print yeah so maybe we should even uh raise an exception here or I don't know do something um right now I know that we will not use anything else apart from this model but of course like this we have to be we have to let the user know that this part of the implementation is missing if we want to start using something else okay so now we just need to add um where is it we need to add this open AI cost and actually like this model tokens model to starts answer or rock of all and total cost will be yeah this one so now we have the costs and yeah now we just need to hope that it still works let me format the file one more time see what we have in our terminal okay well atly started now let me run the test okay something is wrong iation prom template format yeah here we need to use just answer yeah okay so now we saved everything to\",\n",
       "  'timecode_text': 'Logging the cost',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=nQda9etJWW8&t=3135s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.5 - Monitoring and containerization cost but that we have a formula right where is it open cost um if model to mini so let me Google G pt4 or mini cost this one where is the Mini one ah here can I do it for 1,000 tokens how like there is somewhere at toggle I think no ah here so input tokens output tokens and this one is output tokens right okay so we have our cost else uh print yeah so maybe we should even uh raise an exception here or I don't know do something um right now I know that we will not use anything else apart from this model but of course like this we have to be we have to let the user know that this part of the implementation is missing if we want to start using something else okay so now we just need to add um where is it we need to add this open AI cost and actually like this model tokens model to starts answer or rock of all and total cost will be yeah this one so now we have the costs and yeah now we just need to hope that it still works let me format the file one more time see what we have in our terminal okay well atly started now let me run the test okay something is wrong iation prom template format yeah here we need to use just answer yeah okay so now we saved everything to Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '9b2891eca70ba54fb5847fbfc109d039'},\n",
       " {'vid_id': 'nQda9etJWW8',\n",
       "  'title': 'LLM Zoomcamp 7.5 - Monitoring and containerization',\n",
       "  'timecode': '56:20',\n",
       "  'text': \"the database and yeah I don't know if I should show now how to set up grafana should be relatively straightforward so you go to Local Host and then what's the where is our doer compos for grafana 3,000 and what we can do is we can just take is it admin admin wait yeah whatever and uh yeah so we just need to do the same thing we did previously we also have in our course I think we saved the dashboard right so now I can just download it think I will also need to update the data source right in multiple times but now you know what to do right so you just add the data source to grafana in this one uh this is something I will add to the instructions so I will add this dashboard I will add to the instructions how exactly to set up grafana I just don't want to spend time now and take to 20 minutes uh of your time again um because this is relatively straightforward now so so what I want to do is add the instructions on how to do that I'll actually do this first but then add the instructions so it's totally reproducible and then add a screenshot of that and probably in the next video I'll just show you the results okay that's all for now we have\",\n",
       "  'timecode_text': 'Grafana',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=nQda9etJWW8&t=3380s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.5 - Monitoring and containerization the database and yeah I don't know if I should show now how to set up grafana should be relatively straightforward so you go to Local Host and then what's the where is our doer compos for grafana 3,000 and what we can do is we can just take is it admin admin wait yeah whatever and uh yeah so we just need to do the same thing we did previously we also have in our course I think we saved the dashboard right so now I can just download it think I will also need to update the data source right in multiple times but now you know what to do right so you just add the data source to grafana in this one uh this is something I will add to the instructions so I will add this dashboard I will add to the instructions how exactly to set up grafana I just don't want to spend time now and take to 20 minutes uh of your time again um because this is relatively straightforward now so so what I want to do is add the instructions on how to do that I'll actually do this first but then add the instructions so it's totally reproducible and then add a screenshot of that and probably in the next video I'll just show you the results okay that's all for now we have Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '27c3fa046a1f37e7d849589f65df3fbd'},\n",
       " {'vid_id': 'nQda9etJWW8',\n",
       "  'title': 'LLM Zoomcamp 7.5 - Monitoring and containerization',\n",
       "  'timecode': '58:18',\n",
       "  'text': \"done a lot of work so we uh added let me actually see what we did so we added conization we added and we added monitoring right okay so we just have uh one last video about best practices I don't know if I want to record this or not maybe I'll just like I I think you get the idea you just follow what we did in the videos um in the course videos you copy the code from there you tweak it to your use case and then you show right so um yeah maybe I'll show it I don't know but for now I'm kind of tired of recording I just want to um record one more video kind of summary and then later maybe I'll record more but this project will definitely pass because we would get two points according to criteria for this for this for this for this for this for this for this for this and hopefully for this when I finish uh the read me when I add all the instructions for grafana uh right now we will not add any points for this and for this but this is just one two three five points so with this project you would definitely pass okay that has been fun a bit long but fun so see you in next video in the summary video right now I'll work on the readme file and I want to show you the final results and then share some closing notes so see you soon\",\n",
       "  'timecode_text': 'Summary',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=nQda9etJWW8&t=3498s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.5 - Monitoring and containerization done a lot of work so we uh added let me actually see what we did so we added conization we added and we added monitoring right okay so we just have uh one last video about best practices I don't know if I want to record this or not maybe I'll just like I I think you get the idea you just follow what we did in the videos um in the course videos you copy the code from there you tweak it to your use case and then you show right so um yeah maybe I'll show it I don't know but for now I'm kind of tired of recording I just want to um record one more video kind of summary and then later maybe I'll record more but this project will definitely pass because we would get two points according to criteria for this for this for this for this for this for this for this for this and hopefully for this when I finish uh the read me when I add all the instructions for grafana uh right now we will not add any points for this and for this but this is just one two three five points so with this project you would definitely pass okay that has been fun a bit long but fun so see you in next video in the summary video right now I'll work on the readme file and I want to show you the final results and then share some closing notes so see you soon Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '670679f1e1ff908c7b0d1b1f720cc638'},\n",
       " {'vid_id': 'TW9M5VE8vpo',\n",
       "  'title': 'LLM Zoomcamp 7.6 - Summary and closing remarks',\n",
       "  'timecode': '00:00',\n",
       "  'text': \"hi everyone welcome back in this series of videos we are building an end to end project for our course LM Zoom camp and in this video I want to give a summary of what we have done here and I've between this the previous video and this video I've done some work so I want to talk about this work what I exactly I have done and I want to show you the end results right so let's start I first I'll talk about what I've done so I spent uh maybe three extra hours uh working on the project um so that uh included so there was uh three main things first I worked on login with pogress I had uh some weird problems with time stamps which I which I solved and you can find the solution like for some reasons my Docker clock uh clock inside Docker was out of sync with my host machine that is why the time stamps that um were generated by postgress and by applications inside Docker were like six seven hours behind my actual time so I also add added a note to uh for like how to solve that this might not be important for you you might not have this problem but this is what I spend time on so there are always this unexpected problems that take time then\",\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=TW9M5VE8vpo&t=0s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.6 - Summary and closing remarks hi everyone welcome back in this series of videos we are building an end to end project for our course LM Zoom camp and in this video I want to give a summary of what we have done here and I've between this the previous video and this video I've done some work so I want to talk about this work what I exactly I have done and I want to show you the end results right so let's start I first I'll talk about what I've done so I spent uh maybe three extra hours uh working on the project um so that uh included so there was uh three main things first I worked on login with pogress I had uh some weird problems with time stamps which I which I solved and you can find the solution like for some reasons my Docker clock uh clock inside Docker was out of sync with my host machine that is why the time stamps that um were generated by postgress and by applications inside Docker were like six seven hours behind my actual time so I also add added a note to uh for like how to solve that this might not be important for you you might not have this problem but this is what I spend time on so there are always this unexpected problems that take time then Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '1e89282de0d0e7fa639db91997ba6225'},\n",
       " {'vid_id': 'TW9M5VE8vpo',\n",
       "  'title': 'LLM Zoomcamp 7.6 - Summary and closing remarks',\n",
       "  'timecode': '01:30',\n",
       "  'text': \"the other thing I did was um finishing grafana and instead of um so what we did in the course is I showed you how to export a grafana dashboard and then I showed you how to load but that was manual right instead what I wanted to do for the project is automated um loading of a dashboard so I added some instructions uh Gra um so here I addit this monitoring session where I um I mentioned how to actually do that so like there's this in. P script which you can run to make sure that which you run to load this dashboard. Json so this script you can take a look at this you can also use it in your projects if you want um so what it does is uh like there's a lot of code mostly not mostly like it's written actually with CLA it took a few iterations like it didn't work from the first attempt so I needed to spend some time maybe uh an hour to make it work um with a little bit uh um yeah debugging and asking for to help and then Googling to uh yeah can imagine I had to Google anyways uh so what it's doing is first it creates a data source Source if it does not exist yet uh postgress data source and then it uses uh then it creates a dashboard using this data source right so you can take a look at the code here um and yeah so this is what uh I did when it comes to code I don't think I changed anything else with respect to code maybe like some little things here and there but nothing\",\n",
       "  'timecode_text': 'Grafana dashboard',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=TW9M5VE8vpo&t=90s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.6 - Summary and closing remarks the other thing I did was um finishing grafana and instead of um so what we did in the course is I showed you how to export a grafana dashboard and then I showed you how to load but that was manual right instead what I wanted to do for the project is automated um loading of a dashboard so I added some instructions uh Gra um so here I addit this monitoring session where I um I mentioned how to actually do that so like there's this in. P script which you can run to make sure that which you run to load this dashboard. Json so this script you can take a look at this you can also use it in your projects if you want um so what it does is uh like there's a lot of code mostly not mostly like it's written actually with CLA it took a few iterations like it didn't work from the first attempt so I needed to spend some time maybe uh an hour to make it work um with a little bit uh um yeah debugging and asking for to help and then Googling to uh yeah can imagine I had to Google anyways uh so what it's doing is first it creates a data source Source if it does not exist yet uh postgress data source and then it uses uh then it creates a dashboard using this data source right so you can take a look at the code here um and yeah so this is what uh I did when it comes to code I don't think I changed anything else with respect to code maybe like some little things here and there but nothing Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '4df09f76c18489d7c085bab75d4bf547'},\n",
       " {'vid_id': 'TW9M5VE8vpo',\n",
       "  'title': 'LLM Zoomcamp 7.6 - Summary and closing remarks',\n",
       "  'timecode': '03:25',\n",
       "  'text': \"significant and now I want to talk about R me so polished stre me file uh you can see this picture here maybe let me just go to here um so I polished the read me file I added uh so this picture is of course generated with AI so I used chpt to generate that um I just copied the like a few like this I think and ask hey can you generate a banner for my fitness assistant application something like that and like I did I made a few itations on that like it's kind of weird CU like dumbbell is kind of flying and uh I kind of know why this Rose of dumbbell is kind of inside the this thing but doesn't matter right so there is a picture it looks nicer so then I added like I moved things around here uh edited uh I also use chpt I asked chpt hey like can you please fix grammar and typos in my readme and then um I gave the entire content of this file and then update based on the feedback um so yeah you can of course you should take a look at that but I can just go through this and say what uh what I did there right so I added an example of how it should be used and ah yeah I I don't think I mentioned that so I also created a c application which this is how it looks like so you can ask it a question and then the app the CLI App application asks you like whether it was good or not which I demonstrate here then I also recorded a video that shows how to use a project and how to set up uh how to run this project on GitHub codes spaces I'll not open here you can check it uh yourself but is something you can also do uh in your for your own projects and I think uh especially if you use any uh I don't know if you deploy to the cloud and you don't want to keep your application running you can just record a demo show how to access this application and then turn it off right so then you don't need to pay for anything CU like if you create a database in the cloud if you create uh like is2 instance maybe you need to use something else then and like it can quickly add up uh so what you can do instead is just deploy it once record the video and then just have instructions of how to deploy it but anyways so this is uh the video check it out and then I think most of these things you saw like project overview data set I think I add a link to the data set here I describe the Technologies uh then I said what you need to do in order to prepare so install deeral get the key uh install ppen and install the dependencies then this is how you run it first you need to configure the database um and then you actually do Docker compose up and it works uh what if you want to run it outside of Docker what if you just want to run it with Docker without compul all these instructions are here then also this is the issue I struggled with uh for some time like making sure that time stamps are correct so I added a few checks here so you can read about that too um again hopefully if you try to reproduce it it shouldn't be a problem for you it was specific to my uh environment and then uh how exactly you can use it so once the thing is running how you can interact with uh with applications so for that there is the C application then also what I did is instead of just typing they c i can randomly load a question from the ground prooof data set which is what uh which is also what this test.py is doing previously it was sending the same request over and over again but now I thought um it would be nicer if it just picks a random question and then sends it okay and then example with curl then I describ the code what exactly we have like in the F Fitness assistant folder uh outside of it like this test.py clip um then I talk about the interface the for the API then ingestion uh then there's a section on experiments so this is where we did our evolation so evolation for retrieval which is uh a basic approach and then tuning uh our parameters and then evaluating Rock flow in our case we because we don't have a uh like the how to say uh like the answer the ground Ruth answer or the gold standard answer right we only have um we only have uh like a piece of text or not even a roll from a CSV file uh the only L is Judge mric we can use is like okay here's a question here's an answer like is it actually relevant or not and uh yeah evaluation of two and then for monitoring I added a screenshot of graph f a dashboard and then uh I also describe what kind of um tiles are there panels and actually to describe that what I did is I just took the dashboard of Json file put it to chpd and ask hey can you describe me what is inside and this is how you set up grafana so here I describe the files inside the grafana folder and this is how you run them right um so this is uh this is the project um\",\n",
       "  'timecode_text': 'README',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=TW9M5VE8vpo&t=205s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.6 - Summary and closing remarks significant and now I want to talk about R me so polished stre me file uh you can see this picture here maybe let me just go to here um so I polished the read me file I added uh so this picture is of course generated with AI so I used chpt to generate that um I just copied the like a few like this I think and ask hey can you generate a banner for my fitness assistant application something like that and like I did I made a few itations on that like it's kind of weird CU like dumbbell is kind of flying and uh I kind of know why this Rose of dumbbell is kind of inside the this thing but doesn't matter right so there is a picture it looks nicer so then I added like I moved things around here uh edited uh I also use chpt I asked chpt hey like can you please fix grammar and typos in my readme and then um I gave the entire content of this file and then update based on the feedback um so yeah you can of course you should take a look at that but I can just go through this and say what uh what I did there right so I added an example of how it should be used and ah yeah I I don't think I mentioned that so I also created a c application which this is how it looks like so you can ask it a question and then the app the CLI App application asks you like whether it was good or not which I demonstrate here then I also recorded a video that shows how to use a project and how to set up uh how to run this project on GitHub codes spaces I'll not open here you can check it uh yourself but is something you can also do uh in your for your own projects and I think uh especially if you use any uh I don't know if you deploy to the cloud and you don't want to keep your application running you can just record a demo show how to access this application and then turn it off right so then you don't need to pay for anything CU like if you create a database in the cloud if you create uh like is2 instance maybe you need to use something else then and like it can quickly add up uh so what you can do instead is just deploy it once record the video and then just have instructions of how to deploy it but anyways so this is uh the video check it out and then I think most of these things you saw like project overview data set I think I add a link to the data set here I describe the Technologies uh then I said what you need to do in order to prepare so install deeral get the key uh install ppen and install the dependencies then this is how you run it first you need to configure the database um and then you actually do Docker compose up and it works uh what if you want to run it outside of Docker what if you just want to run it with Docker without compul all these instructions are here then also this is the issue I struggled with uh for some time like making sure that time stamps are correct so I added a few checks here so you can read about that too um again hopefully if you try to reproduce it it shouldn't be a problem for you it was specific to my uh environment and then uh how exactly you can use it so once the thing is running how you can interact with uh with applications so for that there is the C application then also what I did is instead of just typing they c i can randomly load a question from the ground prooof data set which is what uh which is also what this test.py is doing previously it was sending the same request over and over again but now I thought um it would be nicer if it just picks a random question and then sends it okay and then example with curl then I describ the code what exactly we have like in the F Fitness assistant folder uh outside of it like this test.py clip um then I talk about the interface the for the API then ingestion uh then there's a section on experiments so this is where we did our evolation so evolation for retrieval which is uh a basic approach and then tuning uh our parameters and then evaluating Rock flow in our case we because we don't have a uh like the how to say uh like the answer the ground Ruth answer or the gold standard answer right we only have um we only have uh like a piece of text or not even a roll from a CSV file uh the only L is Judge mric we can use is like okay here's a question here's an answer like is it actually relevant or not and uh yeah evaluation of two and then for monitoring I added a screenshot of graph f a dashboard and then uh I also describe what kind of um tiles are there panels and actually to describe that what I did is I just took the dashboard of Json file put it to chpd and ask hey can you describe me what is inside and this is how you set up grafana so here I describe the files inside the grafana folder and this is how you run them right um so this is uh this is the project um Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '3d012bc7417e2d1070fdf047d968ae7e'},\n",
       " {'vid_id': 'TW9M5VE8vpo',\n",
       "  'title': 'LLM Zoomcamp 7.6 - Summary and closing remarks',\n",
       "  'timecode': '09:30',\n",
       "  'text': \"one more thing so I also wanted to talk about how much money I spent right so in total I spent $27 out of which $23 C were spent on GPT 40 and 13 cents on GPT 40 mini which means that maybe we shouldn't use CH GPT 40 I use it only for this thing here I had to run it twice um for some reasons like when I run it once uh yeah I lost the results so I had to rerun it so one run of this thing on 200 records costed $1 which is expensive right compared to gbt for or mini which like I could have saved so much money by uh maybe instead of trying different models I could test it two prompts for example or three prompts in order to get two points for the r four evalation right and then I would have spent much less so if for you how much money you spent is a concern then you can just limit yourself to just GPT for Mini or maybe you just use Gro so then you won't have pay anything um I don't know what are the limits for Gro but anyways um yeah so I could have say I could have uh spent uh way less money if I um didn't use um gp4\",\n",
       "  'timecode_text': 'Costs',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=TW9M5VE8vpo&t=570s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.6 - Summary and closing remarks one more thing so I also wanted to talk about how much money I spent right so in total I spent $27 out of which $23 C were spent on GPT 40 and 13 cents on GPT 40 mini which means that maybe we shouldn't use CH GPT 40 I use it only for this thing here I had to run it twice um for some reasons like when I run it once uh yeah I lost the results so I had to rerun it so one run of this thing on 200 records costed $1 which is expensive right compared to gbt for or mini which like I could have saved so much money by uh maybe instead of trying different models I could test it two prompts for example or three prompts in order to get two points for the r four evalation right and then I would have spent much less so if for you how much money you spent is a concern then you can just limit yourself to just GPT for Mini or maybe you just use Gro so then you won't have pay anything um I don't know what are the limits for Gro but anyways um yeah so I could have say I could have uh spent uh way less money if I um didn't use um gp4 Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'd0acd76ca02c1fbebf831849928350c2'},\n",
       " {'vid_id': 'TW9M5VE8vpo',\n",
       "  'title': 'LLM Zoomcamp 7.6 - Summary and closing remarks',\n",
       "  'timecode': '11:00',\n",
       "  'text': \"yeah so that's it so I hope you enjoyed the course uh enjoyed the project I want to record uh one more video we will see so uh we'll have off office hours today if you're watching this in the future I highly recommend to check this office hours um you'll find them maybe I'll add a link somewhere to project so because today we're going to talk about projects and I will add a link to office hours uh where is our llm Zoom Camp I will add the link to this uh project MD file which hopefully will answer many of the questions you have even if you watch this in the future and I want to record one more video depending on what we cover in the uh in office hours of how you can approach other kinds of Text data or data in general cuz we saw how to uh in the course we saw how to do to deal with Q&A data set how to deal with CSV data set uh we can also think how to deal with other kinds of data set okay and one more thing before I\",\n",
       "  'timecode_text': 'Office hours',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=TW9M5VE8vpo&t=660s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.6 - Summary and closing remarks yeah so that's it so I hope you enjoyed the course uh enjoyed the project I want to record uh one more video we will see so uh we'll have off office hours today if you're watching this in the future I highly recommend to check this office hours um you'll find them maybe I'll add a link somewhere to project so because today we're going to talk about projects and I will add a link to office hours uh where is our llm Zoom Camp I will add the link to this uh project MD file which hopefully will answer many of the questions you have even if you watch this in the future and I want to record one more video depending on what we cover in the uh in office hours of how you can approach other kinds of Text data or data in general cuz we saw how to uh in the course we saw how to do to deal with Q&A data set how to deal with CSV data set uh we can also think how to deal with other kinds of data set okay and one more thing before I Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '26d6af784cb12d55952c91688ddc9ddf'},\n",
       " {'vid_id': 'TW9M5VE8vpo',\n",
       "  'title': 'LLM Zoomcamp 7.6 - Summary and closing remarks',\n",
       "  'timecode': '12:15',\n",
       "  'text': \"forgot I wanted to mention that um so here in this project it's kind of a toy project in a sense that we used an llm to generate a data set and then used this data set to feed to an llm which kind of um like if you think about this like wouldn't be we better off by just keeping the part of generating the data and directly asking the llm right so I I'm not sure if in this particular example if we talk about real practical applications this is the way to go maybe it's a good way to start but then the real value in rck is when a data set is unique and curated right so for example the notion page I showed you at the very beginning is created because it's not only generated content but also there is a lot of content that I assembled from different YouTube videos from different articles so there is more value there than just simply asking chpt to generate the data set and uh this is also very true for our FAQ data set that we used right uh and and in general we want to use rock where when uh the data is overwise maybe not as easily available to the llm um um without this extra data right um yeah so maybe this is just the last note so if you have a data set that is created and the value of rck is of course High uh but if you use a data set that is just generated with uh llm maybe sometimes you might not need a an llm or or rck you can just go directly ask an llm uh this is just a side note it does not mean you should not use generated data for your projects it's definitely okay to use generated data uh it's just later when you use um yeah there are valid cases when you need to use a generated data set sometimes maybe you can just directly call an l and it will be okay for you okay that's all and um see you soon and yeah that was fun bye\",\n",
       "  'timecode_text': 'Note on synthetic data generation',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=TW9M5VE8vpo&t=735s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.6 - Summary and closing remarks forgot I wanted to mention that um so here in this project it's kind of a toy project in a sense that we used an llm to generate a data set and then used this data set to feed to an llm which kind of um like if you think about this like wouldn't be we better off by just keeping the part of generating the data and directly asking the llm right so I I'm not sure if in this particular example if we talk about real practical applications this is the way to go maybe it's a good way to start but then the real value in rck is when a data set is unique and curated right so for example the notion page I showed you at the very beginning is created because it's not only generated content but also there is a lot of content that I assembled from different YouTube videos from different articles so there is more value there than just simply asking chpt to generate the data set and uh this is also very true for our FAQ data set that we used right uh and and in general we want to use rock where when uh the data is overwise maybe not as easily available to the llm um um without this extra data right um yeah so maybe this is just the last note so if you have a data set that is created and the value of rck is of course High uh but if you use a data set that is just generated with uh llm maybe sometimes you might not need a an llm or or rck you can just go directly ask an llm uh this is just a side note it does not mean you should not use generated data for your projects it's definitely okay to use generated data uh it's just later when you use um yeah there are valid cases when you need to use a generated data set sometimes maybe you can just directly call an l and it will be okay for you okay that's all and um see you soon and yeah that was fun bye Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '99bf73b9f22fd56cb1a387199652bad2'},\n",
       " {'vid_id': 'tyBRP_WewXA',\n",
       "  'title': 'LLM Zoomcamp 7.7 - Chunking for longer texts',\n",
       "  'timecode': '00:00',\n",
       "  'text': 'hi everyone I decided to record another video So based on the questions from uh from this office hours yesterday some of you are interested in knowing what to do if you have an article or a transcript or something else I did some um I shared some ideas I gave some explanation of what you can possibly do uh in the office hours but here I want to spend a little bit more time uh talking about that',\n",
       "  'timecode_text': 'Introduction',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=tyBRP_WewXA&t=0s',\n",
       "  'text_vector': 'LLM Zoomcamp 7.7 - Chunking for longer texts hi everyone I decided to record another video So based on the questions from uh from this office hours yesterday some of you are interested in knowing what to do if you have an article or a transcript or something else I did some um I shared some ideas I gave some explanation of what you can possibly do uh in the office hours but here I want to spend a little bit more time uh talking about that Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'id': '56e640be2f8a9f1d0c9c2f7f9ea74eff'},\n",
       " {'vid_id': 'tyBRP_WewXA',\n",
       "  'title': 'LLM Zoomcamp 7.7 - Chunking for longer texts',\n",
       "  'timecode': '00:30',\n",
       "  'text': \"and what I did is I took the office hours from yesterday and I took the transcript from there you can use YouTube transcript actually um the like this tool that we talked about I actually tried using it uh so it's called YouTube transcript API I installed it here transcript API transcript API right and then I uh used it for this video uh we can also try using it for um for this office hours this one then I think it's pretty straightforward to use it so it gives you um like this transcript that I copy pasted right so this is how you can programmatically get the transcript I simply copied uh it and then paste it to chpt this is just simpler when I try building a prompt I usually first try chpt play with this and then put it to like open AI API right so here I use GPT for all perhaps you want to do it with uh mini um and the prompt I gave was this your podcast editor highly skilled in data science your task is to turn a podcast St died into readable text while preserving as much of the original information as possible and then I give some instructions of what exactly it needs to do right and then I provide the format I'll share this link with you later then and this uh here I just copy paste this from here and then if I scroll all the way down then it did exactly what I asked right um so it splited into um logical blocks using this um and then like there is no specific reason why I decided to choose this over something else um but I usually like I could already ask it to give me uh names uh like title names section names for each of the uh logical blocks usually in my experience it's better if you first edit this and then you do it as a separate thing you ask it to give it a name so this is what I did here so I say that you're professional editor uh blah blah blah your task is to give each block a name and rearrange blocks if better Arrangement is possible so basically I take this thing from here and give it and then say that the output should be a parable Json and then I say how it should look like and then I just copy this thing from the previous uh results and then it gives me back some um some Json and then I put this Json to uh like some online uh format inv validator so this is how it looks like so it's totally parsible now you can just take and use this already like for example for each block you already have a name uh so then you can Index this block and this could be like a separate document right so one first document second document I wouldn't call them documents uh maybe chunks blocks so block one chunk one chunk two chunk three and so on right so then you have a bunch of um blocks here and um now you can do what we did um in the course um namely you can index it you can use it for generating questions um and so on right and what do I have here yeah um\",\n",
       "  'timecode_text': 'Dealing with transcripts',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=tyBRP_WewXA&t=30s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.7 - Chunking for longer texts and what I did is I took the office hours from yesterday and I took the transcript from there you can use YouTube transcript actually um the like this tool that we talked about I actually tried using it uh so it's called YouTube transcript API I installed it here transcript API transcript API right and then I uh used it for this video uh we can also try using it for um for this office hours this one then I think it's pretty straightforward to use it so it gives you um like this transcript that I copy pasted right so this is how you can programmatically get the transcript I simply copied uh it and then paste it to chpt this is just simpler when I try building a prompt I usually first try chpt play with this and then put it to like open AI API right so here I use GPT for all perhaps you want to do it with uh mini um and the prompt I gave was this your podcast editor highly skilled in data science your task is to turn a podcast St died into readable text while preserving as much of the original information as possible and then I give some instructions of what exactly it needs to do right and then I provide the format I'll share this link with you later then and this uh here I just copy paste this from here and then if I scroll all the way down then it did exactly what I asked right um so it splited into um logical blocks using this um and then like there is no specific reason why I decided to choose this over something else um but I usually like I could already ask it to give me uh names uh like title names section names for each of the uh logical blocks usually in my experience it's better if you first edit this and then you do it as a separate thing you ask it to give it a name so this is what I did here so I say that you're professional editor uh blah blah blah your task is to give each block a name and rearrange blocks if better Arrangement is possible so basically I take this thing from here and give it and then say that the output should be a parable Json and then I say how it should look like and then I just copy this thing from the previous uh results and then it gives me back some um some Json and then I put this Json to uh like some online uh format inv validator so this is how it looks like so it's totally parsible now you can just take and use this already like for example for each block you already have a name uh so then you can Index this block and this could be like a separate document right so one first document second document I wouldn't call them documents uh maybe chunks blocks so block one chunk one chunk two chunk three and so on right so then you have a bunch of um blocks here and um now you can do what we did um in the course um namely you can index it you can use it for generating questions um and so on right and what do I have here yeah um Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'e58ce2dbcd5f6c2befde596b53c58548'},\n",
       " {'vid_id': 'tyBRP_WewXA',\n",
       "  'title': 'LLM Zoomcamp 7.7 - Chunking for longer texts',\n",
       "  'timecode': '04:35',\n",
       "  'text': \"that's one approach so I call this approach uh log logical chunking or smart chunking here you use a an llm to actually find logical blocks and split the text into these blocks right so this is exactly what we have here and this is our results so this is one block this is another block so and um sentence here are related to each other another approach would be to just you take a piece of text and uh I will just think how to so let's pretend we don't have these things here right so what we can do is we can just take this block then take this block of text oops like that's too much take this block of text then uh take this block of text and each of them will be a chunk right and then um like it's SL smart you just kind of have a sliding window through your document and each um each window would be a separate chunk right and then this is what you can do to this is what you can index put in your database um this is how you do it so this is the code I wrote um this is the promp I wrot wrote I have an article I want to chunk the uh chunk the code I meant the text uh so I can use it for rock write a function for doing that uh here are some things I want it to do and then it gave me a reasonable code for that right and then this is something you can later use for uh for indexing right of course you need to play with uh the chunk size overlap size like how exactly how much uh there is overlap between each window each chunk um and then this is how you can turn an article into a bunch of chunks documents that you can index and then when you do retrieval instead of retrieving and this is something I\",\n",
       "  'timecode_text': 'Simple chunking',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=tyBRP_WewXA&t=275s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.7 - Chunking for longer texts that's one approach so I call this approach uh log logical chunking or smart chunking here you use a an llm to actually find logical blocks and split the text into these blocks right so this is exactly what we have here and this is our results so this is one block this is another block so and um sentence here are related to each other another approach would be to just you take a piece of text and uh I will just think how to so let's pretend we don't have these things here right so what we can do is we can just take this block then take this block of text oops like that's too much take this block of text then uh take this block of text and each of them will be a chunk right and then um like it's SL smart you just kind of have a sliding window through your document and each um each window would be a separate chunk right and then this is what you can do to this is what you can index put in your database um this is how you do it so this is the code I wrote um this is the promp I wrot wrote I have an article I want to chunk the uh chunk the code I meant the text uh so I can use it for rock write a function for doing that uh here are some things I want it to do and then it gave me a reasonable code for that right and then this is something you can later use for uh for indexing right of course you need to play with uh the chunk size overlap size like how exactly how much uh there is overlap between each window each chunk um and then this is how you can turn an article into a bunch of chunks documents that you can index and then when you do retrieval instead of retrieving and this is something I Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'ac1ab92793ee636ce92d5268e8249fc0'},\n",
       " {'vid_id': 'tyBRP_WewXA',\n",
       "  'title': 'LLM Zoomcamp 7.7 - Chunking for longer texts',\n",
       "  'timecode': '06:45',\n",
       "  'text': \"didn't talk about but like um why do we actually care about chunks right so when we do rock we don't want to let's say you have uh 1,000 articles from Wikipedia and then you don't want to put the entire article uh to your uh prompt CU it's too long right so you want to find uh parts of the article that are relevant to your query and then only use them uh in the prompt right so that's why we do chunkin and like when it comes to a YouTube video like that there are many things we discuss like maybe 10 different topics so most of them will not be relevant maybe one or two will be relevant to your specific query that is why it doesn't make sense to include the entire transcript in the prompt we only want to find the relevant part that's why we need chunking um so this wasn't uh the case for our FAQ document cuz uh it was already super structured but usually texts aren't as structured as our fq document that's why we need to kind of add some structure there either uh by using n llm to introduce the structure or by just Chunk in it and then using each chunk as a like um yeah as a document okay so this is how you do this I don't want to um like and hold you through this process I think you'll figure it out um and the last thing I want to talk\",\n",
       "  'timecode_text': 'Why do we need chunking?',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=tyBRP_WewXA&t=405s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.7 - Chunking for longer texts didn't talk about but like um why do we actually care about chunks right so when we do rock we don't want to let's say you have uh 1,000 articles from Wikipedia and then you don't want to put the entire article uh to your uh prompt CU it's too long right so you want to find uh parts of the article that are relevant to your query and then only use them uh in the prompt right so that's why we do chunkin and like when it comes to a YouTube video like that there are many things we discuss like maybe 10 different topics so most of them will not be relevant maybe one or two will be relevant to your specific query that is why it doesn't make sense to include the entire transcript in the prompt we only want to find the relevant part that's why we need chunking um so this wasn't uh the case for our FAQ document cuz uh it was already super structured but usually texts aren't as structured as our fq document that's why we need to kind of add some structure there either uh by using n llm to introduce the structure or by just Chunk in it and then using each chunk as a like um yeah as a document okay so this is how you do this I don't want to um like and hold you through this process I think you'll figure it out um and the last thing I want to talk Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'ff6ce1ce9972ba461d8826951e08b71e'},\n",
       " {'vid_id': 'tyBRP_WewXA',\n",
       "  'title': 'LLM Zoomcamp 7.7 - Chunking for longer texts',\n",
       "  'timecode': '08:20',\n",
       "  'text': \"about is uh like how would you approach different um different cases so let's say there's uh one case that I talked about you have multiple articles could be like let's say 100 or 1,000 articles from Wikipedia maybe it's your own blog maybe I don't know it's your notion Pages could be anything right uh so then each article in your data set in your knowledge base will um have its own document ID right uh so this is the document ID that uh this is the document that it contains some information about something right then you take each document each article and you chunk it using logical chunking with ANM or just using simple chunking uh that I showed with python code and then for each chunk you give it a unique chunk ID so there is a document ID for this document and then each chunk each block of text within this document will have a separate ID too right so it means that that you can identify a chunk within the document uh and you can identify the the document itself right and then you do this for all the documents and then you index chunks so you put to your database chunks not the entire documents but you for each chunk you put you index both document ID and chunk ID so it could be like if you uh use uh let's say elastic search could be Doc ID then there is uh some ID right the chunk ID could be uh for example document ID one right and then text here's like uh actual text of the junk and then you have uh many many documents like that right so you put them to your index be elastic search or whatever and then you can use these chunks to generate uh your ground proof data set right or maybe you somehow acquire the ground prooof data set with questions in a different way right um so then from each chunk like let's say you we use an LM from each Chunk we can generate a question five questions like we did in the in the course and here you have your ground through data set and when you evaluate retrieval you actually can and should uh evaluate two things let's say if we evaluate heat rate then you see uh okay for this chunk is for this document was I able to retrieve this document or not right so that's one heat rate we can call it document heat rate and then another heat rate could be chunk hit rate was I able to retrieve the chunk the specific chunk I was looking for uh so what we did in the course was more like chunk heat rate right uh but you can also take a look at documents ID could also be quite interesting uh to see if you accidentally uh also put other documents that uh either maybe should they maybe shouldn't be there right so this is when it comes to Evol and retrieval and of course you can do this for heat rate for Mr for other retrial metrics too and then you can do the same with Rock and with evaluating rock using something llm like llm as a judge is interesting uh because uh if we don't have the FAQ data set um so we don't have the reference uh question we only have or the reference answer we only have the answer that we generated we can ask an llm if this thing answers the question right and then we can tune uh our chunin approach uh by using um by looking at the the the proportion fraction of relevant answers right um so then let's say if we see that simple chunking works and logical smarter chunking does not add much does not improve much our uh performance then we can just do simple chunking right uh yeah um so that's how you can approach uh the case when you have multiple\",\n",
       "  'timecode_text': 'Case 1: multiple articles',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=tyBRP_WewXA&t=500s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.7 - Chunking for longer texts about is uh like how would you approach different um different cases so let's say there's uh one case that I talked about you have multiple articles could be like let's say 100 or 1,000 articles from Wikipedia maybe it's your own blog maybe I don't know it's your notion Pages could be anything right uh so then each article in your data set in your knowledge base will um have its own document ID right uh so this is the document ID that uh this is the document that it contains some information about something right then you take each document each article and you chunk it using logical chunking with ANM or just using simple chunking uh that I showed with python code and then for each chunk you give it a unique chunk ID so there is a document ID for this document and then each chunk each block of text within this document will have a separate ID too right so it means that that you can identify a chunk within the document uh and you can identify the the document itself right and then you do this for all the documents and then you index chunks so you put to your database chunks not the entire documents but you for each chunk you put you index both document ID and chunk ID so it could be like if you uh use uh let's say elastic search could be Doc ID then there is uh some ID right the chunk ID could be uh for example document ID one right and then text here's like uh actual text of the junk and then you have uh many many documents like that right so you put them to your index be elastic search or whatever and then you can use these chunks to generate uh your ground proof data set right or maybe you somehow acquire the ground prooof data set with questions in a different way right um so then from each chunk like let's say you we use an LM from each Chunk we can generate a question five questions like we did in the in the course and here you have your ground through data set and when you evaluate retrieval you actually can and should uh evaluate two things let's say if we evaluate heat rate then you see uh okay for this chunk is for this document was I able to retrieve this document or not right so that's one heat rate we can call it document heat rate and then another heat rate could be chunk hit rate was I able to retrieve the chunk the specific chunk I was looking for uh so what we did in the course was more like chunk heat rate right uh but you can also take a look at documents ID could also be quite interesting uh to see if you accidentally uh also put other documents that uh either maybe should they maybe shouldn't be there right so this is when it comes to Evol and retrieval and of course you can do this for heat rate for Mr for other retrial metrics too and then you can do the same with Rock and with evaluating rock using something llm like llm as a judge is interesting uh because uh if we don't have the FAQ data set um so we don't have the reference uh question we only have or the reference answer we only have the answer that we generated we can ask an llm if this thing answers the question right and then we can tune uh our chunin approach uh by using um by looking at the the the proportion fraction of relevant answers right um so then let's say if we see that simple chunking works and logical smarter chunking does not add much does not improve much our uh performance then we can just do simple chunking right uh yeah um so that's how you can approach uh the case when you have multiple Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '944fc08a46c2084af7b7caf72461a15d'},\n",
       " {'vid_id': 'tyBRP_WewXA',\n",
       "  'title': 'LLM Zoomcamp 7.7 - Chunking for longer texts',\n",
       "  'timecode': '12:50',\n",
       "  'text': \"articles if you just have one article so it could be for example you buil a system for um I don't know YouTube videos so you have an interface interface just ask you asks you for YouTube uh video ID you give uh this uh the ID then it indexes it gets the transcript it indexes the content and then you can start talking to this uh thing so could be an interesting project too uh right so here what you can do is you can again just take it chunk it and then put each Chunk in the database you probably want to go with an inmemory database cuz uh it's just one article right and then how many chunks you can have there I don't know 10 12 20 uh not so many or like even if it's 50 right like can memory database will just work fine uh yeah so maybe you don't like I said probably it's better to use logical chunin I don't know maybe it's actually not better so I'll remove that um so yeah you can just use small sentences as your chunks not sentences like groups of sentences right two three four sentences um like you can experiment with that right uh and the way you experiment with that is you can for example take 10 15 different YouTube videos and then uh play with uh do it evaluate your uh approach your chunkin approach and your entire ra approach on all 10 or all 20 or all 50 right not just on one document but on many and then when you kind of look at the performance across many documents you see that this chunkin approach works better than that chunkin approach so evaluation becomes a little bit more difficult here CU you want to do it just for you want to do it for multiple things not just one right so this is how I would personally approach that right so there could be other approaches other way to approach proo it this is how I would do\",\n",
       "  'timecode_text': 'Case 2: one article',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=tyBRP_WewXA&t=770s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.7 - Chunking for longer texts articles if you just have one article so it could be for example you buil a system for um I don't know YouTube videos so you have an interface interface just ask you asks you for YouTube uh video ID you give uh this uh the ID then it indexes it gets the transcript it indexes the content and then you can start talking to this uh thing so could be an interesting project too uh right so here what you can do is you can again just take it chunk it and then put each Chunk in the database you probably want to go with an inmemory database cuz uh it's just one article right and then how many chunks you can have there I don't know 10 12 20 uh not so many or like even if it's 50 right like can memory database will just work fine uh yeah so maybe you don't like I said probably it's better to use logical chunin I don't know maybe it's actually not better so I'll remove that um so yeah you can just use small sentences as your chunks not sentences like groups of sentences right two three four sentences um like you can experiment with that right uh and the way you experiment with that is you can for example take 10 15 different YouTube videos and then uh play with uh do it evaluate your uh approach your chunkin approach and your entire ra approach on all 10 or all 20 or all 50 right not just on one document but on many and then when you kind of look at the performance across many documents you see that this chunkin approach works better than that chunkin approach so evaluation becomes a little bit more difficult here CU you want to do it just for you want to do it for multiple things not just one right so this is how I would personally approach that right so there could be other approaches other way to approach proo it this is how I would do Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '28c62d45a04cf123a054b8cc3da95c48'},\n",
       " {'vid_id': 'tyBRP_WewXA',\n",
       "  'title': 'LLM Zoomcamp 7.7 - Chunking for longer texts',\n",
       "  'timecode': '15:00',\n",
       "  'text': \"this um another case let's say you have a book and this book has I don't know 20 chapters and this chapter has uh sections right so this is a very long form content and what you can do is you can just say Okay each section in this book is a separate article separate document and then you just you're kind of back to this one right um or let's say if it's it's not a technical book and there are no sections only chapters you can take each chapter would be a document and then you can think of different chunking strategies there and then again like you somehow needed you somehow need to make a guided decision at the end and one of the possible approaches to see that this appro one of the possible ways of seeing which approach works better is to again use llm as a judge to see that this rack function works better than this rack function okay I have just two more uh possible\",\n",
       "  'timecode_text': 'Case 3: book or other long content',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=tyBRP_WewXA&t=900s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.7 - Chunking for longer texts this um another case let's say you have a book and this book has I don't know 20 chapters and this chapter has uh sections right so this is a very long form content and what you can do is you can just say Okay each section in this book is a separate article separate document and then you just you're kind of back to this one right um or let's say if it's it's not a technical book and there are no sections only chapters you can take each chapter would be a document and then you can think of different chunking strategies there and then again like you somehow needed you somehow need to make a guided decision at the end and one of the possible approaches to see that this appro one of the possible ways of seeing which approach works better is to again use llm as a judge to see that this rack function works better than this rack function okay I have just two more uh possible Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '814a529263f7127445862f97d7527d5b'},\n",
       " {'vid_id': 'tyBRP_WewXA',\n",
       "  'title': 'LLM Zoomcamp 7.7 - Chunking for longer texts',\n",
       "  'timecode': '16:00',\n",
       "  'text': \"project ideas uh and how to approach them again this is how I would approach them it does not mean that you have to do it in the same way but maybe it gives you some ideas um so let's say you have a bunch of images then each image what you can do is this scrap each image using uh GPT for all mini uh you can also use something like clip to get uh to turn it into vectors right and then uh each index each image would be a separate document I'm still thinking like what exactly what kind of rack application could it be there uh I mean I can think of uh search applications like let's say you have holidays holiday pictures from your holidays and then you say hey find me a picture when I'm on a beach with a dog right and then it will find you something but that's not track it's just retrieval right so so somehow uh yeah probably you can think of something but this is like if I needed to index images I would just describe these images uh with GPT for and then index that I will also create embeddings with clip I will index that and then somehow use it for\",\n",
       "  'timecode_text': 'Case 4: images',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=tyBRP_WewXA&t=960s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.7 - Chunking for longer texts project ideas uh and how to approach them again this is how I would approach them it does not mean that you have to do it in the same way but maybe it gives you some ideas um so let's say you have a bunch of images then each image what you can do is this scrap each image using uh GPT for all mini uh you can also use something like clip to get uh to turn it into vectors right and then uh each index each image would be a separate document I'm still thinking like what exactly what kind of rack application could it be there uh I mean I can think of uh search applications like let's say you have holidays holiday pictures from your holidays and then you say hey find me a picture when I'm on a beach with a dog right and then it will find you something but that's not track it's just retrieval right so so somehow uh yeah probably you can think of something but this is like if I needed to index images I would just describe these images uh with GPT for and then index that I will also create embeddings with clip I will index that and then somehow use it for Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': '75a06a9a28b94e65e9700374d4d3b105'},\n",
       " {'vid_id': 'tyBRP_WewXA',\n",
       "  'title': 'LLM Zoomcamp 7.7 - Chunking for longer texts',\n",
       "  'timecode': '17:15',\n",
       "  'text': \"searching and last use case I wanted to talk about is slides so let's say you have uh I don't know all your slides slide decks from your studies from University and you still haven't deleted them cuz you're hoping that one day they will become useful uh that's what I do and uh what you can do is you can take each slide deck and uh for each slide each slide would be a chunk then you can um ask GPT and llm some llm to describe it uh and then the description of that would be your chunk right uh and then you index these descriptions and then the document would be uh for example the slide deck and you do this across all slide decks and you're kind of back to this case again okay I think this is actually a very fun case but anyways so these are some ideas and ways to uh approach them I hope you find it useful um and yeah let me know what you think about this and let us know in slack what kind of projects you want to work on so see you soon and have fun\",\n",
       "  'timecode_text': 'Case 5: slides',\n",
       "  'description': 'Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp',\n",
       "  'link': 'https://www.youtube.com/watch?v=tyBRP_WewXA&t=1035s',\n",
       "  'text_vector': \"LLM Zoomcamp 7.7 - Chunking for longer texts searching and last use case I wanted to talk about is slides so let's say you have uh I don't know all your slides slide decks from your studies from University and you still haven't deleted them cuz you're hoping that one day they will become useful uh that's what I do and uh what you can do is you can take each slide deck and uh for each slide each slide would be a chunk then you can um ask GPT and llm some llm to describe it uh and then the description of that would be your chunk right uh and then you index these descriptions and then the document would be uh for example the slide deck and you do this across all slide decks and you're kind of back to this case again okay I think this is actually a very fun case but anyways so these are some ideas and ways to uh approach them I hope you find it useful um and yeah let me know what you think about this and let us know in slack what kind of projects you want to work on so see you soon and have fun Free LLM course: https://github.com/DataTalksClub/llm-zoomcamp\",\n",
       "  'id': 'f40a912584f734daae5d271764cc9d7e'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_ids(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vid_id': 'Q75JgLEXMsM',\n",
       " 'title': 'LLM Zoomcamp 1.1 - Introduction to LLM and RAG',\n",
       " 'timecode': '00:00',\n",
       " 'text': \"hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so\",\n",
       " 'timecode_text': 'Introduction to LLM Zoomcamp',\n",
       " 'description': \"Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       " 'link': 'https://www.youtube.com/watch?v=Q75JgLEXMsM&t=0s',\n",
       " 'text_vector': \"LLM Zoomcamp 1.1 - Introduction to LLM and RAG hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       " 'id': '26c14fe6727a6d5697b4853bcaa6e984'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gary/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-09-15 16:43:21.139658: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-15 16:43:21.281931: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-15 16:43:21.282989: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-15 16:43:21.473678: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-15 16:43:22.452208: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for transcript in transcripts:\n",
    "    transcript['search_vector'] = model.encode(transcript['text_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vid_id': 'Q75JgLEXMsM',\n",
       " 'title': 'LLM Zoomcamp 1.1 - Introduction to LLM and RAG',\n",
       " 'timecode': '00:00',\n",
       " 'text': \"hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so\",\n",
       " 'timecode_text': 'Introduction to LLM Zoomcamp',\n",
       " 'description': \"Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       " 'link': 'https://www.youtube.com/watch?v=Q75JgLEXMsM&t=0s',\n",
       " 'text_vector': \"LLM Zoomcamp 1.1 - Introduction to LLM and RAG hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       " 'id': '26c14fe6727a6d5697b4853bcaa6e984',\n",
       " 'search_vector': array([ 9.75093711e-03, -4.36954796e-02, -1.91113129e-02,  4.47372086e-02,\n",
       "        -2.57605221e-02,  6.02469500e-03,  2.31656358e-02,  2.08821669e-02,\n",
       "        -1.77573431e-02, -1.17108142e-02,  1.00698406e-02, -2.32177842e-02,\n",
       "        -7.62389709e-06,  2.02769712e-02,  2.16188878e-02, -1.95883214e-02,\n",
       "         2.39299480e-02, -1.04031740e-02, -8.20981339e-02, -3.20302993e-02,\n",
       "         1.42401422e-03, -1.46415653e-02,  4.73713782e-03, -2.13004537e-02,\n",
       "        -2.89513953e-02,  9.90438089e-03, -4.71954755e-02,  6.41517118e-02,\n",
       "        -4.07053530e-03, -7.29762614e-02,  1.60810519e-02,  2.99483575e-02,\n",
       "         2.14081276e-02,  1.41311660e-02,  2.38672351e-06, -5.85560277e-02,\n",
       "        -6.65564090e-02, -4.46521398e-03, -5.17948456e-02,  7.38953501e-02,\n",
       "         1.05440049e-02,  7.19093531e-02,  1.49130039e-02,  7.12755695e-03,\n",
       "        -2.96716876e-02,  5.06177545e-03, -9.33260284e-03,  7.60839181e-03,\n",
       "         4.12710048e-02,  2.76684649e-02,  4.22476092e-03, -4.09117155e-02,\n",
       "        -3.21104117e-02, -2.88040238e-03,  1.33954123e-01, -4.81434725e-02,\n",
       "         1.95709448e-02,  8.40216279e-02,  1.18341081e-01,  2.01627295e-02,\n",
       "        -2.50657797e-02,  1.36027047e-02,  6.28455449e-03, -3.35648190e-03,\n",
       "         8.88685510e-02,  1.79009885e-02,  3.01329363e-02, -2.97103692e-02,\n",
       "        -1.19673843e-02,  4.08042362e-03,  7.47368485e-02, -4.83751413e-04,\n",
       "         1.16302827e-02,  6.26691524e-03, -1.20206857e-02,  6.39023408e-02,\n",
       "        -7.67093301e-02, -5.38168941e-03, -2.20486484e-02, -2.39985809e-02,\n",
       "        -6.04965240e-02, -3.64364125e-02, -5.28719202e-02, -3.05049010e-02,\n",
       "        -2.18834877e-02,  4.33142669e-02,  8.60085804e-03, -2.57899407e-02,\n",
       "        -1.59509033e-02, -3.24856769e-03,  9.51617584e-03, -2.90720798e-02,\n",
       "         8.22157115e-02,  8.40183198e-02, -1.72523167e-02, -3.46701331e-02,\n",
       "         2.01753080e-02, -8.41790624e-03,  5.62356710e-02, -3.05086598e-02,\n",
       "         9.23676044e-03,  8.02704971e-03,  2.04007747e-03,  3.28528695e-02,\n",
       "        -1.47794522e-02,  7.17918854e-03, -1.25394147e-02,  2.14182083e-02,\n",
       "        -5.55660352e-02,  5.87460212e-02, -2.65815873e-02,  5.84482215e-04,\n",
       "        -3.43151912e-02,  5.55614568e-02,  1.47985038e-03,  1.75387058e-02,\n",
       "        -7.30705261e-03,  5.02808811e-03,  3.43728028e-02,  4.86884499e-03,\n",
       "        -2.56450307e-02,  5.91285201e-03, -8.56557861e-03, -3.77120823e-03,\n",
       "        -1.73216537e-02, -1.71233248e-02, -3.08624133e-02,  1.08868852e-02,\n",
       "        -1.60909109e-02, -2.02068463e-02, -7.53815193e-03,  9.58135352e-03,\n",
       "         3.20723541e-02,  2.65706126e-02,  5.05993403e-02,  8.47010165e-02,\n",
       "        -2.26979181e-02,  1.01543451e-02, -1.08539499e-01, -5.82683980e-02,\n",
       "         2.14173589e-02, -8.75644200e-03, -1.14542283e-02, -5.95920533e-02,\n",
       "        -4.83250851e-03,  1.38124474e-03, -2.25460529e-02,  1.01460285e-01,\n",
       "         1.97408088e-02,  4.86400463e-02, -5.17393313e-02,  5.26909456e-02,\n",
       "         3.97253362e-03,  1.28725860e-02,  8.02863613e-02,  3.11382078e-02,\n",
       "         6.99697956e-02, -2.32235603e-02, -4.01025601e-02,  4.30080034e-02,\n",
       "         1.14097865e-02, -1.57354847e-02,  4.34961952e-02, -4.36644768e-03,\n",
       "        -3.06366626e-02, -2.03828551e-02, -1.01968106e-02,  1.98004115e-02,\n",
       "         1.45573113e-02, -1.59607567e-02, -4.22172397e-02,  2.06299853e-02,\n",
       "        -1.08749811e-02,  6.02311343e-02, -7.81371165e-03,  1.16070040e-01,\n",
       "         2.82470528e-02, -3.47425453e-02, -4.21522893e-02,  4.85307584e-03,\n",
       "        -1.18969986e-02, -1.41881881e-02,  1.01580387e-02,  5.01852930e-02,\n",
       "        -1.29594700e-02, -1.32535277e-02, -3.40256207e-02,  4.36619893e-02,\n",
       "        -4.34722379e-02, -5.27738333e-02, -6.22279830e-02,  4.36176583e-02,\n",
       "        -5.23457490e-02,  2.65441872e-02,  5.24680130e-03, -3.05625629e-02,\n",
       "        -1.10312430e-04,  2.04903143e-03,  2.05026590e-03, -7.22819269e-02,\n",
       "         3.11494363e-03, -6.75300043e-03,  1.00339890e-01, -5.08385040e-02,\n",
       "         2.33736169e-02, -2.92021055e-02, -6.84510022e-02, -1.49801252e-02,\n",
       "         1.19826011e-02, -2.35612076e-02,  7.61492997e-02, -6.69529894e-03,\n",
       "        -4.92492467e-02, -3.35182846e-02, -2.21654922e-02,  3.60039100e-02,\n",
       "        -4.63595241e-03, -3.04945968e-02, -3.08322627e-02, -2.35310961e-02,\n",
       "        -1.06986072e-02, -1.64324790e-02, -4.60192896e-02,  3.04479580e-02,\n",
       "        -5.54055860e-03, -7.98552821e-04, -9.72484332e-03,  2.58037020e-02,\n",
       "        -5.55492565e-03,  3.43144722e-02,  3.39019150e-02,  2.53889099e-04,\n",
       "         5.65842502e-02,  5.96474949e-03,  2.29835771e-02, -3.79251651e-02,\n",
       "         4.44565378e-02,  1.54065555e-02,  3.81356627e-02, -5.90820014e-02,\n",
       "         1.91310261e-04, -1.27599537e-02,  2.23977445e-03, -1.44775910e-02,\n",
       "        -2.87409853e-02, -6.71413541e-02, -3.15910429e-02, -2.12151539e-02,\n",
       "         3.02477106e-02,  4.13952395e-03, -3.42978723e-02, -4.21499871e-02,\n",
       "         7.21305190e-03,  3.55947502e-02,  2.52664890e-02, -1.19401067e-02,\n",
       "        -2.77683102e-02,  5.80189414e-02, -1.37367342e-02,  2.86110230e-02,\n",
       "         2.70326827e-02, -6.29201531e-02, -3.79967168e-02, -8.62223469e-03,\n",
       "         3.31223980e-02, -4.32162322e-02,  2.76545435e-02, -1.90557782e-02,\n",
       "        -9.05138720e-03,  1.39906835e-02,  9.72291164e-04,  1.97729152e-02,\n",
       "        -2.06975117e-02, -1.79811791e-02, -2.25419328e-02, -6.18451163e-02,\n",
       "         2.59880293e-02,  3.97774614e-02, -4.52738889e-02,  3.50779258e-02,\n",
       "         9.16509982e-03, -3.59999202e-02, -1.34138344e-02,  2.97909975e-03,\n",
       "        -1.20940926e-02,  2.00457070e-02,  8.44897553e-02, -6.64595291e-02,\n",
       "        -5.94223626e-02,  2.10807957e-02, -2.99747288e-02, -2.61640251e-02,\n",
       "         1.81912296e-02, -2.62719188e-02, -1.83165129e-02, -6.32231534e-02,\n",
       "         3.00887171e-02,  6.74408898e-02,  2.32613403e-02,  3.55837494e-02,\n",
       "         2.55264118e-02, -1.33768488e-02, -5.33963926e-02,  8.84183869e-03,\n",
       "        -3.73149551e-02, -2.35782862e-02,  2.16025747e-02, -4.41865288e-02,\n",
       "         8.61236732e-03, -4.03841622e-02, -2.59078993e-03,  1.39045389e-02,\n",
       "         4.25052047e-02, -1.35573614e-02,  7.91300554e-03, -1.08248014e-02,\n",
       "        -2.96281781e-02, -1.66585799e-02, -1.29619623e-02,  6.43998981e-02,\n",
       "         2.67806631e-02, -5.30934939e-03,  1.02159036e-02, -3.99511401e-03,\n",
       "        -2.35007200e-02, -4.84335236e-02, -3.98589671e-02, -9.04849079e-03,\n",
       "        -1.02299321e-02, -9.31453891e-03,  3.51718143e-02, -2.92491969e-02,\n",
       "        -3.05142663e-02, -1.13810590e-02, -4.15724982e-03, -1.23917405e-03,\n",
       "        -3.13288793e-02, -8.72036535e-03,  5.71647985e-03,  1.87773444e-02,\n",
       "        -2.24011634e-02, -2.24691480e-02, -5.01788817e-02,  4.92405742e-02,\n",
       "         5.04708290e-02, -2.54139695e-02, -1.90574173e-02,  1.17367152e-02,\n",
       "        -1.91296022e-02,  1.43280402e-02,  7.60950819e-02, -2.50781402e-02,\n",
       "        -3.24790254e-02, -2.29902249e-02, -6.48397300e-03,  7.38845915e-02,\n",
       "        -2.14841347e-02,  3.30741182e-02,  3.20862718e-02, -4.77564745e-02,\n",
       "        -2.19834726e-02,  6.19955808e-02,  3.02643962e-02,  1.07995206e-02,\n",
       "         1.37897385e-02,  3.17747258e-02,  9.04425979e-02, -4.27771583e-02,\n",
       "         3.05109788e-02,  3.30227874e-02,  5.57300076e-02, -1.48208961e-02,\n",
       "        -5.85407838e-02,  1.39959315e-02,  2.27390453e-02, -6.29207864e-02,\n",
       "         5.82109066e-03, -2.47999821e-02, -5.38934879e-02, -2.17734976e-03,\n",
       "         6.16873540e-02, -5.64959869e-02,  6.33347109e-02,  7.81511236e-03,\n",
       "         5.32565415e-02, -1.67607740e-02, -6.95153186e-03, -1.91641208e-02,\n",
       "        -7.83111155e-02,  9.65636224e-02,  6.48570899e-03, -1.45430975e-02,\n",
       "        -7.05136806e-02, -1.46694826e-02, -2.13853829e-02,  1.69369020e-02,\n",
       "        -4.31028716e-02,  1.89920962e-02, -6.98663900e-03,  1.71042979e-02,\n",
       "         3.58120352e-02,  1.84812937e-02,  2.33328678e-02,  2.08182745e-02,\n",
       "        -7.35740224e-03,  1.95361581e-02, -1.37251290e-03, -1.49271376e-02,\n",
       "        -2.94508040e-02, -8.15675706e-02,  6.61628321e-02,  1.84926577e-02,\n",
       "         8.82118847e-03, -4.31899540e-02,  1.09853623e-04, -5.60047338e-03,\n",
       "         2.27928888e-02, -4.44262102e-03,  7.18058273e-02, -3.97056574e-03,\n",
       "        -6.41136197e-03,  3.16700414e-02,  3.97766419e-02, -3.54391523e-02,\n",
       "         1.51527189e-02, -4.51018997e-02,  4.25906889e-02, -1.16876215e-02,\n",
       "         2.44877599e-02,  2.68372931e-02,  1.89647730e-02,  4.04885691e-03,\n",
       "        -3.08213364e-02,  2.22420245e-02, -2.03509461e-02, -1.25644673e-02,\n",
       "         2.14895932e-03, -3.51759344e-02,  1.75059214e-02,  7.11090630e-03,\n",
       "         8.93108547e-03,  8.00087750e-02,  9.42784250e-02,  8.71903636e-03,\n",
       "        -6.12522522e-03,  2.96875136e-03,  3.19627188e-02, -5.41213863e-02,\n",
       "        -6.05712179e-03, -1.89679547e-03, -9.05876339e-04, -8.12570751e-02,\n",
       "         3.05480566e-02,  2.76454240e-02, -2.36025937e-02, -2.23337822e-02,\n",
       "        -5.76728322e-02, -8.25741664e-02, -5.10155000e-02,  6.34748787e-02,\n",
       "         4.35753725e-02,  5.93942180e-02, -3.60806659e-02, -1.01945931e-02,\n",
       "        -6.88280771e-03, -2.85061616e-02,  4.94415313e-02, -1.01403855e-01,\n",
       "         1.57124717e-02, -6.34149984e-02,  2.02651322e-02, -2.03162022e-02,\n",
       "        -2.32416559e-02, -1.97100155e-02,  6.36162162e-02,  1.09912083e-02,\n",
       "        -2.07231054e-03, -6.50255829e-02, -1.43015226e-02,  8.37615505e-03,\n",
       "         8.68086889e-02,  5.06603718e-02,  3.47944126e-02, -6.57364577e-02,\n",
       "        -6.83145504e-03,  3.45053747e-02,  1.51716573e-02, -9.92051233e-03,\n",
       "         1.26093428e-03, -1.49172191e-02, -9.53568704e-03,  5.46179898e-03,\n",
       "         3.92304314e-03, -6.74472377e-02, -1.63674764e-02,  3.06158941e-02,\n",
       "        -3.20901014e-02,  3.26911323e-02,  5.54977506e-02, -4.03435230e-02,\n",
       "         2.62313569e-03, -6.11702539e-03,  8.25122837e-03,  9.37426055e-04,\n",
       "         6.52833050e-03,  4.25802544e-02,  1.23839621e-02,  5.80952875e-02,\n",
       "        -7.24888034e-03,  2.34044604e-02, -4.46007065e-02,  3.52612720e-03,\n",
       "        -5.43947369e-02, -9.84162614e-02, -4.37549427e-02,  6.71303719e-02,\n",
       "        -2.79149767e-02,  3.37918177e-02, -4.09580953e-02,  1.47230234e-02,\n",
       "         1.61213912e-02,  1.15703121e-02,  3.55841182e-02, -3.47639509e-02,\n",
       "         2.79803760e-02,  5.46905994e-02,  3.02068852e-02, -1.11624105e-02,\n",
       "        -2.98556611e-02,  2.80969534e-02, -3.59165743e-02, -4.49879467e-02,\n",
       "        -3.83290574e-02, -4.11027577e-03, -2.76025049e-02,  1.85180902e-02,\n",
       "         8.30340460e-02, -1.19511504e-03, -1.41609348e-02, -1.46052623e-02,\n",
       "        -4.95330319e-02, -3.49124558e-02, -1.11612240e-02,  2.28576288e-02,\n",
       "         1.17240567e-02,  7.35329837e-03, -5.14013087e-03,  1.97386444e-02,\n",
       "         4.17545363e-02,  2.52005607e-02,  2.70608813e-02,  1.90351699e-02,\n",
       "        -6.13709092e-02, -3.25286090e-02,  4.22381349e-02, -7.21358051e-33,\n",
       "        -1.23268934e-02, -6.75390363e-02,  9.37988516e-03,  3.68694365e-02,\n",
       "        -3.00080981e-02, -2.18559969e-02,  2.53564343e-02, -1.35008702e-02,\n",
       "        -2.13926826e-02, -8.85235518e-02, -1.67920925e-02,  2.87700887e-03,\n",
       "         3.54079492e-02,  2.20449287e-02,  1.88849065e-02, -8.60718731e-03,\n",
       "        -6.97646569e-03,  4.42813188e-02, -1.85539033e-02, -4.04726006e-02,\n",
       "        -1.33720618e-02,  6.25313222e-02,  3.66774611e-02,  1.26631279e-02,\n",
       "         3.61805269e-03, -5.05480804e-02, -2.82208994e-03,  3.11403815e-02,\n",
       "        -1.38001330e-03, -1.81994552e-03, -2.23440286e-02, -3.41530666e-02,\n",
       "        -1.35173369e-02, -4.91115786e-02, -8.05529300e-03,  3.06161046e-02,\n",
       "        -5.82441241e-02, -6.20030500e-02, -2.16536876e-02,  3.07924375e-02,\n",
       "        -4.92212072e-04, -1.31766442e-02,  1.28728244e-02, -2.31314376e-02,\n",
       "        -3.25042419e-02, -7.38157239e-03,  3.87607031e-02,  2.32709832e-02,\n",
       "         3.69359762e-03,  3.06796730e-02, -8.36979132e-03, -6.11329265e-03,\n",
       "        -2.48070955e-02,  5.88912144e-02,  3.37567851e-02,  3.72766070e-02,\n",
       "         1.51896225e-02, -7.98172876e-03, -5.38925454e-03,  3.75549644e-02,\n",
       "         6.83031306e-02,  2.35961061e-02,  2.56405137e-02, -8.20974912e-03,\n",
       "        -1.60170253e-02,  1.34888478e-02, -2.92558242e-02, -8.28828849e-03,\n",
       "         2.11390411e-03, -4.26329672e-02, -4.65291440e-02,  4.94637750e-02,\n",
       "         1.91450380e-02,  4.00902890e-02,  9.13894251e-02,  1.29160332e-02,\n",
       "        -6.95965961e-02,  5.50139137e-02,  6.42934069e-02,  4.58889455e-02,\n",
       "        -5.48473001e-02, -1.15908897e-02, -1.65818483e-02, -4.05180780e-03,\n",
       "         2.14927960e-02, -8.04096926e-03, -2.86172479e-02, -1.75099093e-02,\n",
       "         8.48673191e-03, -2.74639111e-02, -3.98046747e-02,  3.84538546e-02,\n",
       "         4.50183288e-04,  1.55813275e-02,  3.46118920e-02, -3.72587852e-02,\n",
       "         4.33735549e-02,  1.78967923e-04, -4.57904255e-03,  1.97439920e-02,\n",
       "        -4.85951975e-02,  8.69712885e-03,  1.79150817e-03, -3.19840759e-02,\n",
       "        -2.76436587e-03,  1.50215952e-02, -4.83418778e-02,  1.82377100e-02,\n",
       "        -5.06995991e-02,  6.33583637e-03,  2.97104996e-02, -5.47239035e-02,\n",
       "         1.20227803e-02, -3.44982222e-02,  1.93819888e-02,  1.10261934e-02,\n",
       "        -3.95428296e-03,  1.93672422e-02, -2.45316401e-02, -1.99466720e-02,\n",
       "        -5.49640134e-03, -3.32565159e-02,  6.38704840e-03,  2.66941357e-03,\n",
       "        -2.85583399e-02, -8.81919358e-03,  5.57307759e-03, -1.67137831e-02,\n",
       "         6.23833500e-02, -2.99848188e-02,  1.47412336e-02,  3.26487720e-02,\n",
       "         3.07965678e-07,  1.51192620e-02,  1.28572695e-02,  2.85533816e-02,\n",
       "         1.21847717e-02, -3.95563282e-02, -2.84369402e-02, -1.60345491e-02,\n",
       "         2.16351822e-02,  1.89631619e-02,  3.36292456e-03,  2.87513100e-02,\n",
       "        -3.03366184e-02,  1.05083259e-02,  2.65803430e-02, -1.05769737e-02,\n",
       "        -1.11627638e-01,  2.39367131e-02, -7.85742234e-03, -6.35274872e-02,\n",
       "        -1.61781348e-02,  5.42102307e-02,  4.03827503e-02,  3.72170820e-03,\n",
       "        -3.16905752e-02, -2.10239924e-02, -4.96923216e-02, -2.38613901e-03,\n",
       "        -2.81960256e-02,  2.46754717e-02,  2.45866757e-02,  5.87524809e-02,\n",
       "        -4.81288787e-03, -5.85063873e-03,  3.84914279e-02, -1.29416436e-02,\n",
       "        -2.65798774e-02,  2.38059647e-02,  3.00294962e-02,  4.88311537e-02,\n",
       "        -4.84360904e-02, -2.36186162e-02,  7.30287656e-03,  1.99984573e-02,\n",
       "        -6.49757385e-02,  1.85712762e-02, -5.41663729e-03,  2.76337136e-02,\n",
       "         5.02487123e-02, -2.46576574e-02, -4.76096123e-02,  1.65196962e-03,\n",
       "        -3.80463207e-05,  4.77570258e-02,  2.08404325e-02,  3.73102236e-03,\n",
       "        -3.40785808e-03, -1.00445850e-02, -2.78018489e-02,  6.00952655e-02,\n",
       "         2.60557178e-02, -3.67717184e-02, -2.26766467e-02,  4.12575454e-02,\n",
       "        -1.60604231e-02,  3.16775367e-02, -2.78297700e-02, -4.77233306e-02,\n",
       "         3.38869881e-34, -2.97995992e-02, -1.27854403e-02,  1.06352651e-02,\n",
       "         5.31498902e-02, -2.91557573e-02, -1.08674988e-02,  6.58005327e-02,\n",
       "         1.44671835e-02, -1.28480652e-02, -7.83060193e-02, -3.18217836e-02],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('elastic_search_data.pkl', 'wb') as outfile:\n",
    "    pickle.dump(transcripts, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
