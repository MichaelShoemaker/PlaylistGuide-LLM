{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('elastic_search_data.pkl', 'rb') as infile:\n",
    "    transcripts = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for transcript in transcripts:\n",
    "    transcript.pop('search_vector', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vid_id': 'Q75JgLEXMsM',\n",
       " 'title': 'LLM Zoomcamp 1.1 - Introduction to LLM and RAG',\n",
       " 'timecode': '00:00',\n",
       " 'text': \"hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so\",\n",
       " 'timecode_text': 'Introduction to LLM Zoomcamp',\n",
       " 'description': \"Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       " 'link': 'https://www.youtube.com/watch?v=Q75JgLEXMsM&t=0s',\n",
       " 'text_vector': \"LLM Zoomcamp 1.1 - Introduction to LLM and RAG hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       " 'id': '26c14fe6727a6d5697b4853bcaa6e984'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(transcript):\n",
    "    return f\"\"\"\n",
    "You are an expert at generating potential questions based on course content. This course teaches Retrieval Augmented Generation (RAG) and Large Language \n",
    "Models (LLMs). Please generate three potential questions a student might ask, specifically where the given record would be the most relevant to provide \n",
    "answers. Return the original dictionary with a new field 'student_questions' added. The field should contain a list of three questions as \n",
    "'student_questions': [question1, question2, question3]. Return the dictionary in JSON format. Not python. JSON format and just return the dictionary\n",
    "in JSON FORMAT\n",
    "\n",
    "Here is the record for which to generate the questions:\n",
    "\n",
    "DICTIONARY:\n",
    "{transcript}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_215003/2661763515.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscripts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'make_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = make_prompt(transcripts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are an expert at generating potential questions based on course content. This course teaches Retrieval Augmented Generation (RAG) and Large Language \\nModels (LLMs). Please generate three potential questions a student might ask, specifically where the given record would be the most relevant to provide \\nanswers. Return the original dictionary with a new field \\'student_questions\\' added. The field should contain a list of three questions as \\n\\'student_questions\\': [question1, question2, question3].\\n\\nHere is the record for which to generate the questions:\\n\\nDICTIONARY:\\n{\\'vid_id\\': \\'Q75JgLEXMsM\\', \\'title\\': \\'LLM Zoomcamp 1.1 - Introduction to LLM and RAG\\', \\'timecode\\': \\'00:00\\', \\'text\\': \"hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I\\'ll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I\\'ll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there\\'s a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it\\'s not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that\\'s why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we\\'ll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let\\'s start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so\", \\'timecode_text\\': \\'Introduction to LLM Zoomcamp\\', \\'description\\': \"Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don\\'t cover the theory behind LLMs, but we will learn how to utilize them effectively.\", \\'link\\': \\'https://www.youtube.com/watch?v=Q75JgLEXMsM&t=0s\\', \\'text_vector\\': \"LLM Zoomcamp 1.1 - Introduction to LLM and RAG hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I\\'ll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I\\'ll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there\\'s a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it\\'s not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that\\'s why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we\\'ll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let\\'s start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don\\'t cover the theory behind LLMs, but we will learn how to utilize them effectively.\", \\'id\\': \\'26c14fe6727a6d5697b4853bcaa6e984\\'}\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OpenAI API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "def make_recs(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Extract response content\n",
    "    response_content = response.choices[0].message\n",
    "\n",
    "    # Optionally, print or process the response\n",
    "    return response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = make_recs(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = test.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing JSON: Expecting ',' delimiter: line 5 column 285 (char 403)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "start_index = response.index(\"{\")\n",
    "end_index = response.rindex(\"}\") + 1\n",
    "dict_str = response[start_index:end_index]\n",
    "\n",
    "# Step 2: Replace the outer single quotes with double quotes using regex, preserving inner quotes\n",
    "dict_str = re.sub(r\"(?<!\\\\)'\", '\"', dict_str)\n",
    "\n",
    "# Step 3: Try to load the dictionary using the json module\n",
    "try:\n",
    "    parsed_dict = json.loads(dict_str)\n",
    "    print(parsed_dict)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error parsing JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_dict = response.split('```')[1].split('python')[1].replace('\\n','').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixer = f\"\"\"\n",
    "Can you format this into a proper python dictionary and just return it as a dictionary? Return JUST THE DICTIONARY without any extra words:\n",
    "I don't want anything except the dictionary I don't want code blocks with ```. Just the formatted dictionary. Additionally can you remove any single or\n",
    "double quotes and commas in the values which may cause issues when using ast.literal_eval?\n",
    "\n",
    " {broken_dict}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "def ask_ollama(prompt):\n",
    "    response = ollama.chat(model='llama3', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': f\"{prompt}\",\n",
    "    },\n",
    "    ])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_dict = ask_ollama(fixer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vid_id': 'Q75JgLEXMsM',\n",
       " 'title': 'LLM Zoomcamp 1.1 - Introduction to LLM and RAG',\n",
       " 'timecode': '00:00',\n",
       " 'text': 'hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I\\'ll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I\\'ll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there\"s a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it\"s not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that\"s why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we\\'ll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let\"s start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so',\n",
       " 'id': '26c14fe6727a6d5697b4853bcaa6e984',\n",
       " 'student_questions': ['What is Retrieval Augmented Generation (RAG) and how will it be applied in this course?',\n",
       "  'How does the Q&A system we are building utilize the FAQ documents?',\n",
       "  'What are Large Language Models (LLMs) and how do they function in the context of this course?']}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "ast.literal_eval(fixed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('elastic_search_data.pkl', 'rb') as infile:\n",
    "    transcripts = pickle.load(infile)\n",
    "\n",
    "for transcript in transcripts:\n",
    "    transcript.pop('search_vector', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('OpenAI_responses.pkl', 'rb') as infile:\n",
    "    data = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in data:\n",
    "    \n",
    "    try:\n",
    "        i.content.split('```')[1].split('python')[1].replace('\\n','').split(',')\n",
    "    except Exception as e:\n",
    "        count +=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vid_id': 'Q75JgLEXMsM',\n",
       " 'title': 'LLM Zoomcamp 1.1 - Introduction to LLM and RAG',\n",
       " 'timecode': '00:00',\n",
       " 'text': \"hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so\",\n",
       " 'timecode_text': 'Introduction to LLM Zoomcamp',\n",
       " 'description': \"Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       " 'link': 'https://www.youtube.com/watch?v=Q75JgLEXMsM&t=0s',\n",
       " 'text_vector': \"LLM Zoomcamp 1.1 - Introduction to LLM and RAG hi everyone Welcome to our course this is our first module for first unit so in this course the course is called llm Zoom camp in this course we will learn about practical applications of llm and in particular we will focus our attention on rack retrieval a generation I'll shortly talk about these variations what they mean um and what we exactly will do and I want to start first with explaining the problem we are going to use uh to solve throughout the course um so this will be our running problem and in our community in data do club we have multiple courses so this llm Zoom Camp is our fifth course and usually in our courses we have frequently asked questions so there are questions that uh there are no answers in the videos or answers are not uh easy to find and we have these documents I'll quickly open one of them and in these documents we have frequently asked questions so the format is there is a section then there's a question then there is an answer and this is like that question answer question answer and we have quite quite a few of them so in this particular document for the data engineering Zoom camp we have 321 page of such answers and typically we ask the students to uh use this document so before they uh go to slack to the channel to the group to the course channel uh we ask them to check this document first before they ask a question and of course um most of the questions they have the students have they will find here but the problem is that it's not super easy to find right so there are 321 questions like how do you actually find the information you need here so this is not trial that's why we will use the data from uh these FAQs from the three FAQs from the three courses and we will beat a b we will build a report a Q&A system that given a question from a potential student will use the documents the fq documents uh these particular documents to answer questions from the students so this is what we are going to build at end it will be a simple form this is how I see right now we have not buil it yet it will be a simple form where you put an answer a question and get back answer so how we are going to do this we are going to to use llms and tracks so what they are and this is exactly what we are going to talk about in this module so now we will talk about LMS we'll talk about rack what the rack is and what exactly we are going to cover um well what exactly you will learn in the course um and what you will build so let's start so and we will start with what llms are so llms or llm it it stands for um large language model LGE language model uh and we can start with just language model so language models are things that predict the next token the next word based on the words you have typed so far or you have so far in your document so Welcome to the first module of our course, LLM Zoomcamp! We cover the applications of LLM, focusing on RAG: retrieval augmented generation. Throughout the course, we will build a Q&A system using the FAQ data from our courses. We don't cover the theory behind LLMs, but we will learn how to utilize them effectively.\",\n",
       " 'id': '26c14fe6727a6d5697b4853bcaa6e984',\n",
       " 'student_questions': ['What is Retrieval Augmented Generation (RAG) and how is it used in this course?',\n",
       "  'How do large language models (LLMs) work in the context of answering frequently asked questions?',\n",
       "  'What practical applications of LLMs will we explore in this course?']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "ast.literal_eval(data[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "example_questions = []\n",
    "for rec in data:\n",
    "    example_questions.append(ast.literal_eval(rec.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "\n",
    "# Loop through each dictionary in the data list\n",
    "for item in example_questions:\n",
    "    # Extract the common fields\n",
    "    base_dict = {key: item[key] for key in item if key != 'student_questions'}\n",
    "    \n",
    "    # For each question, create a new dictionary with the question added\n",
    "    for question in item['student_questions']:\n",
    "        # Create a new dictionary with the common fields and add the question\n",
    "        new_dict = base_dict.copy()\n",
    "        new_dict['student_question'] = question\n",
    "        \n",
    "        # Append the new dictionary to the result list\n",
    "        new_data.append(new_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "837"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "837"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "279 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ground_truth.pkl','wb') as outfile:\n",
    "    pickle.dump(new_data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
