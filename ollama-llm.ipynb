{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "def ask_ollama(prompt):\n",
    "    response = ollama.chat(model='mistral', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': f\"{prompt}\",\n",
    "    },\n",
    "    ])\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, context):\n",
    "    return f\"\"\"You are a helpful teaching assistant with knowledge of the YouTube playlist used in a course that teaches about LLMs (Large Language Models). Students ask you questions about where\n",
    "    content is in the playlist as well as questions about the concepts and topics in the playlist. Answer the question using the information provided in the CONTEXT section below.\n",
    "    If what they are asking does not appear in the CONTEXT provided. Please explicitly state that you were not able to locate the information in the video content and then provide\n",
    "    an answer as best you can:\n",
    "\n",
    "    QUESTION: {question}\n",
    "\n",
    "    CONTEXT: {context}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def retrieve_documents_question(query, index_name=\"transcripts\", max_results=5):\n",
    "    es = Elasticsearch(\"http://localhost:9200\")\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = model.encode(query)\n",
    "    \n",
    "    search_query = {\n",
    "        \"size\": max_results,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\n",
    "                        \"multi_match\": {\n",
    "                            \"query\": query,\n",
    "                            \"fields\": [\"Title\", \"Text^3\"],\n",
    "                            \"type\": \"most_fields\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"script_score\": {\n",
    "                            \"query\": {\"match_all\": {}},\n",
    "                            \"script\": {\n",
    "                                \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
    "                                \"params\": {\"query_vector\": query_embedding}\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"Title\", \"Text\", \"TimeStamp\"],\n",
    "        # \"sort\": [\n",
    "        #     {\"TimeStamp\": {\"order\": \"asc\"}}\n",
    "        # ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = es.search(index=index_name, body=search_query)\n",
    "        result_docs = [hit['_source'] for hit in response['hits']['hits']]\n",
    "        return result_docs\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving documents: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "query = \"When did we start discussing Mage in the course?\"\n",
    "# query = generate_embeddings(query)\n",
    "results = retrieve_documents_question(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I could not locate any information about \"Mage\" in the provided video content for the course on LLMs (Large Language Models). It seems that the topics discussed in the playlist include RAG evaluation, monitoring and containerization, interface and ingestion pipeline, evaluating vector retrieval. If there is a specific lecture or module where Mage was mentioned, I would recommend double-checking the titles and timestamps of each video again, or reaching out to the course instructor for clarification.\n"
     ]
    }
   ],
   "source": [
    "print(ask_ollama(build_prompt(\"When did we start discussing Mage in the course\",results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful teaching assistant with knowledge of the YouTube playlist used in a course that teaches about LLMs (Large Language Models). Students ask you questions about where\n",
      "    content is in the playlist as well as questions about the concepts and topics in the playlist. Answer the question using the information provided in the CONTEXT section below.\n",
      "    If what they are asking does not appear in the CONTEXT provided. Please explicitly state that you were not able to locate the information in the video content and then provide\n",
      "    an answer as best you can:\n",
      "\n",
      "    QUESTION: When did we start discussing Mage in the course\n",
      "\n",
      "    CONTEXT: [{'Title': 'llm zoomcamp 7.3 - rag evaluation', 'Text': ' the only thing we can do right now is as a judge of course when we start running the um when we start running the', 'TimeStamp': 128.28}, {'Title': 'llm zoomcamp 7.5 - monitoring and containerization', 'Text': ' all the data is there of course we can do it even more automated in a more automated way so when we start the', 'TimeStamp': 1151.72}, {'Title': 'llm zoomcamp 7.4 - interface and ingestion pipeline', 'Text': ' complex and um i think what we did uh in the course llm', 'TimeStamp': 333.28}, {'Title': 'llm zoomcamp 3.3.4 - evaluating vector retrieval', 'Text': ' hi everyone in this series of videos we are discussing evaluating our retrieval methods we have already evaluated uh', 'TimeStamp': 1.12}, {'Title': 'llm zoomcamp 3.3.4 - evaluating vector retrieval', 'Text': ' example so when we did it in the first module our yeah so our query was', 'TimeStamp': 1675.36}]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(build_prompt(\"When did we start discussing Mage in the course\",results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Title': 'llm zoomcamp 7.3 - rag evaluation',\n",
       "  'Text': ' the only thing we can do right now is as a judge of course when we start running the um when we start running the',\n",
       "  'TimeStamp': 128.28},\n",
       " {'Title': 'llm zoomcamp 7.5 - monitoring and containerization',\n",
       "  'Text': ' all the data is there of course we can do it even more automated in a more automated way so when we start the',\n",
       "  'TimeStamp': 1151.72},\n",
       " {'Title': 'llm zoomcamp 7.4 - interface and ingestion pipeline',\n",
       "  'Text': ' complex and um i think what we did uh in the course llm',\n",
       "  'TimeStamp': 333.28},\n",
       " {'Title': 'llm zoomcamp 3.3.4 - evaluating vector retrieval',\n",
       "  'Text': ' hi everyone in this series of videos we are discussing evaluating our retrieval methods we have already evaluated uh',\n",
       "  'TimeStamp': 1.12},\n",
       " {'Title': 'llm zoomcamp 3.3.4 - evaluating vector retrieval',\n",
       "  'Text': ' example so when we did it in the first module our yeah so our query was',\n",
       "  'TimeStamp': 1675.36}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
